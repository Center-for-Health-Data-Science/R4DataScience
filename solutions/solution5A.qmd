---
title: "Exercise 5 - Solutions"
format: html
project:
  type: website
  output-dir: ../docs
---

1.  Load packages

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(ggfortify)
library(factoextra)
```

## Part 1: Linear regression

2.  Load the data `boston.csv`

```{r}
df <- as_tibble(read.delim('../data/boston.csv', sep = ','))
head(df)
```

3.  `Neighborhood` is a categorical variable. We could make it a factor but it will also work as a character column (in the case of using `lm`).

4.  Split the dataset into test and training data.

```{r}
# Set seed to ensure reproducibility
set.seed(123)  

#add an ID column to keep track of observations
df$ID <- 1:nrow(df)

train <- df %>% sample_frac(.75)
test  <- anti_join(df, train, by = 'ID') 
```

5.  Fit the model

```{r}
model <- lm(medv ~ rm + crim + neighborhood, data = train)
```

```{r}
summary(model)
```

6.  `rm` and `crim` have a significant influence on the house price. An increase in the number of rooms increases the price since the coefficient is positive, whereas an increase in crime rate reduces the price. There is a significant difference in price between Rural and Urban zones, but not between Rural and Suburban. Rural is the reference level. Lastly, houses with 0 rooms cost -25k dollar. Perhaps the predictors should be centered before fitting the model around 0 so `rm` == 0 is the average number of rooms for better interpretability.

7.  If you wanted to know if there is a difference in the value of houses between the `Suburban` and `Urban` neighborhood what could you do to the variable `neighborhood` before modelling?

```{r, eval=FALSE}
df <- df %>% 
  mutate(neighborhood= factor(neighborhood, levels=c("Urban", "Suburban", "Rural")))
```

8.  For linear regression there is an assumption that the model residuals (errors) are normally distributed. An easy way visualize this is by simply calling `plot()` on your model (see below). What do you think based on the plots?

```{r, eval=FALSE}
#RMSE
par(mfrow=c(2,2))
plot(model)
```

Overall the plots look okay. There are some outlines, but all of them are within cooks distance, i.e. they are not extreme. The model residuals are close to normally distributed.

9.  Now, use our test set to predict the response `medv` (`median value per house in 1000s`).

```{r}
y_pred <- predict(model, newdata = test)
```

10. Evaluate how well our model performs. There are different ways of doing this but lets use the classic measure of RMSE (Root Mean Square Error).

```{r, eval=FALSE}
#RMSE
rmse <- sqrt(mean((test$medv - y_pred)^2))

rmse

```

11. Make a scatter plot to visualize how the predicted values fit with the observed values.

```{r}

predPlot <- tibble(y_test = test$medv, y_pred=y_pred)
  
ggplot(predPlot, aes(x = y_test, y=y_pred)) +
  geom_point(alpha = 0.7, size = 2) +  # scatter points
  geom_smooth(method = "lm", se = TRUE, color = "blue", linewidth = 1) +
  theme_minimal()
```

## Part 2: Logistic regression

For this part we will use the joined diabetes data since it has a categorical outcome (Diabetes yes or no). We will not use the oral Glucose measurements as predictors since this is literally how you define diabetes, so we're loading the joined dataset we created in exercise 1, e.g. 'diabetes_join.xlsx' or what you have named it. N.B if you did not manage to finish making this dataset or forgot to save it, you can find a copy here: `../out/diabetes_join.Rdata`. Navigate to the file in the file window of Rstudio and click on it. Click "Yes" to confirm that the file can be loaded in your environment and check that it has happened.

As the outcome we are studying, `Diabetes`, is categorical variable we will perform logistic regression. We select serum calcium levels (`Serum_ca2`), `BMI` and smoking habits (`Smoker`) as predictive variables.

12. Read in the diabetes_join.xlsx dataset.

```{r}

load("../out/diabetes_join.Rdata")

head(diabetes_join)
```

13. Logistic regression does not allow for any missing values in the outcome variable. Ensure that the variable `Diabetes` does not have missing values AND that it is a factor variable.

```{r}
diabetes_df <- diabetes_join %>% 
  filter(!is.na(Diabetes)) %>%
  mutate(Diabetes= as.factor(Diabetes))
```

14. Split your data into training and test data. Take care that the two classes of the outcome variables are represented in both training and test data, and at similar ratios.

```{r}
set.seed(123)  


train <- diabetes_df %>% sample_frac(.75)
test  <- anti_join(diabetes_df, train, by = 'ID') 

```

```{r}
train_counts <- train[,-1] %>%
  dplyr::select(where(is.character)) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Level") %>%
  count(Variable, Level, name = "Count")


train_counts



test_counts <- test[,-1] %>%
  dplyr::select(where(is.character)) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Level") %>%
  count(Variable, Level, name = "Count")


test_counts

```

15. Fit a logistic regression model with `Serum_ca2`, `BMI` and `Smoker` as predictors and `Diabetes` as outcome, using your training data.

```{r}
mod1 <- glm(Diabetes ~ Serum_ca2 + BMI + Smoker, data = train, family = 'binomial')
summary(mod1)

```

16. Check the model summary and try to determine whether you could potentially drop one of your variables? If so, make this alternative model and compare it to the original model. Is there a significant loss/gain, i.e. better fit when including the serum calcium levels as predictor?

```{r}
mod2 <- glm(Diabetes ~ BMI + Smoker, data = train, family = binomial)
summary(mod2)
```

```{r}
anova(mod1, mod2, test = "Chisq")
```

17. Now, use your model to predict Diabetes class based on your test set. What does the output of the prediction mean?

```{r}
y_pred <- predict(mod2, test, type = "response")
```

18. Lets evaluate the performance of our model. As we are performing classification, measures such as mse/rmse will not work, instead we will calculate the accuracy. In order to get the accuracy you must first convert our predictions into Diabetes class labels (e.g. 0 or 1).

```{r}
y_pred <- as.factor(ifelse(y_pred > 0.5, 1, 0))

caret::confusionMatrix(y_pred, test$Diabetes)
```

## Part 3: Clustering

In this part we will run clustering on the joined diabetes dataset from exercise 1. Load it here if you don't have it already from Part 2.

```{r}
#in case
load('../out/diabetes_join.Rdata')

head(diabetes_join)
```

In this part we will run clustering on the joined diabetes dataset (`diabetes_join.xlsx`) from exercise 1. Load it here if you don't have it already from Part 2 above.

19. Before running K-means clustering. Remove any missing values across all variables in your dataset. Consider which columns you can use and if you have to do anything to them before clustering?
```{r}
# Set seed to ensure reproducibility
set.seed(123)  

# scale data
diabetes_df <- diabetes_join %>%
  drop_na(.) %>%
  mutate(across(where(is.numeric), scale)) %>%
  dplyr::select(where(is.numeric))
```


20. Run the k-means clustering algorithm with 3 centers on the data.

```{r}
# Set seed to ensure reproducibility
set.seed(123)  

# run kmeans
kmeans_res <- diabetes_df %>% 
  kmeans(centers = 4, nstart = 25)

kmeans_res
```

```{r}
kmeans_res$centers
```


21. Visualize the results of your clustering.

```{r}
fviz_cluster(kmeans_res, data = diabetes_df,
             palette = c("#2E9FDF", "#00AFBB", "#E7B800", "orchid3"), 
             geom = "point",
             ellipse.type = "norm", 
             ggtheme = theme_bw())
```


22. Investigate the best number of clusters. Use the `silhouette` metric.

```{r}
diabetes_df %>%
  fviz_nbclust(kmeans, method = "silhouette")
```

23. Re-do the clustering (plus visualization) with that number. What do you think the clusters represent in this case?

```{r}
# Set seed to ensure reproducibility
set.seed(123)  

#run kmeans

kmeans_res <- diabetes_df %>% 
  kmeans(centers = 2, nstart = 25)

kmeans_res
```



```{r}
fviz_cluster(kmeans_res, data = diabetes_df,
             palette = c("#00AFBB", "#E7B800"), 
             geom = "point",
             ellipse.type = "norm", 
             ggtheme = theme_bw())
```
