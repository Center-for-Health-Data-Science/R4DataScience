---
title: "Exercise 5 - Solutions"
format: html
project:
  type: website
  output-dir: ../docs
---

1.  Load packages

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(ggfortify)
library(factoextra)
```

## Part 1: Linear regression

2.  Load the data `boston.csv`

```{r}
df <- as_tibble(read.delim('../data/boston.csv', sep = ','))
head(df)
```

3.  `Neighborhood` is a categorical variable. We could make it a factor but it will also work as a character column (in the case of using `lm`).

4.  Split the dataset into test and training data.

```{r}
# Set seed to ensure reproducibility
set.seed(123)  

#add an ID column to keep track of observations
df$ID <- 1:nrow(df)

train <- df %>% sample_frac(.75)
test  <- anti_join(df, train, by = 'ID') 
```

5.  Fit the model

```{r}
model <- lm(medv ~ rm + crim + neighborhood, data = train)
```

```{r}
summary(model)
```

6.  `rm` and `crim` have a significant influence on the house price. An increase in the number of rooms increases the price since the coefficient is positive, whereas an increase in crime rate reduces the price. There is a significant difference in price between Rural and Urban zones, but not between Rural and Suburban. Rural is the reference level. Lastly, houses with 0 rooms cost -25k dollar. Perhaps the predictors should be centered before fitting the model around 0 so `rm` == 0 is the average number of rooms for better interpretability.

7.  If you wanted to know if there is a difference in the value of houses between the `Suburban` and `Urban` neighborhood what could you do to the variable `neighborhood` before modelling?

```{r, eval=FALSE}
df <- df %>% 
  mutate(neighborhood= factor(neighborhood, levels=c("Urban", "Suburban", "Rural")))
```

8.  For linear regression there is an assumption that the model residuals (errors) are normally distributed. An easy way visualize this is by simply calling `plot()` on your model (see below). What do you think based on the plots?

```{r, eval=FALSE}
#RMSE
par(mfrow=c(2,2))
plot(model)
```

Overall the plots look okay. There are some outlines, but all of them are within cooks distance, i.e. they are not extreme enough that they majorly effect the model. The model residuals are close to normally distributed.

9.  Now, use our test set to predict the response `medv` (`median value per house in 1000s`).

```{r}
y_pred <- predict(model, newdata = test)
```

10. Evaluate how well our model performs. There are different ways of doing this but lets use the classic measure of RMSE (Root Mean Square Error).

```{r, eval=FALSE}
#RMSE
rmse <- sqrt(mean((test$medv - y_pred)^2))

rmse

```

11. Make a scatter plot to visualize your model fit.

```{r}

predPlot <- tibble(y_test = test$medv, y_pred=y_pred)
  
ggplot(predPlot, aes(x = y_test, y=y_pred)) +
  geom_point(alpha = 0.7, size = 2) +  # scatter points
  geom_smooth(method = "lm", se = TRUE, color = "blue", linewidth = 1) +
  theme_minimal()
```

## Part 2: Logistic regression

For this part we will use the joined diabetes data since it has a categorical outcome (Diabetes yes or no). We will not use the oral Glucose measurements as predictors since this is literally how you define diabetes, so we're loading the joined dataset we created in exercise 1, e.g. 'diabetes_join.xlsx' or what you have named it.

For this part we will use the joined diabetes, so lets load the joined dataset we created in exercise 1, e.g. 'diabetes_join.xlsx' or what you have named it.

As the outcome we are studying, `Diabetes`, is categorical variable we will perform logistic regression. We select serum calcium levels (`Serum_ca2`), `BMI` and smoking habits (`Smoker`) as predictive variables.

12. Read in the diabetes_join.xlsx dataset.

```{r}
diabetes_df <- read_excel('../out/diabetes_join.xlsx')
head(diabetes_df)
```

13. Logistic regression does not allow for any missing values in the outcome variable. Ensure that the variable `Diabetes` does not have missing values AND that it is a factor variable.

```{r}
diabetes_df <- diabetes_df %>% 
  filter(!is.na(Diabetes)) %>%
  mutate(Diabetes= as.factor(Diabetes))
```

14. Split your data into training and test data. Take care that the two classes of the outcome variables are represented in both training and test data, and at similar ratios.

```{r}
set.seed(123)  


train <- diabetes_df %>% sample_frac(.75)
test  <- anti_join(diabetes_df, train, by = 'ID') 

```

```{r}
train_counts <- train[,-1] %>%
  dplyr::select(where(is.character)) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Level") %>%
  count(Variable, Level, name = "Count")


train_counts



test_counts <- test[,-1] %>%
  dplyr::select(where(is.character)) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Level") %>%
  count(Variable, Level, name = "Count")


test_counts

```

15. Fit a logistic regression model with `Serum_ca2`, `BMI` and `Smoker` as predictors and `Diabetes` as outcome, using your training data.

```{r}
mod1 <- glm(Diabetes ~ Serum_ca2 + BMI + Smoker, data = train, family = 'binomial')
summary(mod1)

```

16. Check the model summary and try to determine whether you could potentially drop one of your variables? If so, make this alternative model and compare it to the original model. Is there a significant loss/gain, i.e. better fit when including the serum calcium levels as predictor?

```{r}
mod2 <- glm(Diabetes ~ BMI + Smoker, data = train, family = binomial)
summary(mod2)
```

```{r}
anova(mod1, mod2, test = "Chisq")
```

17. Now, use your model to predict Diabetes class based on your test set. What does the output of the prediction mean?

```{r}
y_pred <- predict(mod2, test, type = "response")
```

18. Lets evaluate the performance of our model. As we are performing classification, measures such as mse/rmse will not work, instead we will calculate the accuracy. In order to get the accuracy you must first convert our predictions into Diabetes class labels (e.g. 0 or 1).

```{r}
y_pred <- as.factor(ifelse(y_pred > 0.5, 1, 0))

caret::confusionMatrix(y_pred, test$Diabetes)
```

## Part 3: Clustering

In this part we will run clustering on the joined diabetes dataset from exercise 1. Load it here if you don't have it already from Part 2.

```{r}
#in case
diabetes_df <- read_excel('../out/diabetes_join.xlsx')
head(diabetes_df)
```

In this part we will run clustering on the joined diabetes dataset (`diabetes_join.xlsx`) from exercise 1. Load it here if you don't have it already from Part 2 above.

19. Before running K-means clustering. Remove any missing values across all variables in your dataset.

```{r}

diabetes_df <- drop_na(diabetes_df)

```

20. Run the k-means clustering algorithm with 4 centers on the data. Consider which columns you can use and if you have to do anything to them before clustering?

```{r}
# Set seed to ensure reproducibility
set.seed(123)  

#run kmeans
kmeans_res <- diabetes_df %>%
  dplyr::select(where(is.numeric)) %>%
  #scale all numeric cols
  mutate(across(where(is.numeric), scale)) %>%
  kmeans(centers = 4, nstart = 25)

kmeans_res
```

21. Visualize the results of your clustering.

We first need to calculate a PCA. This is basically the same we have done in exercise 3B.

```{r}
pca_res <- diabetes_df %>%
  dplyr::select(where(is.numeric)) %>%
  #scale all numeric cols
  mutate(across(where(is.numeric), scale)) %>%
  prcomp()
```

Now we project the k-means centroids in the PCA space

```{r}
#project cluster centers from kmeans into the pca space
centers_pca <- predict(pca_res, newdata = kmeans_res$centers) %>% as.data.frame()
# Label clusters
centers_pca$cluster <- as.factor(1:nrow(centers_pca))  
centers_pca
```

```{r}
diabetes_df$Cluster <- factor(kmeans_res$cluster)
```

Finally we can plot:

```{r}
autoplot(pca_res, data = diabetes_df, color = 'Cluster',
         loadings = TRUE, loadings.colour = "grey30", 
         loadings.label.colour = "black",
         loadings.label = TRUE, loadings.label.size = 3.5,
         scale = 0) + 
  theme_minimal() +
  labs(title = "PCA of Diabetes Dataset") +
  # Cluster centers
  geom_point(data = centers_pca, aes(x = PC1, y = PC2, color = cluster),
             shape = 8, size = 6, stroke = 2)  
```

22. Investigate the best number of clusters.

```{r}
diabetes_df %>%
  dplyr::select(where(is.numeric)) %>%
  #scale all numeric cols
  mutate(across(where(is.numeric), scale)) %>%
  fviz_nbclust(kmeans, method = "gap_stat")
```

23. Re-do the clustering (plus visualization) with that number.

```{r}
# Set seed to ensure reproducibility
set.seed(123)  

#run kmeans
kmeans_res <- diabetes_df %>%
  dplyr::select(where(is.numeric)) %>%
  #drop missing values
  drop_na() %>%
  #scale all numeric cols
  mutate(across(where(is.numeric), scale)) %>%
  kmeans(centers = 2, nstart = 25)

kmeans_res
```

```{r}
#project cluster centers from kmeans into the pca space
centers_pca <- predict(pca_res, newdata = kmeans_res$centers) %>% as.data.frame()
# Label clusters
centers_pca$cluster <- as.factor(1:nrow(centers_pca))  
centers_pca
```

```{r}
diabetes_df$Cluster <- factor(kmeans_res$cluster)
```

```{r}
autoplot(pca_res, data = diabetes_df, color = 'Cluster',
         loadings = TRUE, loadings.colour = "grey30", 
         loadings.label.colour = "black",
         loadings.label = TRUE, loadings.label.size = 3.5,
         scale = 0) + 
  theme_minimal() +
  labs(title = "PCA of Diabetes Dataset") +
  # Cluster centers
  geom_point(data = centers_pca, aes(x = PC1, y = PC2, color = cluster),
             shape = 8, size = 6, stroke = 2)  
```
