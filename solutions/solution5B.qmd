---
title: "Exercise 5B - Solutions"
format: html
project:
  type: website
  output-dir: ../docs
---

1.  Load th R packages needed for analysis:

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)
library(glmnet)
library(MASS)
library(randomForest)
```

## Summary Statistics

2.  Load in the dataset `Obt_Perio_ML.Rdata` and inspect it.

```{r}
load(file = "../data/Obt_Perio_ML.Rdata")
```

3.  Do some basic summary statistics and distributional plots to get a feel for the data. Which types of variables do we have?

```{r}

# Reshape data to long format for ggplot2
long_data <- optML %>% 
  dplyr::select(where(is.numeric)) %>%
  pivot_longer(cols = everything(), 
               names_to = "variable", 
               values_to = "value")

# Plot histograms for each numeric variable in one grid
ggplot(long_data, aes(x = value)) +
  geom_histogram(binwidth = 0.5, fill = "#9395D3", color ='grey30') +
  facet_wrap(~ variable, scales = "free") +
  theme_minimal()
```

4.  Make count tables of your categorical/factor variables, are they balanced?

```{r}
# Count observations per level/group for each categorical variable


factor_counts <- optML[,-1] %>%
  dplyr::select(where(is.character)) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Level") %>%
  count(Variable, Level, name = "Count")

factor_counts

```

## Part 1: Elastic Net Regression

5.  As you will use the response `Preg.ended...37.wk`, you should remove the other five outcome measures from your dataset.

6.  Elastic net regression can be sensitive to large differences in the range of numeric/integer variables, as such these variables should be scaled. Scale all numeric/integer variables in your dataset.

```{r}
optML <- optML %>% 
  dplyr::select(-c(Apgar1, Apgar5, GA.at.outcome, Birthweight, Any.SAE.)) %>%
  mutate(across(where(is.numeric), scale))
```

7.  Split your dataset into train and test set, you should have 70% of the data in the training set and 30% in the test set. How you chose to split is up to you, BUT afterwards you should ensure that for the categorical/factor variables all levels are represented in both sets.

```{r}

# Set seed
set.seed(123)

# Training set
train <- optML %>% 
  sample_frac(0.70) 

# Check group levels
train_counts <- train[,-1] %>%
  dplyr::select(where(is.character)) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Level") %>%
  count(Variable, Level, name = "Count")

train_counts




test  <- anti_join(optML, train, by = 'PID') 



# Check group levels
#test_counts <- optML[,-1] %>%
#  dplyr::select(where(is.character)) %>%
#  pivot_longer(everything(), names_to = "Variable", values_to = "Level") %>%
#  count(Variable, Level, name = "Count")

#test_counts



```

8.  After dividing into train and test set pull out the response variable `Preg.ended...37.wk` into its own vector for both datasets, name these: `y_train` and `y_test`.

```{r}

y_train <- train %>%
  pull(Preg.ended...37.wk)


y_test <- test %>% 
  pull(Preg.ended...37.wk)

```

9.  Remove the response variable `Preg.ended...37.wk` from the train and test set, as well as `PID` (if you have not already done so), as we should obviously not use this for training or testing.

```{r}
train <- train %>% 
  dplyr::select(-c(PID, Preg.ended...37.wk))


test <- test %>% 
  dplyr::select(-c(PID, Preg.ended...37.wk))
```

You will employ the package `glmnet` to perform Elastic Net Regression. The main function from this package is `glmnet()` which we will use to fit the model. Additionally, you will also perform cross validation with `cv.glmnet()` to obtain the best value of the model hyper-parameter, lambda (λ).

As we are working with a mix of categorical and numerical predictors, it is advisable to dummy-code the variables, you can easily do this by creating a model matrix for both test and train set.

10. Create the model matrix needed for input to `glmnet()` and `cv.glmnet()`:

```{r}
modTrain <- model.matrix(~ .- 1, data = train)
modTest <- model.matrix(~ .- 1, data = test)
```

Note that the `- 1` in the `model.matrix()` formula which means, drop the intercept from the matrix.

11. Create your Elastic Net Regression model with `glmnet()`.

```{r}
EN_model <- glmnet(modTrain, y_train, alpha = 0.5, family = "binomial")
```

12. Use `cv.glmnet()` to attain the best value of the hyperparameter lambda (λ). Remember to set a seed for reproducible results.

```{r}
set.seed(123)
cv_model <- cv.glmnet(modTrain, y_train, alpha = 0.5, family = "binomial")
```

13. Plot all the values of lambda tested during cross validation by calling `plot()` on the output of your `cv.glmnet()`. Extract the best lambda value from the `cv.glmnet()` model and save it as an object.

```{r}
plot(cv_model)

bestLambda = cv_model$lambda.min
```

Now, lets see how well your model performed.

14. Predict if a individual is likely to give birth before the 37th week using your model and your test set. See pseudo-code below

```{r}
y_pred <- predict(EN_model, s = bestLambda, newx = modTest, type = 'class')
```

15. Just like for the logistic regression model you can calculate the accuracy of the prediction by comparing it to `y_test` with `confusionMatrix()`. Do you have a good accuracy? N.B look at the 2x2 contingency table, what does it tell you?

```{r}
y_pred <- as.factor(y_pred)

caret::confusionMatrix(y_pred, y_test)
```

16. Lastly, lets extract the variables which were retained in the model (e.g. not penalized out). We do this by calling the coefficient with `coef()` on our model. See pseudo-code below.

```{r}
coeffs <- coef(EN_model, s = bestLambda)

# Convert coefficients to a data frame for easier viewing
coeffsDat <- as.data.frame(as.matrix(coeffs)) %>% 
  rownames_to_column(var = 'VarName')
 
```

16. Make a plot that shows the absolute importance of the variables retained in your model. This could be barplot with variable names on the x-axis and the height of the bars denoting absolute size of coefficient).

```{r}
# Make dataframe ready for plotting, remove intercept and coeffcients that are zero
coeffsDat <- coeffsDat %>% 
  mutate(AbsImp = abs(s1)) %>%
  arrange(AbsImp) %>%
  mutate(VarName = factor(VarName, levels=VarName)) %>%
  filter(AbsImp > 0 & VarName != "(Intercept)")


# Plot
ggplot(coeffsDat, aes(x = VarName, y = AbsImp)) +
  geom_bar(stat = "identity", fill = "#9395D3") +
  coord_flip() +
  labs(title = "Feature Importance for Elastic Net", 
       x = "Features", 
       y = "Absolute Coefficients") +
  theme_classic()
```

17. Now repeat what you just did above, but this time instead of using `Preg.ended...37.wk` as outcome, try using a continuous variable, such as `GA.at.outcome`. N.B remember this means that you should evaluate the model using the RMSE and a scatter plot instead of the accuracy!

## Part 2: Random Forest

Now, lets make a Random Forest. We will continue using the `Obt_Perio_ML.Rdata` with `Preg.ended...37.wk` as outcome.

18. Just like in the section on EN above, remove the outcome variables you will not be using and split the dataset into test and train set - this time keep the outcome variable `Preg.ended...37.wk` in the dataset. Remember to remove the `PID` column before training!

```{r}
load(file = "../data/Obt_Perio_ML.Rdata")

optML <- optML %>%
  mutate(Preg.ended...37.wk = factor(Preg.ended...37.wk, levels = c(0, 1), labels = c("No", "Yes")))

```

```{r}
optML <- optML %>% 
  dplyr::select(-c(Apgar1, Apgar5, GA.at.outcome, Birthweight, Any.SAE.))

```

```{r}
set.seed(123)

# Training set
train <- optML %>% 
  sample_frac(0.70) 


test  <- anti_join(optML, train, by = 'PID') 


train <- train %>% 
  dplyr::select(-PID)


test <- test %>% 
  dplyr::select(-PID)



```

19. Set up a Random Forest model with cross-validation. See pseudo-code below. Remember to set a seed.

```{r}
set.seed(123)

# Set up cross-validation: 5-fold CV
RFcv <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

# Train Random Forest
set.seed(123)
rf_model <- train(
  Preg.ended...37.wk ~ .,  # formula interface
  data = train,
  method = "rf",           # random forest
  trControl = RFcv,
  metric = "ROC",          # optimize AUC
  #preProcess = c("center", "scale"),  # optional
  tuneLength = 5           # try 5 different mtry values
)


# Model summary
print(rf_model)
```

20. Plot your model fit. How does your model improve when you add 10, 20, 30, etc. predictors?

```{r}
# Best parameters
rf_model$bestTune

# Plot performance
plot(rf_model)


```

21. Use your test set to evaluate your model performance. How does the random forest compare to the elastic net regression?

```{r}


# Predict class probabilities
y_pred <- predict(rf_model, newdata = test, type = "prob")

y_pred <- as.factor(ifelse(y_pred$Yes > 0.5, "Yes", "No"))

caret::confusionMatrix(y_pred, test$Preg.ended...37.wk)



```

22. Extract the predictive variables with the greatest importance from your fit.

```{r}
varImpOut <- varImp(rf_model)

varImpOut$importance
varImportance <- as.data.frame(as.matrix(varImpOut$importance)) %>% 
  rownames_to_column(var = 'VarName') %>%
  arrange(desc(Overall))

varImportance
```

23. Make a logistic regression using the same dataset (you already have your train data, test data). How do the results of Elastic Net regression and Random Forest compare to the output of your glm.

```{r}

# Model
model1 <- glm(Preg.ended...37.wk ~ ., data = train, family = 'binomial')


summary(model1)


# Filter for significant p-values and convert to tibble
model1out <- coef(summary(model1)) %>% 
  as.data.frame() %>%
  rownames_to_column(var = 'VarName') %>% 
  filter(`Pr(>|z|)` <= 0.05 & VarName != "(Intercept)")


# Compare output from Elastic Net with output from glm model
intersect(as.character(coeffsDat$VarName), model1out$VarName) %>%
  sort()

```
