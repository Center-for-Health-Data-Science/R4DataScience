---
title: "Presentation 5: Intro to Modelling in R"
format: html
project:
 type: website
 output-dir: ../docs
---

In this section we'll look at how to define and fit a model in R.

## Load packages
```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(caTools)
library(ModelMetrics)
```

## Load data

In order to focus on the technical aspects we'll use a very simple toy dataset. It contains the number of cigarettes smoked per day and how long the person lived. It is inspired by [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2598467/) if you want to take a look. 

```{r}
df_smoke <- as_tibble(read.csv('../data/smoking_cat.csv'))
df_smoke
```


We will use this to perform a linear regression.

![](../figures/lin_reg.jpg){fig-align="center"}

## Linear Regression 

### Split Data into Training and Test Set

First, we will split our data into a test and a training set. There are numerous ways to do this. We here show `sample_frac` from `dplyr`:


```{r}
# Set seed to ensure reproducibility
set.seed(123)  

#add an ID column to keep track of observations
df_smoke$ID <- 1:nrow(df_smoke)

train <- df_smoke %>% sample_frac(.75)
nrow(train)
head(train)

```

As you can see, the ID's in `train` are shuffled and it only has 75 rows since we asked for 75% of the data. Now all we have to do is identify the other 25%, i.e. the observations not in train. `dpylr` has a neat function called `anti_join` for that:

```{r}
#from df_smoke remove what is in train by checking the ID column
test  <- anti_join(df_smoke, train, by = 'ID') 
nrow(test)
head(test)
```

### Defining the model

As stated above, a linear regression model generally has the form of: 

$$y=b_0+b_1*x_i$$

Where we refer to $b_0$ as the intercept and $b_1$ as the coefficient. There will typically be one coefficient for each predictor. The goal of modelling is to estimate the values of $b_0$ and all $b_i$. 

We need to tell R which of our variables is the outcome, $y$ and which predictors $x_i$ we want to include in the model. This is referred to in documentation as the model's **formula**. Have a look:

```{r eval=FALSE}
#the formula is written like so:
lm(y ~ x_1 + x_2 + ...)
#see the help
?lm
```

In our case, $y$ is the number of years lived and we have a singular predictor $x_1$, the number of cigarettes smoked per day. So that will be our model formulation:

```{r}
#remember to select the training data subset we defined above! 
model <- lm(life ~ daily_cigarettes, data = train)
```


### Modelling results

By calling `lm` we have already trained our model! Lets have a look at the results. The summary gives us a lot of information about the model we trained:

```{r}
# View model summary
summary(model)
```

It beings with **Call** which displays the formula used to fit the model. 

The **Residuals** section summarizes the distribution of the residuals, which is the difference between the actual observed $y$ values and the fitted $y$ values.  

The **Coefficients** table shows the estimated values for each coefficient including the intercept, along with their standard errors, t-values, and p-values. These help to determine the significance of each predictor. Smaller p-values indicate stronger evidence against the null hypothesis that the true coefficient is zero. 


In the bottom section we have some information about how well model fits the training data. The **Residual Standard Error (RSE)** provides a measure of accuracy as it represents the average size of the residuals. The **R-squared** value indicates the proportion of variance explained by the model, with the Adjusted R-squared accounting for the number of predictors to prevent overfitting. Finally, the **F-statistic** and its p-value test whether the model as a whole explains a significant portion of the variance in the response variable (the outcome $y$). 

Overall, the `summary` helps us to assess the model fit and identify significant predictors and their effect size (size of the coefficient). 

We can extract the `model` object's components with `$`:

```{r}
model$coefficients

hist(model$residuals, breaks = 30, main = 'Histogram of residuals', 
     xlab = 'Residual')

```

### Model interpretation

What do these results mean? Our model formulation is: 

$$life=b_0+b_1*cigarettes$$

And we estimated these values:

```{r}
model$coefficients
```


Therefore:

* The intercept $b_0$ is the number of years we estimated a person in this dataset will live if they smoke 0 cigarettes. It is 78.7 years

* The coefficient of cigarettes per day is -0.28. This means for every 1 unit increase in cigarettes (one additional cigarette per day) the life expectancy decreases by 0.28 years.  

### Model performance

We now use our held out test data to evaluate the model performance. For that we will predict life expectancy for the 25 observations in `test` and compare with the actual values.

```{r}
#use the fitted model to make predictions for the test data
y_pred <- predict(model, test)
y_pred
```

Let's see how that fits with the known values.

```{r}
pred <- tibble(pred = y_pred, real = test$life)

ggplot(pred, aes(x=real, y=pred)) +
  geom_point()
```

Not too bad! We usually calculate the mean square error (mse) between predictions and the known true values to numerically evaluate regression performance:

```{r}
mse(pred$real,pred$pred)
```

Our predictions are on average 1.7 years wrong. 

## Regression with categorical features 

Now that we know how to make a simple linear model, how can we include categorical variables and what is the interpretation of their coefficients? To investigate this we include the other predictor variable we have: Exercise level.

```{r}
distinct(df_smoke, exercise)
```

Alright, we have three different levels of exercise. They are: low == 0, moderate == 1 and high == 2. Before we go on, let's have a look if our data is represented correctly:

```{r}
str(df_smoke)
```

We can see that the `exercise` column is interpreted as an integer. However, it is actually a category! In R categorical variables are known as `factors` and have their own datatype. Let's convert exercise to a factor:

```{r}
df_smoke$exercise <- as.factor(df_smoke$exercise)
str(df_smoke)
```


As before, before fitting the model we'll split up the data in train and test. Since we're using the same seed we should get the same observations, i.e. rows into training and test as above. 

```{r}
# Set seed to ensure reproducibility
set.seed(123)  

#add an ID column to keep track of observations
df_smoke$ID <- 1:nrow(df_smoke)

train <- df_smoke %>% sample_frac(.75)
test  <- anti_join(df_smoke, train, by = 'ID') 
```

And now we extend our previous model formula with the new predictor:

```{r}
model2 <- lm(life ~ daily_cigarettes + exercise, data = train)
```

```{r}
summary(model2)
```

When we check the summary we see that it has two additional coefficients, `exercise1` and `exercise2`. What are they?

Because `exercise` is a categorical variable it is dummy coded. That means our model formula mathematically looks something like this:

$$y=b_0+b_1*x_1 + b_2 *x_2 + b_3*x_3$$

with:


| Exercise level        | $x_2$           | $x_3$  |
|:---------------:|:-------------:|------:|
| 0   | 0      | 0   |
| 1   |  1     |   0 |
| 2   | 0      |  1  |

And for our coefficients it means:

```{r}
model2$coefficients
```
* `Intercept` == $b_0$: The life expectancy at 0 cigarettes and exercise level 0

* `daily_cigerettes` == $b_1$: The change in life expectancy for each additional cigarette.

* `exercise1` == $b_2$: The change in life expectancy if the exercise level is 1 (assuming the number of cigarettes stays constant).

* `exercise2` == $b_3$: The change in life expectancy if the exercise level is 2 (assuming the number of cigarettes stays constant).

Why is there no coefficient for exercise level 0 (low amount of exercise)? This case is covered in the Intercept. It is referred to as the **reference level** of the categorical variable. You can change which level is regarded as the reference and the effect of having this level will always be modelled in the intercept. 

## Classification

Classification is what we apply when the outcome has two or more classes.

In order to have a categorical outcome, we'll add a column to our toy data that describes whether the person died before age 75 or not. 

```{r}
df_smoke <- df_smoke %>%
  mutate(early_death = factor(ifelse(life < 75, 'yes', 'no')))

df_smoke %>%
  count(early_death)
```

### Training and Test set with class data

Let's remake our training and test data. This time we have classes that we would like to be in the same ratios in training and test set. Therefore, we cannot just grab 75% of the data as we did before. We'll use `sample.split` from `caTools` to achieve balanced classes: 

```{r}
# Set seed to ensure reproducibility
set.seed(123)

split <- sample.split(df_smoke$early_death, SplitRatio = 0.75)

#split is a vector of true and false values we can now directly apply to our tibble
split

train <- df_smoke[split,]
test <- df_smoke[!split,] #! negates the vector, so true becomes false and vice verse

count(train,early_death)
count(test, early_death)

```


Now we can perform logistic regression to see whether there is an influence of the number of cigarettes and amount of exercise on the odds of the person dying before 75. 

Logistic regression belongs to the family of generalized linear models. They all look like this:

$$ y \sim \beta * X $$

with:

* $y$ the outcome
* $\beta$ the coefficient matrix
* $X$ the matrix of predictors
* $\sim$ the link function

In a logistic regression model the link function is the logit. In a linear model the link function is the identity function (so ~ becomes =). 

### Logistic regression: Math

In order to understand what that means we'll need a tiny bit of math.

We see some issues right of the bat. Our $y$ is either 0 or 1 (the person is either dead or not). However we cannot model that so instead we will model the probability of the outcome being 1: $P(earlydeath == 1)$. Except probabilities are bounded between 0 and 1 which is mathematically difficult to impose (it means all $y$'s have to be between these two values and how are we gonna enforce that?) So instead, we will model the log-odds of early death:

$$ y = \log(\frac{P(earlydeath == 1)}{1-P(earlydeath == 1)})$$

It may not look like it but we promise you this $y$ is a well behaved number because it can be anywhere between - infinity and + infinity. So therefore our actual model is:

$$ \log(\frac{P(earlydeath == 1)}{1-P(earlydeath == 1)} =  \beta * X$$

And if we want to know what the means for the probability of dying we just take the logit of $y$ :

$$ P(earlydeath == 1) = \frac{1}{1+ e^{(-y)}} $$ 

Which makes the link between what we're actually interested in (people's chances of dying) and what we're modelling the logit. End of math.

### Model formulation in R

So in order to fit a logistic regression we will use the function for generalized linear models, `glm`. We will specify that we want logistic regression (using the logit as the link) by setting `family = binomial`:


```{r}
model_log <- glm(early_death ~ daily_cigarettes + exercise, data = train, family = "binomial")
summary(model_log)
```

### Model interpretation

We see from looking at the summary that the coefficient of exercise level 1 and level 2 is not significant. This means that we are not confident that doing a high amount of exercise (level 3) has a significant impact on the probability of dying before 75 compare to doing moderate or low amounts of exercise. This does not mean that there can be no influence, merely that we do not have enough data to detect it if it is there. 

Are you surprised? 
Exercise level was significant when we modelled the number of years lived, which is arguably a more fine-grained information than the binary split and perhaps therefore we picked up the influence.

With the number of daily cigarettes predictor we have a high degree of certainty that it influences the probability of dying before 75 (in this dataset!), but what does a coefficient of 0.84 mean? 

We know that: 

$$ P(earlydeath == 1) = \frac{1}{1+ e^{(-y)}} $$ 

and (leaving out the exercise level since it's not significant):

$$ y = \beta_0 + \beta_1 * cigs $$

So how does $y$ change as $0.84 * cigs$ becomes larger? Let's agree that $y$ becomes larger. What does that mean for the probability of dying? Is $e^{(-y)}$ a large number if $y$ is large? Luckily we have a calculator handy

```{r}
#exp(b) is e^b in R

exp(-1)

exp(-10)

exp(-100)
```

We see that $e^{(-y)}$ becomes increasingly smaller with larger $y$ which means that 

$$ P(earlydeath == 1) = \frac{1}{1+ small} \sim \frac{1}{1} $$ 

So the larger $y$ the smaller $e^{(-y)}$ and the closer we get to $P(earlydeath == 1)$ being 1. That was a lot of math for: If the coefficient is positive you increase the likelihood of getting the outcome, i.e. dying. 


## Clustering



