---
title: "Presentation 2: Summary Statitics, Data Wrangling and Plotting"
format: html
project:
 type: website
 output-dir: ../docs
---

Now that we've cleaned our data, it’s time to dig deeper into the actual contents of our dataset. Exploratory Data Analysis (EDA) is an essential step in any data analysis or modeling workflow.

EDA is about getting an overview of your data before fitting it to any models. It helps us:

-   Understand **distributions**

-   Spot **outliers**

-   Reveal **patterns**

-   Explore **relationships** between variables

There are three broad ways to investigate variables:

1.  **Univariate** – Examining one variable at a time.

2.  **Bivariate** – Exploring relationships between two variables.

3.  **Multivariate** – Looking at relationships among multiple variables.

## Load Packages and Data

Let’s begin by loading the packages we’ll need for data wrangling and plotting:

```{r warning=FALSE, message=FALSE}
library(tidyverse) 
# library(ggforce) 
```

Now, we load our dataset which was pre-processed and saved as an `.RData` file at the end of Presentation 1:

```{r message=FALSE}
load("../data/Ovarian_comb_clean.RData")

# Quick peek at the structure and contents
class(df_comb)
str(df_comb)
```

Depending on how we want to investigate the variables, there are different tools in your EDA toolkit we can employ:

Univariate Analysis

-   **Histograms** – Check distributions of numeric variables

-   **Boxplots** – Detect outliers and skewness

-   **Bar charts** – Explore frequency of categorical variables

-   **Summary stats** – Mean, median, standard deviation (SD), min, max, interquartile range (IQR)

Bivariate Analysis

-   **Scatter plots** – Explore relationships between two numeric variables

-   **Correlation coefficients** – Assess linear strength (e.g., Pearson’s *r*)

-   **Contingency tables** – Analyze relationships between categorical variables

-   **Line graphs** – Track trends over time

## Data Overview and `ggplot2` Recap

Let’s revisit some of the variables in the dataset and refresh our `ggplot2` skills.

If you need a detailed refresher, refer to the [From Excel to R: Presentation 3](https://center-for-health-data-science.github.io/FromExceltoR/Presentations/presentation3.html).

```{r warning=FALSE}
# Distribution of tumor cell percentage
ggplot(df_comb, 
       aes(x = percent_tumor_cells)) +
  geom_histogram(bins = 30) +
  theme_bw()

# Tumor percentage by summary grade
ggplot(df_comb, 
       aes(x = grade, 
           y = percent_tumor_cells, 
           fill = grade)) +
  geom_boxplot() +
  theme_bw() + 
  scale_fill_viridis_d(na.value = "grey50")

# Count with barplot
ggplot(df_comb, 
       aes(x = grade, 
           fill = grade)) +
  geom_bar() +
  theme_bw() + 
  scale_fill_viridis_d(na.value = "grey50")
```

These simple plots give us an initial overview of the distribution of some of the variables - on their own and stratified by groups.

### Pipe into `ggplot`

Tired of cluttering your environment with dozens of intermediate data frames? There’s a solution: you can pipe (`%>%`) your filtered data directly into `ggplot()` without saving it first. When you do this, you don’t need to specify the `data = ...` argument inside `ggplot()` — the data is being *piped* into the function automatically.

```{r warning=FALSE}
df_comb %>%
  filter(sample_recode  == "Tumor") %>%
  ggplot(aes(x = CXCL11, 
             y = CXCL10, 
             color = summarygrade)) + 
  geom_point() +
  scale_fill_viridis_d(na.value = "grey50") +
  theme_bw()
```

### Plotting several dataframes

Sometimes we want to add extra layers of information to a plot — for example, combining raw data distributions with summary statistics.

In this case, we’ll create a violin plot to show the distribution of the `age_at_initial_path_diagn` values stratified by the `summarygrade` categories. At the same time, we want to overlay the mean `age_at_initial_path_diagn` value for each `summarygrade` category as a red point.

We start by summarizing our data to get the mean `age_at_initial_path_diagn` for each `summarygrade` category:

```{r}
gene_mean <- df_comb %>%
  group_by(grade) %>%
  summarise(mean = mean(age_at_initial_path_diagn)) %>% 
  filter(!is.na(grade))

gene_mean
```

Now we plot the distributions with `geom_violin()` and add the mean values using a second data frame in `geom_point()`:

```{r warning=FALSE}
df_comb %>% 
  filter(!(is.na(age_at_initial_path_diagn) | is.na(grade))) %>% 
  ggplot(aes(x = grade, 
             y = age_at_initial_path_diagn, 
             fill = grade)) +
  geom_violin() +
  geom_point(data = gene_mean, aes(x = grade, y = mean), color = 'red', size = 3) +
  scale_fill_viridis_d() +
  labs(title = 'Age at Initial Pathologic Diagnosis by grade', 
       caption = 'Only one individual in grade 4.') +
  theme_bw() 
```

`geom_point(data = gene_mean)` tells `ggplot` to use a **different data frame** for this layer — a powerful way to combine data sources in a single plot.

This makes it easy to visualize both **variation** and **central tendency** at the same time.

### Plots are objects

Just like data frames, `ggplot` plots are R objects. You can assign them to variables and display them later:

```{r}
awesome_plot <- df_comb %>% 
  filter(!(is.na(age_at_initial_path_diagn) | is.na(grade))) %>% 
  ggplot(aes(x = grade,
             y = age_at_initial_path_diagn,
             fill = grade)) +
  geom_violin() +
  geom_point(data = gene_mean, aes(x = grade, y=mean), color = 'red', size = 3) +
  scale_fill_viridis_d() + 
  labs(title = 'Age at Initial Pathologic Diagnosis by grade', 
       caption = 'Only one individual in grade 4.') +
  theme_bw() 
```

If R is being every pesky about showing you plots (e.g. if you want to display them in a loop) wrapping `print()` around the name of the plot object usually helps:

```{r warning="FALSE"}
print(awesome_plot)
```

This is especially useful for creating multiple plots programmatically.

## Formats: long and wide

The doctrine of `ggplot` is that every information you want to plot must be in a column. There is one column that describes the x-axis, one for the y-axis, and one for each additional aesthetic like color, size, shape, ect.

Now, let’s say we want to create a single plot and compare several **CXCL gene expression levels** across different categories.

In the current **wide format** (like most spreadsheets), each gene is stored in its own column:

| ID  | CXCL1 | CXCL2 | CXCL3 | ... |
|-----|-------|-------|-------|-----|

This layout is fine for browsing spreadsheets, but to create the plot described above you’d have to write a separate plot for each gene/column — not efficient.

To plot and analyze more effectively, we need to reshape the data into **long format**, where each row represents a single observation:

| ID  | gene  | value |
|-----|-------|-------|
| 1   | CXCL1 | 2.3   |
| 1   | CXCL2 | 1.9   |
| 2   | CXCL1 | 3.1   |
| ... | ...   | ...   |

We can do this using `pivot_longer()`:

The data can be reformatted to long format such that there is one line per gene per person. For this we use the `pivot_longer` function.

```{r}
df_comb_long <- df_comb %>% 
  pivot_longer(cols = starts_with('CXCL'),
               names_to = "gene",
               values_to = "value")

df_comb_long %>% select(unique_patient_ID, age_at_initial_path_diagn, gene, value) %>% head() 

# one line per gene per person
nrow(df_comb)
nrow(df_comb_long)
df_comb_long$gene %>% unique() %>% length() * nrow(df_comb)
```

### The Long Format is ggplot’s Best Friend

With the reshaped `df_comb_long`, we can now create one **combined plot** that shows distributions for **all genes** in **a single** `ggplot` call:

```{r}
ggplot(df_comb_long, 
       aes(x = gene, y = value, fill = gene)) +
  geom_boxplot() +
  scale_fill_viridis_d() + 
  theme_minimal() 
```

Want histograms for all genes?

```{r}
ggplot(df_comb_long, 
       aes(x = value)) +
  geom_histogram(bins = 30) +
  facet_wrap(vars(gene), nrow = 3) + 
  theme_minimal()
```

This plot gives us a histogram for each gene, all in one go.

-   No need to write separate plots manually.

-   Much easier to compare variables side-by-side.

More context: add color-stratification by `summarygrade` and compare distributions side-by-side:

```{r}
ggplot(na.omit(df_comb_long), 
       aes(x = gene, y = value, fill = summarygrade)) +
  geom_boxplot() + 
  scale_fill_viridis_d() + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
  
```

### Pivot back into wide format

The `pivot_wider` function is used to transform data to wide format.

```{r eval=FALSE}
df_comb_wide <- df_comb_long %>% 
  pivot_wider(names_from = gene, 
              values_from = value)

head(df_comb_wide)
head(df_comb)
```

## Exploring Factors (Categorical Variables)

When working with **factor variables**, we often want to know:

-   Which levels (categories) exist

-   Whether groups are **balanced**

-   How **missingness** overlaps across factors

Let's have a look at the `vital_status` variable. It looks fairly balanced.

```{r}
table(df_comb$vital_status, useNA = "ifany")

df_comb %>% 
  count(vital_status)
```

Now, let's see how the `vital_status` levels are distributed across the `tumorstage` levels.

```{r}
table(df_comb$vital_status, df_comb$tumorstage, useNA = "ifany")
```

### Pairwise Relationships Between Categorical Variables

We will now have a look at the distribution of levels of the factor variables across `vital_status`.

```{r}
# Keep only factor variables including vital_status
df_comb %>% 
  select(where(is.factor), vital_status) %>% 
  drop_na() %>% 
  pivot_longer(cols = -vital_status, names_to = "variable", values_to = "value") %>% 
  # Plot faceted bar plots colored by vital_status
  ggplot(aes(x = value, fill = vital_status)) +
  geom_bar() +
  facet_wrap(vars(variable), scales = "free_x") +
  scale_fill_viridis_d() +
  theme_minimal() 
```

#### After reviewing our categorical variables, we observed the following:

-   **`Tumorstage` and `grade` are imbalanced** — with certain stages and grades (e.g., 1 or 4) having low sample counts. This limits our ability to make reliable comparisons across all detailed levels.

    -   To address this, we may use broader categories:

        -   *Early vs. Late Stage* (e.g., Stage 1–2 vs. 3-4)

        -   *Low vs. High Grade* (e.g., Grade 1–2 vs. 3–4)

-   **`Batch` is evenly represented**, which reduces concerns about batch effects in our analysis.

-   **`Vital_status` is nicely balanced** making it well-suited for comparisons and modeling.

These insights will help us structure our downstream analysis in a statistically sound and interpretable way.

## Summary Statistics

Even after initial data cleaning and harmonization, it’s important to make sure our variables are well-behaved and ready for downstream analysis and modeling.

Instead of looking at individual variable, we’ll introduce some tidyverse helper functions that make summarizing variables much more efficient and scalable — so you don’t have to write repetitive code for every column.

### Tidyverse helpers: `across()` and `where()`

In this section we will say a bit more about the so called tidyverse helpers such as `across()`, `where()`, and `starts_with()`. These helpers are useful when we want to apply a functions, i.e. `summarise()` or `mutate()` to several columns.

### Using `across()` to select columns

Let’s say we want to compute the mean of several columns. A basic (but tedious) approach might look like this:

```{r}
df_comb %>%
  summarise(mean_CXCL1 = mean(CXCL1),
            mean_CXCL2 = mean(CXCL2),
            mean_CXCL3 = mean(CXCL3),
            mean_CXCL5 = mean(CXCL5),
            mean_age_at_diagn = mean(age_at_initial_path_diagn))
```

This works, but we need to name every column we want to apply `summarise` to. It’s verbose and error-prone — especially if you have dozens of variables.

Enter `across()`: lets us select the columns which we want to apply `summarise` **across** in a dynamic fashion:

```{r warning=FALSE, message=FALSE}
df_comb %>%
  summarise(across(.cols = everything(), # Columns to run fuction on 
                   .fns = mean)) %>% # Function 
  select(15:25)
```

Another useful example:

```{r eval=FALSE}
df_comb %>% 
  summarise(across(.cols = everything(), 
                   .fns = ~ sum(is.na(.))))
```

### Using `where()` to Select Numeric Columns

We will probably not want to calculate means on non-numeric columns, so let's select only numeric columns. For that we need another helper caller `where()` that lets us select columns based on their properties, like data type.

```{r}
df_comb %>%
  summarise(across(.cols = where(fn = is.numeric), # fn is a function that returns TRUE or FALSE
                   .fns = mean)) %>% 
  select(5:15)
```

### Other Column Selection Helpers

There is another group of helpers that refers to column names:

-   `starts_with()`
-   `ends_with()`
-   `contains()`

And we can use them to select only columns starting with 'CXC':

All these ways can be used to select columns in tidyverse, also outside of `across()`.

```{r}
# Columns that start with "CXC"
df_comb %>%
  summarise(across(starts_with('CXC'), mean))
```

Lastly, we can always straight up supply the names of the columns we want to select in a vector:

```{r}
# Specific columns listed manually
df_comb %>%
  summarise(across(c(CXCL3, CXCL5), mean))
```

These helpers make your code cleaner, more scalable, and easier to maintain.

### `summarise()` becomes more powerful!

So far, we've only applied a single function. But why stop at the mean? What if you want **multiple statistics** like mean, SD, min, and max — all in one go?

With `across()`, you can pass a **list of functions**:

```{r}
df_comb %>%
  summarise(across(.cols = starts_with("CXC"), 
                   .fns = list(mean, sd, min, max)))
```

This gives you one wide row per column, with new columns like `CXCL1_1`, `CXCL1_2`, etc. A bit cryptic, right?

Let’s clean it up by naming the functions and columns:

```{r}
gene_summary <- df_comb %>%
  summarise(across(.cols = starts_with("CXC"), 
                   .fns = list(mean = mean, 
                               sd = sd, 
                               min = min, 
                               max = max),
                   .names = "{.col}-{.fn}"))

gene_summary
```

Much better! Now the column names are readable and include both the variable and the statistic.

Still not your preferred format?

You can probably pivot your way out of that!

```{r}
gene_summary %>%
  pivot_longer(cols = everything(), 
               names_to = c("gene", "statistic"), 
               names_sep = "-") %>%
  pivot_wider(names_from = statistic, 
              values_from = value)
```

Now you get a long format table with one row per variable and all your stats in columns — clean, tidy, and ready for interpretation.

![](../figures/satisfying_meme.jpg){width="200px"}

### The anonymous function: `~` and `.`

We promised to get back to handling the `NA`s when doing summary stats. Let's just add the `na.rm=TRUE` argument. To not have too many things going on at once we'll only do `mean()` for now:

```{r error=TRUE}
df_comb %>%
  summarise(across(.cols = where(is.numeric), 
                   .fns = list(mean = mean(na.rm = TRUE)),
                   .names = "{.col}-{.fn}"))
```

Brrrtt! We may not.

Why doesn’t this work?

When you pass functions directly into `across()` using the **shorthand syntax** (`mean`, `sd`, etc.), you're only allowed to use the **bare function with default arguments**. You will also notice that we didn't use brackets after their names, which is part of using the function shorthanded. Once you try to add something like `na.rm = TRUE`, the shorthandness breaks.

To pass arguments to a function that is called inside another function (i.e. calling `mean` inside `summarise`), we need to use what’s called an **anonymous function**. Don't worry — it’s not as scary as it sounds.

It is written as a `~` and looks like this:

```{r}
df_comb %>%
  summarise(across(.cols = where(is.numeric), 
                   .fns = list(mean = ~ mean(., na.rm = TRUE)),
                   .names = "{.col}-{.fn}"))
```

Let’s break it down:

-   `~` to define the function. `~ mean(., na.rm = TRUE)` tells R:\
    *“for each column, compute the mean, ignoring NAs”*

-   `.` is a placeholder for the current column being operated or 'the data previously referred to'. We need to use the `.` because `mean` when called as a proper function needs to have an argument (a vector of numbers) to work on.

Compare it to this:

```{r}
mean(df_comb$age_at_initial_path_diagn, na.rm = TRUE)
```

In both cases, the column must be explicitly passed in and `.` serves that role inside anonymous functions.

### Multiple Summary Statistics with `na.rm = TRUE`

Now let’s compute **several** stats per column, all with `na.rm = TRUE`. Here we chose just the integer columns:

```{r}
stats <- df_comb %>%
  summarise(across(.cols = where(is.integer),
                   .fns = list(mean = ~ mean(., na.rm = TRUE),  
                               sd = ~ sd(., na.rm = TRUE), 
                               min = ~ min(., na.rm = TRUE), 
                               max = ~ max(., na.rm = TRUE)),
                   .names = "{.col}-{.fn}")) %>%
  # add reformating
  pivot_longer(cols = everything(), 
               names_to = c("variable", "statistic"), 
               names_sep = "-") %>%
  pivot_wider(names_from = statistic, 
              values_from = value) 

print(stats)
```

This produces a tidy table with one row per gene and columns for `mean`, `sd`, `min`, and `max`.

The helper functions can be used in other tidyverse operation such as `mutate` and `select`.

## Handling Outliers

Now that we’ve explored summary statistics, it's time to consider **outliers** — data points that differ markedly from the rest. Outliers can be caused by measurement or entry errors, natural variability or rare events or anomalies.

Outliers can skew your summary statistics, distort visualizations and can affect model performance, especially in regression.

Let’s use our tidyverse skills to **calculate some basic stats** and **visualize potential outliers**.

```{r warning=FALSE}
stats <- stats %>% 
  mutate(threshold_upper = mean + 2 * sd,
         threshold_lower = mean - 2 * sd) %>%
  pivot_longer(cols = c(threshold_lower, threshold_upper), names_to = "threshold_type", values_to = "threshold")

print(stats)


df_comb_longer <- df_comb %>% 
  pivot_longer(cols = where(is.integer),
               names_to = "variable",
               values_to = "value")

print(df_comb_longer)
```

```{r}
ggplot(df_comb_longer, 
       aes(x = value)) +
  geom_histogram(bins = 30) +
  geom_vline(data = stats, 
             aes(xintercept = threshold),
             color = "red", linetype = "dashed") +
  facet_wrap(vars(variable), ncol = 2, scales = "free") + 
  theme_minimal()
```

Sometimes, visualizing values in relation to **other variables** can make potential outliers easier to spot. Below, you’ll notice that a patient with 0% cancer cells and 0% tumor cells likely represents an outlier:

```{r warning=FALSE}
# Bivariate scatter plot colored by stromal cell percentage
ggplot(df_comb, 
       aes(x = percent_tumor_cells, y = percent_not_cancer_cells, color = percent_stromal_cells)) +
  geom_point() + 
  scale_color_viridis_c() +
  theme_bw()
```

There are many different tools and strategies available to detect and handle outliers. However, in this case, we’ll keep it simple: we will remove patients where both `percent_tumor_cells` and `percent_not_cancer_cells` are less than 5%.

```{r warning=FALSE}
df_comb <- df_comb %>%
  mutate(isOutlier = (percent_tumor_cells < 5 & percent_not_cancer_cells < 5))

df_comb %>%
  filter(isOutlier == TRUE)

df_comb <- df_comb %>% 
  filter(isOutlier == FALSE | is.na(isOutlier))
```

> **Be cautious when removing data**: Outlier removal should always be guided by domain knowledge and clear justification. Removing too much — or for the wrong reasons — can distort your analysis.


## Sample outliers across variables

Instead of visualizing variables one by one, we can also look at samples across a set of variables and identify outliers that way. If we have a lot of integer and numeric values in our dataset, one thing that might make sense is to make a dendogram (tree) using hierarchical clustering and identify if any of the samples 'stand out'.

Let's try this for the integer variables in our dataset. 

We first select all integer columns, drop missing values (they are non-informative for clustering anyways). Then, we make a plot to get an idea of data distributions:
```{r}
df_int <- df_comb %>%
  select(where(is.integer)) %>%
  drop_na()

df_int %>% head()

df_int %>% 
  pivot_longer(cols = where(is.integer),
               names_to = "variable",
               values_to = "value") %>%
  ggplot(., aes(x = value)) +
  geom_histogram(color= 'white') +
  facet_wrap(vars(variable), ncol = 2, scales = "free") + 
  theme_minimal()
```

Next, we scale our data (more on this in Presentation 5.) and calculate the distances between all pairs of scaled variables and perform hierarchical clustering.

Do you not see the plot clearly? Copy-paste the code into the console and the plot will appear in the *Plots* window. Press *Export* --> *Copy to Clipboard...* and drag out the plot as large as you can. Alternatively, save it in a large format and look at it outside of R studio. 
```{r}
# Euclidean pairwise distances
df_int_dist <- df_int %>% 
  mutate(across(everything(), scale)) %>%
  dist(., method = 'euclidean')

# Hierarchical clustering with Ward's distance metric
hclust_int <- hclust(df_int_dist, method = 'ward.D2')

# Plot the result of clustering
# png("dendrogram_plot.png", width = 2000, height = 500)
plot(hclust_int, main = "Clustering based on scaled integer values", cex = 0.7)
# dev.off()
```

From our plot we see that a couple of samples seem to have very short branch lengths (which may indicate an outlier). One such sample is 208 (row id). Lets have a look at it:

```{r}
df_int <- df_comb %>%
  select(where(is.integer)) %>%
  drop_na() 

df_int[208, ]
```
We can now compare the values of this sample to the distributions above (histograms) to try an figure out what makes it an outlier and IF it should be removed. 
For the sample in row 208, age at diagnosis and days to death is with in normal range, though percent stromal and not tumor cell is in the very high end while percent tumor cells is in the very low end. These are pretty extreme values for several of the variables - why one would consider removing this sample. 

## Handling Missing Data

Missing data is common in most real-world datasets and can significantly affect the quality of our analysis. During EDA, it's essential to **identify**, **understand**, and **properly handle** missing values to avoid biased or misleading conclusions.

Missing data can fall into three categories:

-   **Missing Completely At Random (MCAR)**: The missingness is independent of both the observed and unobserved data.

-   **Missing At Random (MAR)**: The missingness is related to the observed data.

-   **Missing Not At Random (MNAR)**: The missingness depends on the observed values themselves.

Understanding which category your data falls into helps guide how to deal with it. Let’s inspect rows with the most missing values:

```{r}
df_comb %>%
  mutate(na_count = rowSums(is.na(.))) %>%
  arrange(desc(na_count)) %>%
  slice_head(n = 10)
```

Here we notice that most of the missing values come from samples where the patients were  (**healthy (sample_recode)** - this points to either MAR or MNAR. Unless however we have evidence that, missingness is directly related to the observed values themselves, we prefer to assume MAR.

There are two main approaches when deciding what to do:

-   **Remove rows** with missing values — useful when the proportion is small and missingness is MCAR.

-   **Impute values** — helps preserve sample size and may improve robustness, especially when missingness is MAR.

Consider the impact of missing data. Even after imputing, missing data can cause uncertainty and bias so understands the result with caution.

> **Caution**: Even after imputation, results may be biased. Always interpret findings with an understanding of how missing data was handled.

Since we have very few samples with a high number of missing values AND we are not interested in working with these likely either misclassified or mislabled samples in our analysis, we can safely them:

```{r}
df_comb <- df_comb %>% filter(sample_recode == "Tumor")
```

We’ll keep the remaining `NA`s as-is for now, depending on context.

<br> <br>



## In Summary

Key Takeaways

-   Long format is your **best friend** when plotting or modeling.

-   Use `across()` and helpers to write **clean, scalable summaries**.

-   Visual checks are as important as statistical checks.

-   **EDA is not optional** — it’s the foundation of trustworthy analysis.

-   Keep your workflow **reproducible** and **tidy** for future modeling stages.

We have conducted an initial **exploratory data analysis (EDA)** and overall, the dataset looks clean and well-structured.

-   We identified and **removed a few samples**, as they had many missing values.

-   One sample was also removed due to inconsistencies in cell type proportions.

-   We observed that the categories within stage and grade are **not fully balanced**, therefore we decided to use the **summary stage** and **summary grade** variables

```{r}
save(df_comb, file="../data/Ovarian_comb_prep.RData")
```
