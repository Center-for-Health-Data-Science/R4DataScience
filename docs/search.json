[
  {
    "objectID": "about_heads.html",
    "href": "about_heads.html",
    "title": "About HeaDS",
    "section": "",
    "text": "In the Center for Health Data Science (HeaDS) we do both research and in the DataLab and Sandbox we develop and host course. Read more about all the cool stuff we do on our website.\n\n\nThe DataLab offers a range of services to support SUND employees in their data science analyses. Here’s an overview:\n\nCourses: We offer data science and bioinformatics courses for all SUND staff (researchers, administrative staff, technical staff, etc.). Our most popular courses include “From Excel to R,” “Python Tsunami,” and “Introduction to Bulk RNA-seq Analysis.”\nConsultations: We host drop-in sessions every Thursday from 13:00 to 15:00, where we assist with data science-related challenges and questions. Alternatively, you can reach out to arrange a one-on-one meeting on another day.\nCommissions: We conduct commissioned research, such as bioinformatics and data science analyses tailored to your needs. Additionally, we offer a commissioned supervision scheme if you would like to learn how to perform the analysis yourself.\nEvents: We host various data science events. Join us for our seminar series, “Talking HeaDS.” Our next session is in the fall (date and speeker TBA). Join Health Data Science Day i November: https://eventsignup.ku.dk/hdsd2025."
  },
  {
    "objectID": "about_heads.html#center-for-health-data-science",
    "href": "about_heads.html#center-for-health-data-science",
    "title": "About HeaDS",
    "section": "",
    "text": "In the Center for Health Data Science (HeaDS) we do both research and in the DataLab and Sandbox we develop and host course. Read more about all the cool stuff we do on our website.\n\n\nThe DataLab offers a range of services to support SUND employees in their data science analyses. Here’s an overview:\n\nCourses: We offer data science and bioinformatics courses for all SUND staff (researchers, administrative staff, technical staff, etc.). Our most popular courses include “From Excel to R,” “Python Tsunami,” and “Introduction to Bulk RNA-seq Analysis.”\nConsultations: We host drop-in sessions every Thursday from 13:00 to 15:00, where we assist with data science-related challenges and questions. Alternatively, you can reach out to arrange a one-on-one meeting on another day.\nCommissions: We conduct commissioned research, such as bioinformatics and data science analyses tailored to your needs. Additionally, we offer a commissioned supervision scheme if you would like to learn how to perform the analysis yourself.\nEvents: We host various data science events. Join us for our seminar series, “Talking HeaDS.” Our next session is in the fall (date and speeker TBA). Join Health Data Science Day i November: https://eventsignup.ku.dk/hdsd2025."
  },
  {
    "objectID": "tdhh/solution_not_up.html",
    "href": "tdhh/solution_not_up.html",
    "title": "Solution not up yet!",
    "section": "",
    "text": "Solutions will be available only after you’ve had a chance to work through the exercises on your own. If you’re unsure how to proceed, check the slides, cheat sheets, ask a peer, or reach out to a TA.\n\nEnjoy!"
  },
  {
    "objectID": "tdhh/tdhh5A.html",
    "href": "tdhh/tdhh5A.html",
    "title": "Exercise 5A - Solutions: Intro to Regression in R",
    "section": "",
    "text": "Load packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggfortify)\nlibrary(factoextra)\nlibrary(readxl)"
  },
  {
    "objectID": "tdhh/tdhh5A.html#part-1-linear-regression",
    "href": "tdhh/tdhh5A.html#part-1-linear-regression",
    "title": "Exercise 5A - Solutions: Intro to Regression in R",
    "section": "Part 1: Linear regression",
    "text": "Part 1: Linear regression\n\nLoad the data boston.csv\n\n\ndf &lt;- as_tibble(read.delim('../data/boston.csv', sep = ','))\nhead(df)\n\n# A tibble: 6 × 6\n     crim indus   nox    rm  medv neighborhood\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 0.00632  2.31 0.538  6.58  24   Suburban    \n2 0.0273   7.07 0.469  6.42  21.6 Urban       \n3 0.0273   7.07 0.469  7.18  34.7 Rural       \n4 0.0324   2.18 0.458  7.00  33.4 Urban       \n5 0.0690   2.18 0.458  7.15  36.2 Suburban    \n6 0.0298   2.18 0.458  6.43  28.7 Suburban    \n\n\n\nNeighborhood is a categorical variable. We could make it a factor but it will also work as a character column (in the case of using lm).\nSplit the dataset into test and training data.\n\n\n# Set seed to ensure reproducibility\nset.seed(123)  \n\n#add an ID column to keep track of observations\ndf$ID &lt;- 1:nrow(df)\n\ntrain &lt;- df %&gt;% sample_frac(.75)\ntest  &lt;- anti_join(df, train, by = 'ID') \n\n\nFit the model\n\n\nmodel &lt;- lm(medv ~ rm + crim + neighborhood, data = train)\n\n\nsummary(model)\n\n\nCall:\nlm(formula = medv ~ rm + crim + neighborhood, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.920  -3.167  -0.468   2.746  35.052 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          -25.52560    3.08810  -8.266 2.43e-15 ***\nrm                     7.63165    0.49122  15.536  &lt; 2e-16 ***\ncrim                  -0.22845    0.03525  -6.482 2.86e-10 ***\nneighborhoodSuburban   0.07995    0.75050   0.107    0.915    \nneighborhoodUrban      3.66323    0.84058   4.358 1.70e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.183 on 375 degrees of freedom\nMultiple R-squared:  0.5573,    Adjusted R-squared:  0.5525 \nF-statistic:   118 on 4 and 375 DF,  p-value: &lt; 2.2e-16\n\n\n\nrm and crim have a significant influence on the house price. An increase in the number of rooms increases the price since the coefficient is positive, whereas an increase in crime rate reduces the price. There is a significant difference in price between Rural and Urban zones, but not between Rural and Suburban. Rural is the reference level. Lastly, houses with 0 rooms cost -25k dollar. Perhaps the predictors should be centered before fitting the model around 0 so rm == 0 is the average number of rooms for better interpretability.\nIf you wanted to know if there is a difference in the value of houses between the Suburban and Urban neighborhood what could you do to the variable neighborhood before modelling?\n\n\ndf &lt;- df %&gt;% \n  mutate(neighborhood= factor(neighborhood, levels=c(\"Urban\", \"Suburban\", \"Rural\")))\n\n\nFor linear regression there is an assumption that the model residuals (errors) are normally distributed. An easy way visualize this is by simply calling plot() on your model (see below). What do you think based on the plots?\n\n\n#RMSE\npar(mfrow=c(2,2))\nplot(model)\n\nOverall the plots look okay. There are some outlines, but all of them are within cooks distance, i.e. they are not extreme. The model residuals are close to normally distributed.\n\nNow, use our test set to predict the response medv (median value per house in 1000s).\n\n\ny_pred &lt;- predict(model, newdata = test)\n\n\nEvaluate how well our model performs. There are different ways of doing this but lets use the classic measure of RMSE (Root Mean Square Error).\n\n\n#RMSE\nrmse &lt;- sqrt(mean((test$medv - y_pred)^2))\n\nrmse\n\n\nMake a scatter plot to visualize how the predicted values fit with the observed values.\n\n\npredPlot &lt;- tibble(y_test = test$medv, y_pred=y_pred)\n  \nggplot(predPlot, aes(x = y_test, y=y_pred)) +\n  geom_point(alpha = 0.7, size = 2) +  # scatter points\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\", linewidth = 1) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "tdhh/tdhh5A.html#part-2-logistic-regression",
    "href": "tdhh/tdhh5A.html#part-2-logistic-regression",
    "title": "Exercise 5A - Solutions: Intro to Regression in R",
    "section": "Part 2: Logistic regression",
    "text": "Part 2: Logistic regression\nFor this part we will use the joined diabetes data since it has a categorical outcome (Diabetes yes or no). We will not use the oral Glucose measurements as predictors since this is literally how you define diabetes, so we’re loading the joined dataset we created in exercise 1, e.g. ‘diabetes_join.xlsx’ or what you have named it. N.B if you did not manage to finish making this dataset or forgot to save it, you can find a copy here: ../out/diabetes_join.Rdata. Navigate to the file in the file window of Rstudio and click on it. Click “Yes” to confirm that the file can be loaded in your environment and check that it has happened.\nAs the outcome we are studying, Diabetes, is categorical variable we will perform logistic regression. We select serum calcium levels (Serum_ca2), BMI and smoking habits (Smoker) as predictive variables.\n\nRead in the diabetes_join.xlsx dataset.\n\n\ndiabetes_join &lt;- read_xlsx(\"../data/exercise1_diabetes_join.xlsx\")\n\nhead(diabetes_join)\n\n# A tibble: 6 × 11\n  ID       Sex      Age BloodPressure   BMI PhysicalActivity Smoker  Diabetes\n  &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 ID_34120 Female    28            75  25.4               92 Never          0\n2 ID_27458 Female    55            72  24.6               86 Never          0\n3 ID_70630 Male      22            80  24.9              139 Unknown        0\n4 ID_13861 Female    56            72  37.1               64 Unknown        1\n5 ID_68794 Female    21            62  23                 82 Former         0\n6 ID_64778 Female    54            76  33.8               63 Smoker         1\n# ℹ 3 more variables: Serum_ca2 &lt;dbl&gt;, Married &lt;chr&gt;, Work &lt;chr&gt;\n\n\n\nLogistic regression does not allow for any missing values in the outcome variable. Ensure that the variable Diabetes does not have missing values AND that it is a factor variable.\n\n\ndiabetes_df &lt;- diabetes_join %&gt;% \n  filter(!is.na(Diabetes)) %&gt;%\n  mutate(Diabetes= as.factor(Diabetes))\n\n\nSplit your data into training and test data. Take care that the two classes of the outcome variables are represented in both training and test data, and at similar ratios.\n\n\nset.seed(123)  \n\n\ntrain &lt;- diabetes_df %&gt;% sample_frac(.75)\ntest  &lt;- anti_join(diabetes_df, train, by = 'ID') \n\n\ntrain_counts &lt;- train[,-1] %&gt;%\n  dplyr::select(where(is.character)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n  dplyr::count(Variable, Level, name = \"Count\")\n\ntrain_counts\n\n# A tibble: 12 × 3\n   Variable Level        Count\n   &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt;\n 1 Married  No             126\n 2 Married  Yes            230\n 3 Sex      Female         202\n 4 Sex      Male           154\n 5 Smoker   Former          80\n 6 Smoker   Never          112\n 7 Smoker   Smoker         115\n 8 Smoker   Unknown         49\n 9 Work     Private        189\n10 Work     Public         106\n11 Work     Retired          2\n12 Work     SelfEmployed    59\n\ntest_counts &lt;- test[,-1] %&gt;%\n  dplyr::select(where(is.character)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n  dplyr::count(Variable, Level, name = \"Count\")\n\n\ntest_counts\n\n# A tibble: 12 × 3\n   Variable Level        Count\n   &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt;\n 1 Married  No              39\n 2 Married  Yes             79\n 3 Sex      Female          63\n 4 Sex      Male            55\n 5 Smoker   Former          33\n 6 Smoker   Never           31\n 7 Smoker   Smoker          33\n 8 Smoker   Unknown         21\n 9 Work     Private         63\n10 Work     Public          35\n11 Work     Retired          2\n12 Work     SelfEmployed    18\n\n\n\nFit a logistic regression model with Serum_ca2, BMI and Smoker as predictors and Diabetes as outcome, using your training data.\n\n\nmod1 &lt;- glm(Diabetes ~ Serum_ca2 + BMI + Smoker, data = train, family = 'binomial')\nsummary(mod1)\n\n\nCall:\nglm(formula = Diabetes ~ Serum_ca2 + BMI + Smoker, family = \"binomial\", \n    data = train)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -29.66172    9.24103  -3.210  0.00133 ** \nSerum_ca2       0.62235    0.91791   0.678  0.49777    \nBMI             0.77975    0.09837   7.927 2.25e-15 ***\nSmokerNever    -1.33474    0.63225  -2.111  0.03476 *  \nSmokerSmoker    1.71880    0.67054   2.563  0.01037 *  \nSmokerUnknown  -0.27908    0.75901  -0.368  0.71310    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 493.34  on 355  degrees of freedom\nResidual deviance: 122.91  on 350  degrees of freedom\nAIC: 134.91\n\nNumber of Fisher Scoring iterations: 7\n\n\n\nCheck the model summary and try to determine whether you could potentially drop one of your variables? If so, make this alternative model and compare it to the original model. Is there a significant loss/gain, i.e. better fit when including the serum calcium levels as predictor?\n\n\nmod2 &lt;- glm(Diabetes ~ BMI + Smoker, data = train, family = binomial)\nsummary(mod2)\n\n\nCall:\nglm(formula = Diabetes ~ BMI + Smoker, family = binomial, data = train)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -23.84692    3.07881  -7.745 9.52e-15 ***\nBMI             0.78180    0.09873   7.918 2.41e-15 ***\nSmokerNever    -1.32566    0.63378  -2.092   0.0365 *  \nSmokerSmoker    1.64646    0.65738   2.505   0.0123 *  \nSmokerUnknown  -0.28021    0.75992  -0.369   0.7123    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 493.34  on 355  degrees of freedom\nResidual deviance: 123.38  on 351  degrees of freedom\nAIC: 133.38\n\nNumber of Fisher Scoring iterations: 7\n\n\n\nanova(mod1, mod2, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: Diabetes ~ Serum_ca2 + BMI + Smoker\nModel 2: Diabetes ~ BMI + Smoker\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       350     122.91                     \n2       351     123.38 -1 -0.46762   0.4941\n\n\n\nNow, use your model to predict Diabetes class based on your test set. What does the output of the prediction mean?\n\n\ny_pred &lt;- predict(mod2, test, type = \"response\")\n\n\nLets evaluate the performance of our model. As we are performing classification, measures such as mse/rmse will not work, instead we will calculate the accuracy. In order to get the accuracy you must first convert our predictions into Diabetes class labels (e.g. 0 or 1).\n\n\ny_pred &lt;- as.factor(ifelse(y_pred &gt; 0.5, 1, 0))\n\ncaret::confusionMatrix(y_pred, test$Diabetes)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 60  4\n         1  2 52\n                                          \n               Accuracy : 0.9492          \n                 95% CI : (0.8926, 0.9811)\n    No Information Rate : 0.5254          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.8979          \n                                          \n Mcnemar's Test P-Value : 0.6831          \n                                          \n            Sensitivity : 0.9677          \n            Specificity : 0.9286          \n         Pos Pred Value : 0.9375          \n         Neg Pred Value : 0.9630          \n             Prevalence : 0.5254          \n         Detection Rate : 0.5085          \n   Detection Prevalence : 0.5424          \n      Balanced Accuracy : 0.9482          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "tdhh/tdhh5A.html#part-3-k-means-clustering",
    "href": "tdhh/tdhh5A.html#part-3-k-means-clustering",
    "title": "Exercise 5A - Solutions: Intro to Regression in R",
    "section": "Part 3: K-Means Clustering",
    "text": "Part 3: K-Means Clustering\nIn this part we will run K-means clustering. To mix it up a bit we will work with a new dataset from patients with kidney disease. The dataset contains approximately 20 biological measures (variables) collected across 400 patients. The outcome is the classification variable which denotes whether a person suffers from ckd=chronic kidney disease ckd or not notckd.\n        age     -   age\n        bp      -   blood pressure\n        rbc     -   red blood cells\n        pc      -   pus cell\n        pcc     -   pus cell clumps\n        ba      -   bacteria\n        bgr     -   blood glucose random\n        bu      -   blood urea\n        sc      -   serum creatinine\n        sod     -   sodium\n        pot     -   potassium\n        hemo    -   hemoglobin\n        pcv     -   packed cell volume\n        wc      -   white blood cell count\n        rc      -   red blood cell count\n        htn     -   hypertension\n        dm      -   diabetes mellitus\n        cad     -   coronary artery disease\n        appet   -   appetite\n        pe      -   pedal edema\n        ane     -   anemia\n        class   -   classification  \n\nLoad in the dataset named kidney_disease.Rdata.\n\n\n#in case\nload(\"../data/kidney_disease.Rdata\")\n\n\nBefore running K-means clustering please remove rows with any missing values across all variables in your dataset - yes, you will lose quite a lot of rows. Consider which columns you can use and if you have to do anything to them before clustering?\n\n\n# Set seed to ensure reproducibility\nset.seed(123)  \n\n# remove any missing values\nkidney &lt;- kidney %&gt;%\n  drop_na() \n\n# scale numeric values and only use these\nkidney_num &lt;- kidney %&gt;%\n  mutate(across(where(is.numeric), scale)) %&gt;%\n  dplyr::select(where(is.numeric))\n\n\nRun the k-means clustering algorithm with 4 centers on the data. Look at the clusters you have generated.\n\n\n# Set seed to ensure reproducibility\nset.seed(123)  \n\n# run kmeans\nkmeans_res &lt;- kidney_num %&gt;% \n  kmeans(centers = 4, nstart = 25)\n\nkmeans_res\n\nK-means clustering with 4 clusters of sizes 119, 26, 1, 13\n\nCluster means:\n         age         bp        bgr         bu         sc        sod         pot\n1 -0.1606270 -0.1812766 -0.3421452 -0.3945903 -0.4097699  0.3696236 -0.09212654\n2  0.6371017  0.2644634  1.2990025  0.3971316  0.3647090 -0.9188501 -0.08119564\n3  0.1637295  1.4325099  1.4314972  2.4018426  1.1140015 -0.7849388 12.22513695\n4  0.1835566  1.0202582  0.4238246  2.6329989  2.9358598 -1.4853976  0.06530832\n        hemo        pcv         wc         rc\n1  0.4923705  0.4918043 -0.2332733  0.4495784\n2 -1.2670571 -1.2727402  0.9780463 -1.1415414\n3 -1.9462630 -2.0832161 -1.1005490 -1.9608496\n4 -1.8232572 -1.7961726  0.2639126 -1.6814542\n\nClustering vector:\n  [1] 2 4 2 2 2 4 2 2 2 2 4 4 4 2 1 2 2 2 3 4 1 2 2 4 1 2 2 2 2 2 2 4 2 4 4 2 2\n [38] 2 4 2 4 2 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 1 1\n\nWithin cluster sum of squares by cluster:\n[1] 409.4247 267.2635   0.0000 123.4990\n (between_SS / total_SS =  54.0 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\nkmeans_res$centers\n\n         age         bp        bgr         bu         sc        sod         pot\n1 -0.1606270 -0.1812766 -0.3421452 -0.3945903 -0.4097699  0.3696236 -0.09212654\n2  0.6371017  0.2644634  1.2990025  0.3971316  0.3647090 -0.9188501 -0.08119564\n3  0.1637295  1.4325099  1.4314972  2.4018426  1.1140015 -0.7849388 12.22513695\n4  0.1835566  1.0202582  0.4238246  2.6329989  2.9358598 -1.4853976  0.06530832\n        hemo        pcv         wc         rc\n1  0.4923705  0.4918043 -0.2332733  0.4495784\n2 -1.2670571 -1.2727402  0.9780463 -1.1415414\n3 -1.9462630 -2.0832161 -1.1005490 -1.9608496\n4 -1.8232572 -1.7961726  0.2639126 -1.6814542\n\n\n\nVisualize the results of your clustering. Do 4 clusters seems like a good fit for our data in the first two dimensions (Dim1 and Dim2)? How about if you have a look at Dim3 or Dim4?\n\n\nfviz_cluster(kmeans_res, data = kidney_num, axes=c(1,2),\n             palette = c(\"#2E9FDF\", \"#00AFBB\", \"#E7B800\", \"orchid3\"), \n             geom = \"point\",\n             ellipse.type = \"norm\", \n             ggtheme = theme_bw())\n\nToo few points to calculate an ellipse\n\n\n\n\n\n\n\n\n\n\nInvestigate the best number of clusters for this dataset. Use the silhouette metric.\n\n\nkidney_num %&gt;%\n  fviz_nbclust(kmeans, method = \"silhouette\")\n\n\n\n\n\n\n\n\n\nRe-do the clustering (plus visualization) with the optimal number of clusters.\n\n\n# Set seed to ensure reproducibility\nset.seed(123)  \n\n#run kmeans\n\nkmeans_res &lt;- kidney_num %&gt;% \n  kmeans(centers = 2, nstart = 25)\n\nkmeans_res\n\nK-means clustering with 2 clusters of sizes 120, 39\n\nCluster means:\n         age         bp       bgr         bu        sc        sod        pot\n1 -0.1826253 -0.1901587 -0.344100 -0.3887455 -0.409560  0.3622341 -0.0907221\n2  0.5619239  0.5851036  1.058769  1.1961401  1.260185 -1.1145665  0.2791449\n       hemo        pcv         wc         rc\n1  0.477263  0.4767479 -0.2093952  0.4450345\n2 -1.468501 -1.4669168  0.6442931 -1.3693368\n\nClustering vector:\n  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2\n [38] 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 1 1\n\nWithin cluster sum of squares by cluster:\n[1] 433.7458 647.1223\n (between_SS / total_SS =  37.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\nfviz_cluster(kmeans_res, data = kidney_num,\n             palette = c(\"#00AFBB\", \"#E7B800\"), \n             geom = \"point\",\n             ellipse.type = \"norm\", \n             ggtheme = theme_bw())\n\n\n\n\n\n\n\n\n\nNow, try to figure out what the two clusters might represent. There are different ways to do this, but one easy way would be to simply compare the clusters IDs from the Kmeans output with one or more of the categorical variables from the dataset. You could use count() or table() for this.\n\n\ntable(kidney$htn, kmeans_res$cluster)\n\n     \n        1   2\n  no  118   7\n  yes   2  32\n\ntable(kidney$dm, kmeans_res$cluster)\n\n     \n        1   2\n  no  119  12\n  yes   1  27\n\ntable(kidney$classification, kmeans_res$cluster)\n\n        \n           1   2\n  ckd      4  39\n  notckd 116   0\n\n\n\nThe withiness measure (within cluster variance/spread) is much larger for one cluster then the other, what biological reason could there be for that?"
  },
  {
    "objectID": "tdhh/tdhh4B.html",
    "href": "tdhh/tdhh4B.html",
    "title": "Exercise 4B - Solutions: Scripting in R - Functions",
    "section": "",
    "text": "In this exercise you will practice creating and applying user-defined functions.",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 4B: Functions"
    ]
  },
  {
    "objectID": "tdhh/tdhh4B.html#getting-started",
    "href": "tdhh/tdhh4B.html#getting-started",
    "title": "Exercise 4B - Solutions: Scripting in R - Functions",
    "section": "Getting started",
    "text": "Getting started\nLoad libraries and the joined diabetes data set.\n\nlibrary(tidyverse)\nlibrary(glue)\n\n\ndiabetes_glucose &lt;- readxl::read_excel('../data/exercise2_diabetes_glucose.xlsx')\ndiabetes_glucose\n\n# A tibble: 1,422 × 13\n   ID    Sex      Age BloodPressure   BMI PhysicalActivity Smoker  Diabetes\n   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 34120 Female    28            75  25.4               92 Never          0\n 2 34120 Female    28            75  25.4               92 Never          0\n 3 34120 Female    28            75  25.4               92 Never          0\n 4 27458 Female    55            72  24.6               86 Never          0\n 5 27458 Female    55            72  24.6               86 Never          0\n 6 27458 Female    55            72  24.6               86 Never          0\n 7 70630 Male      22            80  24.9              139 Unknown        0\n 8 70630 Male      22            80  24.9              139 Unknown        0\n 9 70630 Male      22            80  24.9              139 Unknown        0\n10 13861 Female    56            72  37.1               64 Unknown        1\n# ℹ 1,412 more rows\n# ℹ 5 more variables: Serum_ca2 &lt;dbl&gt;, Married &lt;chr&gt;, Work &lt;chr&gt;,\n#   Measurement &lt;chr&gt;, `Glucose (mmol/L)` &lt;dbl&gt;",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 4B: Functions"
    ]
  },
  {
    "objectID": "tdhh/tdhh4B.html#user-defined-functions",
    "href": "tdhh/tdhh4B.html#user-defined-functions",
    "title": "Exercise 4B - Solutions: Scripting in R - Functions",
    "section": "User defined Functions",
    "text": "User defined Functions\n\nCreate a function named calculate_risk_score().\n\nIt should accept the following parameters:\n\nBloodPressure\nBMI\nSmoking\nPhysicalActivity\n\nInitiate the risk score with a value of 0 and calculate the final risk score with the following rules:\n\nif BloodPressure is &gt; 90, add 0.5\nif Smoking is ‘Smoker’, add 1\nif BMI is &gt; 30, add 1. If BMI &gt; 40, add 2\nif PhysicalActivity is &gt; 110, substract 1\n\nThe function should return the final risk score. Test it with some different inputs to verify that it works according to the rules detailed above.\n\ncalculate_risk_score &lt;- function(BloodPressure, BMI, Smoking, PhysicalActivity){\n  \n  risk_score &lt;- 0\n  \n  if (BloodPressure &gt; 90){\n    risk_score &lt;- risk_score + 0.5\n    }\n\n  if (Smoking == 'Smoker'){\n    risk_score &lt;- risk_score + 1\n    }\n  \n  #BMI &gt; 30 includes BMIs that are larger than 40, so we need to be careful of\n  #the order in which we check, or include that the BMI should be &lt;= 40 in order \n  #to add 1.   \n  \n if (BMI &gt; 40){\n    risk_score &lt;- risk_score + 2\n    } \n  else if( BMI &gt; 30) {\n    risk_score &lt;- risk_score + 1\n    } \n  \n  if (PhysicalActivity &gt; 110){\n    risk_score &lt;- risk_score - 1\n    }\n    \n  return(risk_score)\n  \n}\n\n\n#demo\ncalculate_risk_score(120, 45, 'no', 115)\n\n[1] 1.5\n\ncalculate_risk_score(120, 45, 'Smoker', 115)\n\n[1] 2.5\n\ncalculate_risk_score(90, 35, 'Smoker', 60)\n\n[1] 2\n\n\n\nAdd a check to your function whether the supplied parameters are numeric, except for Smoking which should be a factor or a character (you can check that too if you want to). Test that your check works correctly.\n\n\ncalculate_risk_score &lt;- function(BloodPressure, BMI, Smoking, PhysicalActivity){\n  \n  if (!is.numeric(BloodPressure) | !is.numeric(BMI) | !is.numeric(PhysicalActivity)) {\n    stop(\"BloodPressure, BMI, and PhysicalActivity must be numeric values.\")\n  }\n  \n  if(! (is.character(Smoking) | is.factor(Smoking)) ){\n    stop(\"Smoking should be a factor or character.\")\n  }\n  \n  risk_score &lt;- 0\n  \n  if (BloodPressure &gt; 90){\n    risk_score &lt;- risk_score + 0.5\n    }\n\n  if (Smoking == 'Smoker'){\n    risk_score &lt;- risk_score + 1\n    }\n  \n  #BMI &gt; 30 includes BMIs that are larger than 40, so we need to be careful of\n  #the order in which we check, or include that the BMI should be &lt;= 40 in order \n  #to add 1.   \n  \n if (BMI &gt; 40){\n    risk_score &lt;- risk_score + 2\n    } \n  else if( BMI &gt; 30) {\n    risk_score &lt;- risk_score + 1\n    } \n  \n  if (PhysicalActivity &gt; 110){\n    risk_score &lt;- risk_score - 1\n    }\n    \n  return(risk_score)\n  \n}\n\n\ncalculate_risk_score('Hi', 35, 'Smoker', 60)\n\n# Error in calculate_risk_score(\"Hi\", 35, \"Smoker\", 60) : \n#   BloodPressure, BMI, and PhysicalActivity must be numeric values.\n\n\ncalculate_risk_score(90, 35, 1, 60)\n\n# Error in calculate_risk_score(90, 35, 1, 60) : \n#   Smoking should be a factor or character.\n\n\ncalculate_risk_score(90, 35, 'Smoker', 60)\n\n[1] 2\n\n\n\nIn a for-loop, apply your calculate_risk_score to each row in diabetes_glucose. Remember to use the appropriate column, i.e. diabetes_glucose$BloodPressure should be the argument for BloodPressure in the function. Add the calculated risk score to diabetes_glucose as a column Risk_score.\n\n\nfor (i in 1:nrow(diabetes_glucose)) {\n  \n  risk_score &lt;- calculate_risk_score(BloodPressure = diabetes_glucose$BloodPressure[i], \n                                     BMI = diabetes_glucose$BMI[i], \n                                     Smoking = diabetes_glucose$Smoker[i], \n                                     PhysicalActivity = diabetes_glucose$PhysicalActivity[i])\n  \n  diabetes_glucose[i, 'Risk_score'] &lt;- risk_score\n  \n}\n\n\ndiabetes_glucose\n\n# A tibble: 1,422 × 14\n   ID    Sex      Age BloodPressure   BMI PhysicalActivity Smoker  Diabetes\n   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 34120 Female    28            75  25.4               92 Never          0\n 2 34120 Female    28            75  25.4               92 Never          0\n 3 34120 Female    28            75  25.4               92 Never          0\n 4 27458 Female    55            72  24.6               86 Never          0\n 5 27458 Female    55            72  24.6               86 Never          0\n 6 27458 Female    55            72  24.6               86 Never          0\n 7 70630 Male      22            80  24.9              139 Unknown        0\n 8 70630 Male      22            80  24.9              139 Unknown        0\n 9 70630 Male      22            80  24.9              139 Unknown        0\n10 13861 Female    56            72  37.1               64 Unknown        1\n# ℹ 1,412 more rows\n# ℹ 6 more variables: Serum_ca2 &lt;dbl&gt;, Married &lt;chr&gt;, Work &lt;chr&gt;,\n#   Measurement &lt;chr&gt;, `Glucose (mmol/L)` &lt;dbl&gt;, Risk_score &lt;dbl&gt;\n\n\n\nNow, run calculate_risk_score on each row in diabetes_glucose by using mapply instead. Confirm that your result is the same.\n\n\nrisk_scores &lt;- mapply(FUN = calculate_risk_score, \n                      BloodPressure = diabetes_glucose$BloodPressure, \n                      BMI = diabetes_glucose$BMI,\n                      Smoking = diabetes_glucose$Smoker,\n                      PhysicalActivity = diabetes_glucose$PhysicalActivity)\n\nA quick check confirms that the risk_scores vector contains the same as the column we previously added.\n\nall(risk_scores == diabetes_glucose$Risk_score)\n\n[1] TRUE\n\n\n\nCreate an R script file to contain your functions. Copy your functions there and remove them from your global environment with rm(list=\"name_of_your_function\"). Now source the function R script in your quarto document and test that the functions work.\n\n\n#remove function from global environment so we can test if it loads properly from the script\nrm(list = \"calculate_risk_score\")\n\nSource the script we copied our calculate_risk_score function into:\n\nsource('tdhh4B.R')\n\nThe function works still:\n\ncalculate_risk_score(90, 35, 'Smoker', 60)\n\n[1] 2",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 4B: Functions"
    ]
  },
  {
    "objectID": "tdhh/tdhh4B.html#extra-exercises",
    "href": "tdhh/tdhh4B.html#extra-exercises",
    "title": "Exercise 4B - Solutions: Scripting in R - Functions",
    "section": "Extra exercises",
    "text": "Extra exercises\nThese exercises will ask you to first perform some tidyverse operations. Once you succeeded, you should abstract your code into a function.\ne1. Calculate the mean of Glucose (mmol/L) for each measuring time point across all patients. You should obtain 3 values, one for 0, one for 60 and one for 120.\n\ndiabetes_glucose %&gt;%\n  group_by(Measurement) %&gt;%\n  summarise(mean_gluc = mean(`Glucose (mmol/L)`))\n\n# A tibble: 3 × 2\n  Measurement mean_gluc\n  &lt;chr&gt;           &lt;dbl&gt;\n1 0                8.07\n2 120             11.0 \n3 60               9.71\n\n\ne2. Now we would like to stratify this mean further by a second variable, Sex. We would like to obtain 6 means: 0_female, 0_male, 60_female, 60_male, 120_female, 120_male.\n\ndiabetes_glucose %&gt;%\n  group_by(Measurement, Sex) %&gt;%\n  summarize(glucose_mean = mean(`Glucose (mmol/L)`))\n\n`summarise()` has grouped output by 'Measurement'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   Measurement [3]\n  Measurement Sex    glucose_mean\n  &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;\n1 0           Female         8.10\n2 0           Male           8.03\n3 120         Female        10.9 \n4 120         Male          11.1 \n5 60          Female         9.71\n6 60          Male           9.72\n\n\ne3. Now we would like to abstract the name of the second column. Imagine we would like glucose measurement means per marriage status or per workplace instead of per sex. Create a variable column &lt;- 'Sex' and redo what you did above but using column instead of the literal name of the column (Sex). This works in the same way as when plotting with a variable instead of the literal column name. Have a look at presentation 4A if you don’t remember how to do it.\n\ncolumn &lt;- 'Sex'\n\nglucose_group_mean &lt;- diabetes_glucose %&gt;%\n  group_by(Measurement, !!sym(column)) %&gt;%\n  summarize(glucose_mean = mean(`Glucose (mmol/L)`))\n\n`summarise()` has grouped output by 'Measurement'. You can override using the\n`.groups` argument.\n\nglucose_group_mean\n\n# A tibble: 6 × 3\n# Groups:   Measurement [3]\n  Measurement Sex    glucose_mean\n  &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;\n1 0           Female         8.10\n2 0           Male           8.03\n3 120         Female        10.9 \n4 120         Male          11.1 \n5 60          Female         9.71\n6 60          Male           9.72\n\n\ne4. Now, make your code into a function calc_mean_meta() that can calculate glucose measurement means based on different meta data columns. It should be called like so calc_mean_meta('Sex') and give you the same result as before. Try it on other metadata columns in diabetes_glucose_unnest too!\n\ncalc_mean_meta &lt;- function(column){\n  glucose_group_mean &lt;- diabetes_glucose %&gt;%\n    group_by(Measurement, !!sym(column)) %&gt;%\n    summarize(glucose_mean = mean(`Glucose (mmol/L)`))\n  \n  return(glucose_group_mean)\n}\n\n\ncalc_mean_meta('Sex')\n\n`summarise()` has grouped output by 'Measurement'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   Measurement [3]\n  Measurement Sex    glucose_mean\n  &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;\n1 0           Female         8.10\n2 0           Male           8.03\n3 120         Female        10.9 \n4 120         Male          11.1 \n5 60          Female         9.71\n6 60          Male           9.72\n\n\n\ncalc_mean_meta('Work')\n\n`summarise()` has grouped output by 'Measurement'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 12 × 3\n# Groups:   Measurement [3]\n   Measurement Work         glucose_mean\n   &lt;chr&gt;       &lt;chr&gt;               &lt;dbl&gt;\n 1 0           Private              8.01\n 2 0           Public               7.92\n 3 0           Retired              8.41\n 4 0           SelfEmployed         8.50\n 5 120         Private             11.1 \n 6 120         Public              10.8 \n 7 120         Retired             11.8 \n 8 120         SelfEmployed        11.3 \n 9 60          Private              9.74\n10 60          Public               9.50\n11 60          Retired             10.2 \n12 60          SelfEmployed         9.99",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 4B: Functions"
    ]
  },
  {
    "objectID": "tdhh/tdhh3.html",
    "href": "tdhh/tdhh3.html",
    "title": "Exercise 3: Exploratory Data Analysis - Solution",
    "section": "",
    "text": "Load in the packages the you think you need for this exercise. You will be doing PCA, so have a look at which packages we used in Presentation 3. No worries if you forget any, you always load then later on.\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(FactoMineR)\nlibrary(factoextra)\n\n\nRead in the joined diabetes data set you created in Exercise 2. If you did not make it all the way through Exercise 2 you can find the dataset you need in ../data/exercise2_diabetes_glucose.xlsx.\nHave a look at the data type (numeric, categorical, factor) of each column to ensure these make sense. If need, convert variables to the correct (most logical) type.\n\n\ndiabetes_glucose &lt;- read_xlsx('../data/exercise2_diabetes_glucose.xlsx')\n\ndiabetes_glucose\n\n# A tibble: 1,422 × 13\n   ID    Sex      Age BloodPressure   BMI PhysicalActivity Smoker  Diabetes\n   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 34120 Female    28            75  25.4               92 Never          0\n 2 34120 Female    28            75  25.4               92 Never          0\n 3 34120 Female    28            75  25.4               92 Never          0\n 4 27458 Female    55            72  24.6               86 Never          0\n 5 27458 Female    55            72  24.6               86 Never          0\n 6 27458 Female    55            72  24.6               86 Never          0\n 7 70630 Male      22            80  24.9              139 Unknown        0\n 8 70630 Male      22            80  24.9              139 Unknown        0\n 9 70630 Male      22            80  24.9              139 Unknown        0\n10 13861 Female    56            72  37.1               64 Unknown        1\n# ℹ 1,412 more rows\n# ℹ 5 more variables: Serum_ca2 &lt;dbl&gt;, Married &lt;chr&gt;, Work &lt;chr&gt;,\n#   Measurement &lt;chr&gt;, `Glucose (mmol/L)` &lt;dbl&gt;",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "tdhh/tdhh3.html#getting-started",
    "href": "tdhh/tdhh3.html#getting-started",
    "title": "Exercise 3: Exploratory Data Analysis - Solution",
    "section": "",
    "text": "Load in the packages the you think you need for this exercise. You will be doing PCA, so have a look at which packages we used in Presentation 3. No worries if you forget any, you always load then later on.\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(FactoMineR)\nlibrary(factoextra)\n\n\nRead in the joined diabetes data set you created in Exercise 2. If you did not make it all the way through Exercise 2 you can find the dataset you need in ../data/exercise2_diabetes_glucose.xlsx.\nHave a look at the data type (numeric, categorical, factor) of each column to ensure these make sense. If need, convert variables to the correct (most logical) type.\n\n\ndiabetes_glucose &lt;- read_xlsx('../data/exercise2_diabetes_glucose.xlsx')\n\ndiabetes_glucose\n\n# A tibble: 1,422 × 13\n   ID    Sex      Age BloodPressure   BMI PhysicalActivity Smoker  Diabetes\n   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 34120 Female    28            75  25.4               92 Never          0\n 2 34120 Female    28            75  25.4               92 Never          0\n 3 34120 Female    28            75  25.4               92 Never          0\n 4 27458 Female    55            72  24.6               86 Never          0\n 5 27458 Female    55            72  24.6               86 Never          0\n 6 27458 Female    55            72  24.6               86 Never          0\n 7 70630 Male      22            80  24.9              139 Unknown        0\n 8 70630 Male      22            80  24.9              139 Unknown        0\n 9 70630 Male      22            80  24.9              139 Unknown        0\n10 13861 Female    56            72  37.1               64 Unknown        1\n# ℹ 1,412 more rows\n# ℹ 5 more variables: Serum_ca2 &lt;dbl&gt;, Married &lt;chr&gt;, Work &lt;chr&gt;,\n#   Measurement &lt;chr&gt;, `Glucose (mmol/L)` &lt;dbl&gt;",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "tdhh/tdhh3.html#check-data-distributions",
    "href": "tdhh/tdhh3.html#check-data-distributions",
    "title": "Exercise 3: Exploratory Data Analysis - Solution",
    "section": "Check Data Distributions",
    "text": "Check Data Distributions\nLet’s have a look at the distributions of the numerical variables.\n\nMake histograms of the three Glucose (mmol/L) measurements in your dataset. What do you observe? Are the three groups of values normally distributed?\n\n\nggplot(diabetes_glucose, aes(x = `Glucose (mmol/L)`)) +\n  geom_histogram(bins = 30, fill = \"#482878FF\", color = \"white\") +\n  theme_minimal() +\n  facet_wrap(vars(Measurement), nrow = 2, scales = \"free\")\n\n\n\n\n\n\n\n\n\nJust as in question 3 above, make histograms of the three Glucose (mmol/L) measurement variables, BUT this time stratify your dataset by the variable Diabetes. How do your distributions look now?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTry: facet_wrap(Var1 ~ Var2, scales = \"free\").\n\n\n\n\nggplot(diabetes_glucose, aes(x = `Glucose (mmol/L)`)) +\n  geom_histogram(bins = 30, fill = \"#482878FF\", color = \"white\") +\n  theme_minimal() +\n  facet_wrap(Diabetes ~ Measurement, nrow = 2, scales = \"free\")\n\n\n\n\n\n\n\n\n\nMake a qqnorm plot for the other numeric variables; Age, Bloodpressure, BMI, PhysicalActivity and Serum_ca2. What do the plots tell you?\n\n\ndf_long &lt;- diabetes_glucose %&gt;% \n  dplyr::select(Age, BloodPressure, BMI, PhysicalActivity, Serum_ca2)  %&gt;%\n  pivot_longer(cols = everything(),\n               names_to = \"variable\", \n               values_to = \"value\")\n\n# QQ plot\nggplot(df_long, aes(sample = value)) + \n  geom_qq_line(color = \"magenta4\") +\n  geom_qq(alpha = 0.7, size = 0.5) +\n  labs(title = \"QQ Plot for Gene Expression\",\n       x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal() +\n  facet_wrap(vars(variable), nrow = 2, scales = \"free\")\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\n\n\n\n\n\n\n\n\nFrom the qq-norm plot above you will see that especially one of the variables seems to be far from normally distributed. What type of transformation could you potentially apply to this variable to make it normal? Transform and make a histogram or qqnorm plot. Did the transformation help?\n\n\nggplot(diabetes_glucose, aes(x = Age)) +\n  geom_histogram(bins = 30, fill = \"#A8D898\", color = \"white\") +\n  theme_minimal()\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nggplot(diabetes_glucose, aes(x = log2(Age))) +\n  geom_histogram(bins = 30, fill = \"#A8D898\", color = \"white\") +\n  theme_minimal()\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nLuckily for us it is not a requirement for dimensionality reduction methods like PCA that neither variables nor their residuals were normally distributed. Requirements for normality becomes important when performing statistical tests and modelling, including t-tests, ANOVA and regression models (more on this in part 5).",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "tdhh/tdhh3.html#data-cleaning",
    "href": "tdhh/tdhh3.html#data-cleaning",
    "title": "Exercise 3: Exploratory Data Analysis - Solution",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nWhile PCA is very forgiving in terms of variable distributions, there are some things it does not handle well, including missing values and varying ranges of numeric variables. So, before you go on you need to do a little data management.\n\nThe Glucose (mmol/L) variable in the dataset which denotes the result of the Oral Glucose Tolerance Test with measurements at times the 0, 60, 120 min should be separated out into three columns, one for each time point.\n\n\ndiabetes_glucose &lt;- diabetes_glucose %&gt;% \n  pivot_wider(names_from = Measurement,\n              values_from = `Glucose (mmol/L)`,\n              names_prefix = 'Measurement_')\n\ndiabetes_glucose\n\n\nRemove any missing values from your dataset.\n\n\ndiabetes_glucose &lt;- diabetes_glucose %&gt;% \n  drop_na()\n\n\nExtract all numeric variables AND scale these so they have comparable ranges. This numeric dataset you will use for PCA.\n\n\ndiab_glu_num &lt;- diabetes_glucose %&gt;% \n  dplyr::select(where(is.numeric)) %&gt;%\n  scale()\n\nhead(diab_glu_num)\n\n            Age BloodPressure        BMI PhysicalActivity  Diabetes  Serum_ca2\n[1,] -0.5095355    0.18808364 -0.7123872       0.31233047 -1.003892  0.6085078\n[2,] -0.5095355    0.18808364 -0.7123872       0.31233047 -1.003892  0.6085078\n[3,] -0.5095355    0.18808364 -0.7123872       0.31233047 -1.003892  0.6085078\n[4,]  1.8586337   -0.04390803 -0.8322613       0.07595423 -1.003892 -0.9482168\n[5,]  1.8586337   -0.04390803 -0.8322613       0.07595423 -1.003892 -0.9482168\n[6,]  1.8586337   -0.04390803 -0.8322613       0.07595423 -1.003892 -0.9482168\n     Glucose (mmol/L)\n[1,]       -1.0027491\n[2,]       -1.0027491\n[3,]       -1.0027491\n[4,]       -1.1514897\n[5,]       -0.7756125\n[6,]       -0.2183909\n\n\n\nExtract all categorical variables from your dataset and save them to a new object. You will use these as labels for the PCA.\n\n\ndiab_glu_cat &lt;- diabetes_glucose %&gt;% \n  dplyr::select(where(is.character), Diabetes) %&gt;% \n  mutate(Diabetes = as.factor(Diabetes))\n\nhead(diab_glu_cat)\n\n# A tibble: 6 × 7\n  ID    Sex    Smoker Married Work         Measurement Diabetes\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;       &lt;fct&gt;   \n1 34120 Female Never  No      Public       0           0       \n2 34120 Female Never  No      Public       60          0       \n3 34120 Female Never  No      Public       120         0       \n4 27458 Female Never  No      SelfEmployed 0           0       \n5 27458 Female Never  No      SelfEmployed 60          0       \n6 27458 Female Never  No      SelfEmployed 120         0",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "tdhh/tdhh3.html#principal-component-analysis",
    "href": "tdhh/tdhh3.html#principal-component-analysis",
    "title": "Exercise 3: Exploratory Data Analysis - Solution",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nCalculate the PCs on your numeric dataset using the function PCA() from the FactoMineR package. Note that you should set scale.unit = FALSE as you have already scaled your dataset.\n\n\nres.pca &lt;- PCA(diab_glu_num, graph = FALSE)\n\nres.pca\n\n**Results for the Principal Component Analysis (PCA)**\nThe analysis was performed on 1416 individuals, described by 7 variables\n*The results are available in the following objects:\n\n   name               description                          \n1  \"$eig\"             \"eigenvalues\"                        \n2  \"$var\"             \"results for the variables\"          \n3  \"$var$coord\"       \"coord. for the variables\"           \n4  \"$var$cor\"         \"correlations variables - dimensions\"\n5  \"$var$cos2\"        \"cos2 for the variables\"             \n6  \"$var$contrib\"     \"contributions of the variables\"     \n7  \"$ind\"             \"results for the individuals\"        \n8  \"$ind$coord\"       \"coord. for the individuals\"         \n9  \"$ind$cos2\"        \"cos2 for the individuals\"           \n10 \"$ind$contrib\"     \"contributions of the individuals\"   \n11 \"$call\"            \"summary statistics\"                 \n12 \"$call$centre\"     \"mean of the variables\"              \n13 \"$call$ecart.type\" \"standard error of the variables\"    \n14 \"$call$row.w\"      \"weights for the individuals\"        \n15 \"$call$col.w\"      \"weights for the variables\"          \n\n\n\nMake a plot that shows how much variance is captured by each component (in each dimension). There are two ways of making this plot: either you can use the function fviz_screeplot() from the factoextra package as we did in the exercise OR, you can use ggplot2, by extracting the res.pca$eig and plotting the percentage of variance column.\n\n\nPC_df &lt;- as.data.frame(res.pca$eig) %&gt;% \n  rownames_to_column(var = 'components')\n\nlabs &lt;- round(PC_df$`percentage of variance`, digits = 2) %&gt;% \n  as.character()\n\nggplot(PC_df, aes(x=components, y=`percentage of variance`, fill = eigenvalue)) + \n  geom_col() +\n  theme_minimal() + \n  geom_text(aes(label = labs), vjust = -0.5)\n\n\n\n\n\n\n\n\n\nNow, make a biplot (the round one with the arrows we showed in the presentation). This type of plot is a little complicated with ggplot alone, so either use the fviz_pca_var() function from the factoextra package we showed in the presentation, or - if you want to challenge yourself - try autoplot() from the ggfortify package.\n\n\nfviz_pca_var(res.pca, col.var = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE) # Avoid text overlapping)\n\n\n\n\n\n\n\n\n\nHave a look at the $var$contrib from your PCA object and compare it to the biplot, what do they tell you, i.e. which variables contribute the most to PC1 (dim1) and PC2 (dim2)? Also, look at the correlation matrix between variables in each component ($var$cor), what can you conclude from this?\nPlot your samples in the new dimensions, i.e. PC1 (dim1) vs PC2 (dim2), with the function fviz_pca_ind(). Add color and/or labels to the points using categorical variables you extracted above. What pattern of clustering (in any) do you observe?\n\n\np1 &lt;- fviz_pca_ind(res.pca, \n                   axes = c(1, 2), \n                   col.ind = diab_glu_cat$Diabetes,\n                   geom = \"point\", \n                   legend.title = \"Diabetes\") + \n  scale_color_viridis_d() +\n  theme(legend.position = \"bottom\")\n\np1\n\n\n\n\n\n\n\n\n\nTry to call $var$cos2 from your PCA object. What does it tell you about the first 5 components? Are there any of these, in addition to Dim 1 and Dim 2, which seem to capture some variance? If so, try to plot these components and overlay the plot with the categorical variables from above to figure out which it might be capturing.\n\n\np1 &lt;- fviz_pca_ind(res.pca, \n                   axes = c(1, 4), \n                   col.ind = diab_glu_cat$Smoker,\n                   geom = \"point\", \n                   legend.title = \"Smoker\") + \n  scale_color_viridis_d() +\n  theme(legend.position = \"bottom\")\n\np1\n\n\n\n\n\n\n\n\nThe Oral Glucose Tolerance Test is the one used to produce the OGTT measurements (0, 60, 120). As these measurement are directly used to diagnose diabetes we are not surprised that they separate the dataset well - we are being completely bias.\n\nOmit the Glucose measurement columns, calculate a PCA and create the plot above. Do the remaining variables still capture the Diabetes pattern?\n\n\ndiab_glu_num &lt;- diabetes_glucose %&gt;% \n  dplyr::select(Age,BloodPressure, BMI, PhysicalActivity, Serum_ca2) %&gt;%\n  scale()\n\nhead(diab_glu_num)\n\n\nres.pca &lt;- PCA(diab_glu_num, graph = FALSE)\n\nres.pca$var$cos2\n\n                       Dim.1      Dim.2        Dim.3       Dim.4        Dim.5\nAge              0.179044228 0.47738224 1.179010e-02 0.237077768 0.0822648503\nBloodPressure    0.113238994 0.60479871 2.125569e-06 0.156507251 0.1174344511\nBMI              0.697020728 0.03199509 6.799622e-04 0.066308130 0.0018402460\nPhysicalActivity 0.472545917 0.06049427 5.974280e-03 0.186643304 0.2670768712\nDiabetes         0.865617192 0.01854276 1.265714e-05 0.004176319 0.0085933098\nSerum_ca2        0.002695452 0.00411849 9.832735e-01 0.009567730 0.0002967391\nGlucose (mmol/L) 0.685119304 0.01840264 7.080813e-04 0.014552326 0.0759039042\n\nfviz_pca_var(res.pca, col.var = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE) \n\n\n\n\n\n\n\nfviz_pca_ind(res.pca, \n             axes = c(1, 2), \n             col.ind = diab_glu_cat$Diabetes,\n             geom = \"point\", \n             legend.title = \"Diabetes\") + \n  scale_color_viridis_d() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nFrom the exploratory analysis you have performed, which variables (numeric and/or categorical) would you as a minimum include in a model for prediction of Diabetes? N.B again you should not include the actual glucose measurement variables as they where used to the define the outcome you are looking at.\n\n\ndiab_model &lt;- lm(Diabetes ~ PhysicalActivity + BMI + BloodPressure + Smoker, data = diabetes_glucose)\n\nsummary(diab_model)\n\n\nCall:\nlm(formula = Diabetes ~ PhysicalActivity + BMI + BloodPressure + \n    Smoker, data = diabetes_glucose)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.98302 -0.17105  0.00882  0.21386  0.60857 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.6850976  0.0653602 -10.482  &lt; 2e-16 ***\nPhysicalActivity -0.0047429  0.0003280 -14.459  &lt; 2e-16 ***\nBMI               0.0456946  0.0012681  36.035  &lt; 2e-16 ***\nBloodPressure     0.0026039  0.0005694   4.573 5.23e-06 ***\nSmokerNever      -0.0829057  0.0202943  -4.085 4.65e-05 ***\nSmokerSmoker      0.1329873  0.0198258   6.708 2.85e-11 ***\nSmokerUnknown     0.0147172  0.0238776   0.616    0.538    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2712 on 1409 degrees of freedom\nMultiple R-squared:  0.7073,    Adjusted R-squared:  0.7061 \nF-statistic: 567.6 on 6 and 1409 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "tdhh/tdhh2.html",
    "href": "tdhh/tdhh2.html",
    "title": "Solution 2: Summary Statistics and Data Wrangling",
    "section": "",
    "text": "In this exercise you will do some more advance tidyverse operations such as pivoting and nesting.",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "tdhh/tdhh2.html#introduction",
    "href": "tdhh/tdhh2.html#introduction",
    "title": "Solution 2: Summary Statistics and Data Wrangling",
    "section": "",
    "text": "In this exercise you will do some more advance tidyverse operations such as pivoting and nesting.",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "tdhh/tdhh2.html#first-steps",
    "href": "tdhh/tdhh2.html#first-steps",
    "title": "Solution 2: Summary Statistics and Data Wrangling",
    "section": "First steps",
    "text": "First steps\n\nLoad packages.\n\n\nlibrary(tidyverse)\nlibrary(writexl)\n\n\nLoad the joined diabetes data set you created in exercise 1 (e.g. “diabetes_join.xlsx”) and the glucose dataset df_glucose.xlsx from the data folder. If you did not make it all the way through exercise 1 you can find the dataset you need in data/exercise1_diabetes_join.xlsx\n\n\ndiabetes_join &lt;- readxl::read_excel('../data/exercise1_diabetes_join.xlsx')\ndf_glucose &lt;- readxl::read_excel('../data/df_glucose.xlsx')",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "tdhh/tdhh2.html#change-format",
    "href": "tdhh/tdhh2.html#change-format",
    "title": "Solution 2: Summary Statistics and Data Wrangling",
    "section": "Change format",
    "text": "Change format\n\nHave a look at the glucose dataset. It has three columns with measurements from a Oral Glucose Tolerance Test where blood glucose is measured at fasting (Glucose_0), 1 hour/60 mins after glucose intake (Glucose_6), and 2 hours/120 mins after (Glucose_120). The last columns is an ID column. Change the data type of the ID column to factor in both diabetes_join and df_glucose.\n\n\ndf_glucose$ID &lt;- as.factor(df_glucose$ID)\ndiabetes_join$ID &lt;- as.factor(diabetes_join$ID)\n\n\nRestructure the glucose dataset into a long format. Think about which columns should be included in the pivot.\n\n\ndf_glucose_long &lt;- df_glucose %&gt;% \n  pivot_longer(cols = starts_with(\"Glucose\"),\n               names_to = \"Measurement\",\n               values_to = \"Glucose (mmol/L)\"\n               )\n\nhead(df_glucose_long)\n\n# A tibble: 6 × 3\n  ID    Measurement `Glucose (mmol/L)`\n  &lt;fct&gt; &lt;chr&gt;                    &lt;dbl&gt;\n1 9046  Glucose_0                 6.65\n2 9046  Glucose_60                8.04\n3 9046  Glucose_120              10.0 \n4 51676 Glucose_0                 4.49\n5 51676 Glucose_60                5.40\n6 51676 Glucose_120               6.22\n\n\n\nHow many rows are there per ID? Does that make sense?\n\n\n# There are three rows for each ID, corresponding to the three glucose measurements\ndf_glucose_long %&gt;%\n  count(ID) %&gt;%\n  head()\n\n# A tibble: 6 × 2\n  ID        n\n  &lt;fct&gt; &lt;int&gt;\n1 129       3\n2 210       3\n3 491       3\n4 530       3\n5 621       3\n6 712       3",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "tdhh/tdhh2.html#change-factor-levels",
    "href": "tdhh/tdhh2.html#change-factor-levels",
    "title": "Solution 2: Summary Statistics and Data Wrangling",
    "section": "Change factor levels",
    "text": "Change factor levels\n\nIn your long formatted dataframe you should have one column that described which measurement the row refers to, i.e. Glucose_0, Glucose_60 or Glucose_120. Transform this column so you only have the numerical part, i.e. only 0, 60 or 120. Then change the data type of that column to factor. Check the order of the factor levels and if necessary change them to the proper order.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nHave a look at the help for factors ?factors to see how to influence the levels.\n\n\n\n\ndf_glucose_long &lt;- df_glucose_long %&gt;% \n  mutate(Measurement = str_split_i(Measurement, '_', 2) %&gt;% as.factor())\n\nCheck factor levels:\n\nlevels(df_glucose_long$Measurement)\n\n[1] \"0\"   \"120\" \"60\" \n\n\nAdjust levels to proper order:\n\ndf_glucose_long$Measurement &lt;- factor(df_glucose_long$Measurement, levels = c('0', '60','120'))",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "tdhh/tdhh2.html#join-datasets",
    "href": "tdhh/tdhh2.html#join-datasets",
    "title": "Solution 2: Summary Statistics and Data Wrangling",
    "section": "Join datasets",
    "text": "Join datasets\n\nJoin the long formatted glucose dataset you made in 4 with the joined diabetes dataset you loaded in 2. Do the wrangling needed to make it happen!\n\n\ndiabetes_glucose &lt;- diabetes_join %&gt;% \n  mutate(ID = str_split_i(ID, pattern = '_', i = 2)) %&gt;% \n  left_join(df_glucose_long, by = 'ID') \n\nhead(diabetes_glucose)\n\n# A tibble: 6 × 13\n  ID    Sex      Age BloodPressure   BMI PhysicalActivity Smoker Diabetes\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 34120 Female    28            75  25.4               92 Never         0\n2 34120 Female    28            75  25.4               92 Never         0\n3 34120 Female    28            75  25.4               92 Never         0\n4 27458 Female    55            72  24.6               86 Never         0\n5 27458 Female    55            72  24.6               86 Never         0\n6 27458 Female    55            72  24.6               86 Never         0\n# ℹ 5 more variables: Serum_ca2 &lt;dbl&gt;, Married &lt;chr&gt;, Work &lt;chr&gt;,\n#   Measurement &lt;fct&gt;, `Glucose (mmol/L)` &lt;dbl&gt;",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "tdhh/tdhh2.html#summary-stats",
    "href": "tdhh/tdhh2.html#summary-stats",
    "title": "Solution 2: Summary Statistics and Data Wrangling",
    "section": "Summary stats",
    "text": "Summary stats\n\nCalculate the mean glucose levels for each time point.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou will need to use group_by() and summerise().\n\n\n\n\ndiabetes_glucose %&gt;%\n  group_by(Measurement) %&gt;%\n  summarise(mean = mean(`Glucose (mmol/L)`))\n\n# A tibble: 3 × 2\n  Measurement  mean\n  &lt;fct&gt;       &lt;dbl&gt;\n1 0            8.07\n2 60           9.71\n3 120         11.0 \n\n\n\nMake a figure with boxplots of the glucose measurements across the three time points.\n\n\ndiabetes_glucose %&gt;% \n  ggplot(aes(y = `Glucose (mmol/L)`,\n             x = Measurement, \n             fill = Measurement)) + \n  geom_boxplot() + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCalculate mean and standard deviation for all numeric columns and reformat the data frame into a manageable format like we did in the presentation.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou will need to use summarise() and across() to select numeric columns.\nFor a nice reformatting like in the presentation with need both pivot_longer and pivot_wider.\n\n\n\n\ndiabetes_glucose %&gt;%\n  summarise(across(.cols = where(is.numeric), \n                   .fns = list(mean = ~ mean(., na.rm = TRUE),\n                               sd = ~ sd(., na.rm = TRUE)),\n                   .names = \"{.col}-{.fn}\")) %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = c(\"variable\", \"statistic\"), \n               names_sep = \"-\") %&gt;%\n  pivot_wider(names_from = statistic, \n              values_from = value)\n\n# A tibble: 7 × 3\n  variable           mean     sd\n  &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n1 Age              33.8   11.4  \n2 BloodPressure    72.6   13.0  \n3 BMI              30.1    6.67 \n4 PhysicalActivity 84.1   25.3  \n5 Diabetes          0.502  0.500\n6 Serum_ca2         9.44   0.257\n7 Glucose (mmol/L)  9.60   3.76 \n\n\n\nPlot PhysicalActivity and BMI against each other in a scatter plot and color by Glucose (mmol/L).\n\n\ndiabetes_glucose %&gt;% \n  ggplot(aes(x = PhysicalActivity, \n             y = BMI, \n             color = `Glucose (mmol/L)`)) + \n  geom_point() + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nHighlights any trends or patterns that emerge across PhysicalActivity and BMI values. Includes an indication of confidence around those trends. Change color palette.\n\n\ndiabetes_glucose %&gt;% \n  ggplot(aes(x = PhysicalActivity, \n             y = BMI, \n             color = `Glucose (mmol/L)`))  + \n  geom_point() + \n  geom_smooth(method = \"loess\", level = 0.95) + \n  labs(title = \"Glucose Measurements with Mean by Diabetes Status\") + \n  scale_color_viridis_c(option = \"C\") + \n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "tdhh/tdhh2.html#sample-outliers-across-variables",
    "href": "tdhh/tdhh2.html#sample-outliers-across-variables",
    "title": "Solution 2: Summary Statistics and Data Wrangling",
    "section": "Sample outliers across variables",
    "text": "Sample outliers across variables\n\nJust like we did in the presentation, make a histogram for each numeric variable in the dataset. You can copy-paste the code from the presentation and make very few changes to make it work in this case.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCheck the class of the variables in the dataset.\n\n\n\n\ndiabetes_glucose_num &lt;- diabetes_glucose %&gt;%\n  select(where(is.numeric)) %&gt;%\n  drop_na()\n\ndiabetes_glucose_num[1:5, ]\n\n# A tibble: 5 × 7\n    Age BloodPressure   BMI PhysicalActivity Diabetes Serum_ca2\n  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    28            75  25.4               92        0       9.6\n2    28            75  25.4               92        0       9.6\n3    28            75  25.4               92        0       9.6\n4    55            72  24.6               86        0       9.2\n5    55            72  24.6               86        0       9.2\n# ℹ 1 more variable: `Glucose (mmol/L)` &lt;dbl&gt;\n\ndiabetes_glucose_num %&gt;% \n  pivot_longer(cols = where(is.numeric),\n               names_to = \"variable\",\n               values_to = \"value\") %&gt;%\n  ggplot(., aes(x = value)) +\n  geom_histogram(color= 'white') +\n  facet_wrap(vars(variable), ncol = 2, scales = \"free\") + \n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nCheck for outlier samples using the dendogram showed in the presentation. This includes scaling, calculating the distances between all pairs of scaled variables and hierarchical clustering. Remember that this can only be done for the numeric variables.\n\nWe will not spot any outliers via the dendogram in this dataset so we will add a fake outlier for you to detect. Here, the outlier has a value of the maximum value + 5 in each of the variables - pretty unrealistic. Play around with the values of the fake outlier and see how extreme it needs to be to be spotted in the dendogram.\n\noutlier &lt;- tibble(Age = max(diabetes_glucose_num$Age) + 5, \n                  BloodPressure = max(diabetes_glucose_num$BloodPressure) + 5,\n                  BMI = max(diabetes_glucose_num$BMI) + 5,\n                  PhysicalActivity = max(diabetes_glucose_num$PhysicalActivity) + 5,\n                  Serum_ca2 = max(diabetes_glucose_num$Serum_ca2) + 5,\n                  `Glucose (mmol/L)` = max(diabetes_glucose_num$`Glucose (mmol/L)`) + 5,\n                  )\n\n# Add a new row\ndiabetes_glucose_num_outlier &lt;- add_row(diabetes_glucose_num, outlier)\n\n# Keep adding outliters\n# diabetes_glucose_num_outlier &lt;- add_row(diabetes_glucose_num_outlier, outlier)\n\n\n# Euclidean pairwise distances\ndiabetes_glucose_dist &lt;- diabetes_glucose_num_outlier %&gt;% \n  mutate(across(everything(), scale)) %&gt;%\n  dist(., method = 'euclidean') \n\n# Hierarchical clustering with Ward's distance metric\nhclust_num &lt;- hclust(diabetes_glucose_dist, method = 'ward.D2')\n\nplot(hclust_num, main=\"Clustering based on scaled integer values\", cex=0.7)\n\n\n\n\n\n\n\n\n\nExport the final dataset (without the outlier!) in whichever format you prefer.\n\n\nwritexl::write_xlsx(diabetes_glucose, '../data/exercise2_diabetes_glucose.xlsx')",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "tdhh/tdhh2.html#more-plotting",
    "href": "tdhh/tdhh2.html#more-plotting",
    "title": "Solution 2: Summary Statistics and Data Wrangling",
    "section": "More Plotting",
    "text": "More Plotting\nLet’s make some more advanced ggplots!\n\nFor p1, plot the density of the glucose measurements at time 0. For p2, make the same plot as p1 but add stratification on the diabetes status. Make sure Diabetes is a factor. Give the plot a meaningful title. Consider the densities - do the plots make sense?\n\n\ndiabetes_glucose$Diabetes &lt;- as.factor(diabetes_glucose$Diabetes)\n\np1 &lt;- diabetes_glucose %&gt;% \n  filter(Measurement == 0) %&gt;% \n  ggplot(aes(x = `Glucose (mmol/L)`)) + \n  geom_density()\n\np2 &lt;- diabetes_glucose %&gt;% \n  filter(Measurement == 0) %&gt;% \n  ggplot(aes(x = `Glucose (mmol/L)`,\n             linetype = Diabetes)) + \n  geom_density()\n\nlibrary(patchwork)\np1 + p2 + plot_annotation(title = \"Glucose measurements at time 0\")\n\n\n\n\n\n\n\n\n\nNow, create one plot for the glucose measurement where the densities are stratified on measurement time (0, 60, 120) and diabetes status (0, 1). You should see 6 density curves in your plot. ::: {.callout-tip collapse=“true”} ## Hint There two ways of stratifying a density plot in ggplot2: color and linetype. :::\n\n\np1 &lt;- diabetes_glucose %&gt;% \n  ggplot(aes(x = `Glucose (mmol/L)`, \n             color = Measurement,\n             linetype = Diabetes)) + \n  geom_density() + \n  labs(title = \"Glucose measurements at time 0, 60, and 120 across diabetes status\")\n\np1\n\n\n\n\n\n\n\n\n\nThat is not a very intuitive plot. Could we change the geom and the order of the variables to visualize the same information more intuitively?\n\n\np1 &lt;- diabetes_glucose %&gt;% \n  ggplot(aes(x = Diabetes, y = `Glucose (mmol/L)`, \n             color = Diabetes,\n             linetype = Measurement)) + \n  geom_boxplot() + \n  labs(title = \"Glucose measurements at time 0, 60, and 120 across diabetes status\")\n\np1\n\n\n\n\n\n\n\n\n\nCalculate mean Glucose level by time point and Diabetes status. Save the data frame in a variable.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nGroup by several variables: group_by(var1, var2).\n\n\n\n\nglucose_mean &lt;- diabetes_glucose %&gt;%\n  group_by(Measurement, Diabetes) %&gt;%\n  summarize(`Glucose (mmol/L)` = mean(`Glucose (mmol/L)`)) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'Measurement'. You can override using the\n`.groups` argument.\n\nglucose_mean\n\n# A tibble: 6 × 3\n  Measurement Diabetes `Glucose (mmol/L)`\n  &lt;fct&gt;       &lt;fct&gt;                 &lt;dbl&gt;\n1 0           0                      5.49\n2 0           1                     10.6 \n3 60          0                      6.82\n4 60          1                     12.6 \n5 120         0                      7.88\n6 120         1                     14.2 \n\n\n\nCreate a plot that visualizes glucose measurements across time points, with one line for each patient ID. Then color the lines by their diabetes status. In summary, each patient’s glucose measurements should be connected with a line, grouped by their ID, and color-coded by Diabetes. Give the plot a meaningful title.\n\n\ndiabetes_glucose %&gt;%\n  ggplot(aes(x = Measurement,\n             y = `Glucose (mmol/L)`)) +\n  geom_point(aes(color = Diabetes)) + \n  geom_line(aes(group = ID, color = Diabetes)) + \n  labs(title = 'Glucose Measurements Across Time Points by Diabetes Status')\n\n\n\n\n\n\n\n\n\nRecreate the plot you made above.\n\nBut this time: Jittered individual values to avoid over plotting. Overlay and connect the group means showing the mean trend for each group. Add a title, change color pallet and themes.\nThis plot should look like this:\n\n\n\n\n\n\ndiabetes_glucose %&gt;%\n     ggplot(aes(x = Measurement,\n                y = `Glucose (mmol/L)`)) +\n  geom_jitter(aes(color = Diabetes), width = 0.2) +  \n  geom_point(data = glucose_mean, aes(x = Measurement, y = `Glucose (mmol/L)`)) +\n  geom_line(data = glucose_mean, aes(x = Measurement, y = `Glucose (mmol/L)`, \n                                     group = Diabetes, linetype = Diabetes)) +\n     labs(title = \"Glucose Measurements with Mean by Diabetes Status\") + \n  scale_colour_manual(values = c(\"#440154FF\", \"#FDE725FF\")) + \n  theme_minimal() \n\n\n\n\n\n\n\nggsave('../figures/figure3_13.png')\n\nSaving 7 x 5 in image",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "exercises/exercise2.html",
    "href": "exercises/exercise2.html",
    "title": "Exercise 2: Summary Statistics and Data Wrangling",
    "section": "",
    "text": "In this exercise you will do some more advance tidyverse operations such as pivoting and nesting.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "exercises/exercise2.html#introduction",
    "href": "exercises/exercise2.html#introduction",
    "title": "Exercise 2: Summary Statistics and Data Wrangling",
    "section": "",
    "text": "In this exercise you will do some more advance tidyverse operations such as pivoting and nesting.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "exercises/exercise2.html#first-steps",
    "href": "exercises/exercise2.html#first-steps",
    "title": "Exercise 2: Summary Statistics and Data Wrangling",
    "section": "First steps",
    "text": "First steps\n\nLoad packages.\nLoad the joined diabetes data set you created in exercise 1 (e.g. “diabetes_join.xlsx”) and the glucose dataset df_glucose.xlsx from the data folder. If you did not make it all the way through exercise 1 you can find the dataset you need in data/exercise1_diabetes_join.xlsx",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "exercises/exercise2.html#change-format",
    "href": "exercises/exercise2.html#change-format",
    "title": "Exercise 2: Summary Statistics and Data Wrangling",
    "section": "Change format",
    "text": "Change format\n\nHave a look at the glucose dataset. It has three columns with measurements from a Oral Glucose Tolerance Test where blood glucose is measured at fasting (Glucose_0), 1 hour/60 mins after glucose intake (Glucose_6), and 2 hours/120 mins after (Glucose_120). The last columns is an ID column. Change the data type of the ID column to factor in both diabetes_join and df_glucose.\nRestructure the glucose dataset into a long format. Think about which columns should be included in the pivot.\nHow many rows are there per ID? Does that make sense?",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "exercises/exercise2.html#change-factor-levels",
    "href": "exercises/exercise2.html#change-factor-levels",
    "title": "Exercise 2: Summary Statistics and Data Wrangling",
    "section": "Change factor levels",
    "text": "Change factor levels\n\nIn your long formatted dataframe you should have one column that described which measurement the row refers to, i.e. Glucose_0, Glucose_60 or Glucose_120. Transform this column so you only have the numerical part, i.e. only 0, 60 or 120. Then change the data type of that column to factor. Check the order of the factor levels and if necessary change them to the proper order.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nHave a look at the help for factors ?factors to see how to influence the levels.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "exercises/exercise2.html#join-datasets",
    "href": "exercises/exercise2.html#join-datasets",
    "title": "Exercise 2: Summary Statistics and Data Wrangling",
    "section": "Join datasets",
    "text": "Join datasets\n\nJoin the long formatted glucose dataset you made in 4 with the joined diabetes dataset you loaded in 2. Do the wrangling needed to make it happen!",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "exercises/exercise2.html#summary-stats",
    "href": "exercises/exercise2.html#summary-stats",
    "title": "Exercise 2: Summary Statistics and Data Wrangling",
    "section": "Summary stats",
    "text": "Summary stats\n\nCalculate the mean glucose levels for each time point.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou will need to use group_by() and summerise().\n\n\n\n\nMake a figure with boxplots of the glucose measurements across the three time points.\nCalculate mean and standard deviation for all numeric columns and reformat the data frame into a manageable format like we did in the presentation.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou will need to use summarise() and across() to select numeric columns.\nFor a nice reformatting like in the presentation with need both pivot_longer and pivot_wider.\n\n\n\n\nPlot PhysicalActivity and BMI against each other in a scatter plot and color by Glucose (mmol/L).\nHighlights any trends or patterns that emerge across PhysicalActivity and BMI values. Includes an indication of confidence around those trends. Change color palette.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "exercises/exercise2.html#sample-outliers-across-variables",
    "href": "exercises/exercise2.html#sample-outliers-across-variables",
    "title": "Exercise 2: Summary Statistics and Data Wrangling",
    "section": "Sample outliers across variables",
    "text": "Sample outliers across variables\n\nJust like we did in the presentation, make a histogram for each numeric variable in the dataset. You can copy-paste the code from the presentation and make very few changes to make it work in this case.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCheck the class of the variables in the dataset.\n\n\n\n\nCheck for outlier samples using the dendogram showed in the presentation. This includes scaling, calculating the distances between all pairs of scaled variables and hierarchical clustering. Remember that this can only be done for the numeric variables.\n\nWe will not spot any outliers via the dendogram in this dataset so we will add a fake outlier for you to detect. Here, the outlier has a value of the maximum value + 5 in each of the variables - pretty unrealistic. Play around with the values of the fake outlier and see how extreme it needs to be to be spotted in the dendogram.\n\noutlier &lt;- tibble(Age = max(diabetes_glucose_num$Age) + 5, \n                  BloodPressure = max(diabetes_glucose_num$BloodPressure) + 5,\n                  BMI = max(diabetes_glucose_num$BMI) + 5,\n                  PhysicalActivity = max(diabetes_glucose_num$PhysicalActivity) + 5,\n                  Serum_ca2 = max(diabetes_glucose_num$Serum_ca2) + 5,\n                  `Glucose (mmol/L)` = max(diabetes_glucose_num$`Glucose (mmol/L)`) + 5,\n                  )\n\n# Add a new row\ndiabetes_glucose_num_outlier &lt;- add_row(diabetes_glucose_num, outlier)\n\n# Keep adding outliters\n# diabetes_glucose_num_outlier &lt;- add_row(diabetes_glucose_num_outlier, outlier)\n\n\nExport the final dataset (without the outlier!) in whichever format you prefer.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "exercises/exercise2.html#more-plotting",
    "href": "exercises/exercise2.html#more-plotting",
    "title": "Exercise 2: Summary Statistics and Data Wrangling",
    "section": "More Plotting",
    "text": "More Plotting\nLet’s make some more advanced ggplots!\n\nFor p1, plot the density of the glucose measurements at time 0. For p2, make the same plot as p1 but add stratification on the diabetes status. Make sure Diabetes is a factor. Give the plot a meaningful title. Consider the densities - do the plots make sense?\n\n\np1 &lt;- # code for plot 1\n\np2 &lt;- # code for plot 2\n\nlibrary(patchwork)\np1 + p2 + plot_annotation(title = \"Glucose measurements at time 0\")\n\n\nNow, create one plot for the glucose measurement where the densities are stratified on measurement time (0, 60, 120) and diabetes status (0, 1). You should see 6 density curves in your plot.\nThat is not a very intuitive plot. Could we change the geom and the order of the variables to visualize the same information more intuitively?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere two ways of stratifying a density plot in ggplot2: color and linetype.\n\n\n\n\nCalculate mean Glucose level by time point and Diabetes status. Save the data frame in a variable.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nGroup by several variables: group_by(var1, var2).\n\n\n\n\nCreate a plot that visualizes glucose measurements across time points, with one line for each patient ID. Then color the lines by their diabetes status. In summary, each patient’s glucose measurements should be connected with a line, grouped by their ID, and color-coded by Diabetes. Give the plot a meaningful title.\nRecreate the plot you made above.\nBut this time: Jittered individual values to avoid over plotting. Overlay and connect the group means showing the mean trend for each group. Add a title, change color pallet and themes.\n\nThis plot should look like this:",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "exercises/exercise1.html",
    "href": "exercises/exercise1.html",
    "title": "Exercise 1: Data Cleaning",
    "section": "",
    "text": "Load packages.\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readxl)\n\n\nLoad in the diabetes_clinical_toy_messy.xlsx data set.\n\n\ndiabetes &lt;- read_xlsx(\"../data/diabetes_clinical_toy_messy.xlsx\")\nhead(diabetes)\n\n# A tibble: 6 × 9\n     ID Sex      Age BloodPressure   BMI PhysicalActivity Smoker  Diabetes\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1  9046 Male      34            84  24.7               93 Unknown        0\n2 51676 Male      25            74  22.5              102 Unknown        0\n3 31112 Male      30             0  32.3               75 Former         1\n4 60182 Male      50            80  34.5               98 Unknown        1\n5  1665 Female    27            60  26.3               82 Never          0\n6 56669 Male      35            84  35                 58 Smoker         1\n# ℹ 1 more variable: Serum_ca2 &lt;dbl&gt;",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 1: Data Cleaning"
    ]
  },
  {
    "objectID": "exercises/exercise1.html#getting-started",
    "href": "exercises/exercise1.html#getting-started",
    "title": "Exercise 1: Data Cleaning",
    "section": "",
    "text": "Load packages.\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readxl)\n\n\nLoad in the diabetes_clinical_toy_messy.xlsx data set.\n\n\ndiabetes &lt;- read_xlsx(\"../data/diabetes_clinical_toy_messy.xlsx\")\nhead(diabetes)\n\n# A tibble: 6 × 9\n     ID Sex      Age BloodPressure   BMI PhysicalActivity Smoker  Diabetes\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1  9046 Male      34            84  24.7               93 Unknown        0\n2 51676 Male      25            74  22.5              102 Unknown        0\n3 31112 Male      30             0  32.3               75 Former         1\n4 60182 Male      50            80  34.5               98 Unknown        1\n5  1665 Female    27            60  26.3               82 Never          0\n6 56669 Male      35            84  35                 58 Smoker         1\n# ℹ 1 more variable: Serum_ca2 &lt;dbl&gt;",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 1: Data Cleaning"
    ]
  },
  {
    "objectID": "exercises/exercise1.html#explore-the-data",
    "href": "exercises/exercise1.html#explore-the-data",
    "title": "Exercise 1: Data Cleaning",
    "section": "Explore the data",
    "text": "Explore the data\n\nHow many missing values (NA’s) are there in each column.\nCheck the ranges and distribution of each of the numeric variables in the dataset. Do any values seem weird or unexpected? Extract summary statistics on these, e.g. means and standard deviation.\nSome variables in the dataset are categorical or factor variables. Figure out what levels these have and how many observations there are for each level.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 1: Data Cleaning"
    ]
  },
  {
    "objectID": "exercises/exercise1.html#clean-up-the-data",
    "href": "exercises/exercise1.html#clean-up-the-data",
    "title": "Exercise 1: Data Cleaning",
    "section": "Clean up the data",
    "text": "Clean up the data\nNow that we have had a look at the data, it is time to correct fixable mistakes and remove observations that cannot be corrected.\nConsider the following:\n\nWhat should we do with the rows that contain NA’s? Do we remove them or keep them?\nWhich odd things in the data can we correct with confidence and which cannot?\nAre there zeros in the data? Are they true zeros or errors?\nDo you want to change any of the classes of the variables?\n\n\nMake a clean version of the dataset according to your considerations.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nHave a look at BloodPressure, BMI, and Sex.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 1: Data Cleaning"
    ]
  },
  {
    "objectID": "exercises/exercise1.html#metadata",
    "href": "exercises/exercise1.html#metadata",
    "title": "Exercise 1: Data Cleaning",
    "section": "Metadata",
    "text": "Metadata\n\nThere is some metadata to accompany the dataset you have just cleaned in diabetes_meta_toy_messy.csv. This is a csv file, not an excel sheet, so you need to use the read_delim() function to load it. Load in the dataset and inspect it.\n\n7.1. How many missing values (NA’s) are there in each column.\n7.2. Check the distribution of each of the variables. Consider that the variables are of different classes. Do any of the distributions seam odd to you?\n7.3. Make a clean version of the dataset according to your considerations.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 1: Data Cleaning"
    ]
  },
  {
    "objectID": "exercises/exercise1.html#join-the-datasets",
    "href": "exercises/exercise1.html#join-the-datasets",
    "title": "Exercise 1: Data Cleaning",
    "section": "Join the datasets",
    "text": "Join the datasets\n\nConsider which variable the datasets should be joined on.\nConsider how you want to join the datasets. Do you want to use full_join, inner_join, left_join and rigth_join?\n\nThe joining variable must be the same type in both datasets.\n\nJoin the cleaned versions of the clinical and metadataset by the variable and with the function you considered above.\nHow many rows does the joined dataset have? Explain how the join-function you used resulted in the given number of rows.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 1: Data Cleaning"
    ]
  },
  {
    "objectID": "exercises/exercise1.html#manipulate-some-strings",
    "href": "exercises/exercise1.html#manipulate-some-strings",
    "title": "Exercise 1: Data Cleaning",
    "section": "Manipulate some strings",
    "text": "Manipulate some strings\nWhen we look at the column Work, we can see that we have three levels Public, Private and Self-employed. Some special characters, including: - / \\[\\] (), etc. can sometimes prove problematic in variable names, as they are also used for operations in R. For example, are dashes (-) used to indicate a contrast in some functions.\n\nTo avoid potential issues in downstream analysis, change the Work variable so that Self-employed becomes SelfEmployed.\n\nAdditionally, we are not so happy with the fact that the ID is simply denoted by a number. The reason for this is that if we write out our dataset and read it in again (which we will do later), R will think it is a numeric or integer variable.\n\nAdd the string ID_ in front of the number and convert it to a factor variable.\nExport the joined dataset. Think about which directory you want to save the file in.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 1: Data Cleaning"
    ]
  },
  {
    "objectID": "exercises/exercise5B.html",
    "href": "exercises/exercise5B.html",
    "title": "Exercise 5B - Models and Model Evaluation in R",
    "section": "",
    "text": "In this exercise you will try to implement a Penalized Regression model and a Random Forest in R.\nFor modelling we will use the Obstetrics and Periodontal Therapy Dataset, which was collected in a randomized clinical trial. The trail was concerned with the effects of treatment of maternal periodontal disease and if it may reduce preterm birth risk.\nThis dataset has a total of 823 observations and 171 variables. As is often the case with clinical data, the data has many missing values, and for most categorical variables there is an unbalanced in number of observations for each level.\nIn order for you to not spend too much time on data cleaning and wrangling we have selected 33 variables (plus a patient ID column, PID) for which we have just enough observations per group level. As Lasso cannot handle NAs, we have imputed missing values and removed a few outlier samples.\n\nLoad the R packages needed for analysis:\n\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(MASS)\n\nNote that you may have some clashes of function names between these packages. Because of this, you will need to specify the package you are calling some of the functions from. You do this with the package name followed by two colons followed by the name of the function, e.g. dplyr::select()",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 5B: Models and Model Evaluation in R"
    ]
  },
  {
    "objectID": "exercises/exercise5B.html#introduction",
    "href": "exercises/exercise5B.html#introduction",
    "title": "Exercise 5B - Models and Model Evaluation in R",
    "section": "",
    "text": "In this exercise you will try to implement a Penalized Regression model and a Random Forest in R.\nFor modelling we will use the Obstetrics and Periodontal Therapy Dataset, which was collected in a randomized clinical trial. The trail was concerned with the effects of treatment of maternal periodontal disease and if it may reduce preterm birth risk.\nThis dataset has a total of 823 observations and 171 variables. As is often the case with clinical data, the data has many missing values, and for most categorical variables there is an unbalanced in number of observations for each level.\nIn order for you to not spend too much time on data cleaning and wrangling we have selected 33 variables (plus a patient ID column, PID) for which we have just enough observations per group level. As Lasso cannot handle NAs, we have imputed missing values and removed a few outlier samples.\n\nLoad the R packages needed for analysis:\n\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(MASS)\n\nNote that you may have some clashes of function names between these packages. Because of this, you will need to specify the package you are calling some of the functions from. You do this with the package name followed by two colons followed by the name of the function, e.g. dplyr::select()",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 5B: Models and Model Evaluation in R"
    ]
  },
  {
    "objectID": "exercises/exercise5B.html#summary-statistics",
    "href": "exercises/exercise5B.html#summary-statistics",
    "title": "Exercise 5B - Models and Model Evaluation in R",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\nLoad in the dataset Obt_Perio_ML.Rdata and inspect it.\n\n\n\n\n\n\n\nLoading an Rdata file\n\n\n\n\n\nNavigate to the data/Obt_Perio_ML.Rdata file in the file panel of your Rstudio and click the file. Confirm that you want to load the data into your environment by clicking “Yes”.\nFor reproducibility, copy the load function call printed in the console and paste it to the code chunk.\n\n\n\nYou can look at what the individual variables contain here. The curated dataset contains five variables which may be regarded as outcomes, these are:\n\nGA.at.outcome: Gestational age at end of pregnancy (days)\nBirthweight: Weight of baby at birth (grams)\nApgar1: Apgar score, a summary of a newborn infant’s appearance at birth, range: 0-10\nApgar5: Apgar score at 5 minutes after birth, range: 0-10\nPreg.ended...37.wk: Pregnancy ended before week 37, categorical (0 = no, 1 = yes)\nAny.SAE.: Whether participant experienced any serious adverse events (Yes, No)\n\nThe remaining 28 variables we will consider as potential explanatory variables for these outcomes.\n\nDo some basic summary statistics. How many categorical variables and how many numeric variables do you have? Try to make distributional plots for a couple of your numeric variables (or all if you would like) to get a feel for some of the data distributions you have.\nSome of the numeric variables are actually categorical. We have identified them in the facCols vector. Here, we change their type from numeric to character (since the other categorical variables are of this type). This code is sightly different from changing the type to factor, why we have written the code for you. Try to understand what is going on.\n\n\nfacCols &lt;- c(\"Race\", \n             \"ETXU_CAT5\", \n             \"BL.Anti.inf\", \n             \"BL.Antibio\", \n             \"V3.Anti.inf\", \n             \"V3.Antibio\", \n             \"V3.Bac.vag\", \n             \"V5.Anti.inf\",\n             \"V5.Antibio\",\n             \"V5.Bac.vag\",\n             \"X..Vis.Att\")\n\n\noptML &lt;- optML %&gt;%\n  mutate(across(all_of(facCols), as.character))\n\nhead(optML)\n\n\nMake count tables for all your categorical/factor variables. Are they balanced?",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 5B: Models and Model Evaluation in R"
    ]
  },
  {
    "objectID": "exercises/exercise5B.html#part-1-elastic-net-regression",
    "href": "exercises/exercise5B.html#part-1-elastic-net-regression",
    "title": "Exercise 5B - Models and Model Evaluation in R",
    "section": "Part 1: Elastic Net Regression",
    "text": "Part 1: Elastic Net Regression\nAs described above we have five variables which could be considered outcomes as these where all measured at the end of pregnancy. We can only work with one outcome at a time and we will pick Preg.ended...37.wk for now. This variable is a factor variable which denotes if a women gave birth prematurely (1=yes, 0=no).\n\nAs you will use the outcome Preg.ended...37.wk, you should remove the other five possible outcome variables from your dataset.\nElastic net regression can be sensitive to large differences in the range of numeric/integer variables, as such these variables should be scaled. Scale all numeric/integer variables in your dataset.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nmutate(across(where(…)))\n\n\n\n\nSplit your dataset into train and test set, you should have 70% of the data in the training set and 30% in the test set. How you chose to split is up to you, BUT afterwards you should ensure that for the categorical/factor variables all levels are represented in both sets.\nAfter dividing into train and test set, pull out the outcome variable, Preg.ended...37.wk, into its own vector for both datasets, name these: y_train and y_test.\nRemove the outcome variable, Preg.ended...37.wk, from the train and test set, as well as PID (if you have not already done so), as we should obviously not use this for training or testing.\n\nYou will employ the package glmnet to perform Elastic Net Regression. The main function from this package is glmnet() which we will use to fit the model. Additionally, you will also perform cross validation with cv.glmnet() to obtain the best value of the model hyper-parameter, lambda (\\(λ\\)).\nAs we are working with a mix of categorical and numerical predictors, it is advisable to dummy-code the variables. You can easily do this by creating a model matrix for both the test and train set.\n\nCreate the model matrix needed for input to glmnet() and cv.glmnet(). See pseudo-code below:\n\n\nmodTrain &lt;- model.matrix(~ .- 1, data = train)\nmodTest &lt;- model.matrix(~ .- 1, data = test)\n\n\nCreate your Elastic Net Regression model with glmnet().\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAs you are performing Elastic Net Regression, you set the mixing parameter alpha = 0.5. This gives you a 50/50 balance of the L1 (LASSO) and L2 (Ridge) penalties.\n\n\n\n\nUse cv.glmnet() to attain the best value of the hyperparameter lambda (\\(λ\\)). Remember to set a seed for reproducible results.\nPlot all the values of lambda tested during cross validation by calling plot() on the output of your cv.glmnet(). Extract the best lambda value from the cv.glmnet() model and save it as an object.\n\nNow, let’s see how well your model performed.\n\nPredict if an individual is likely to give birth before the 37th week using your model and your test set. See pseudo-code below.\n\n\ny_pred &lt;- predict(model, test, type = 'class')\n\n\nJust like for the logistic regression model you can calculate the accuracy of the prediction by comparing it to y_test with confusionMatrix(). Do you have a good accuracy? N.B look at the 2x2 contingency table, what does it tell you?\nLastly, let’s extract the variables which were retained in the model (e.g. not penalized out). We do this by calling the coefficient with coef() on our model. See pseudo-code below.\n\n\ncoeffs &lt;- coef(mode, s = bestLambda)\n\n# Convert coefficients to a data frame for easier viewing\ncoeffsDat &lt;- as.data.frame(as.matrix(coeffs))\n\n\nMake a plot that shows the absolute importance of the variables retained in your model. This could be a barplot with variable names on the y-axis and the length of the bars denoting absolute size of coefficient.\nNow repeat what you just did above, but this time instead of using Preg.ended...37.wk as outcome, try using a continuous variable, such as GA.at.outcome. N.B remember this means that you should evaluate the model using the RMSE and a scatter plot instead of the accuracy!",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 5B: Models and Model Evaluation in R"
    ]
  },
  {
    "objectID": "exercises/exercise5B.html#part-2-random-forest",
    "href": "exercises/exercise5B.html#part-2-random-forest",
    "title": "Exercise 5B - Models and Model Evaluation in R",
    "section": "Part 2: Random Forest",
    "text": "Part 2: Random Forest\nNow, let’s try a Random Forest classifier. We will continue using the Obt_Perio_ML.Rdata dataset with Preg.ended...37.wk as outcome.\n\nJust like in the section on Elastic Net above:\n\nLoad the dataset (if you have not already)\nRemove the possiable outcome variables you will not be using.\nSplit the dataset into test and train set - this time keep the outcome variable Preg.ended...37.wk in the dataset.\nRemember to remove the PID column before training!\n\nSet up a Random Forest model with cross-validation. See pseudo-code below. Remember to set a seed.\n\nFirst the cross-validation parameters:\n\nset.seed(123)\n\n# Set up cross-validation: 5-fold CV\nRFcv &lt;- trainControl(\n  method = \"cv\",\n  number = 5,\n  classProbs = TRUE,\n  summaryFunction = twoClassSummary,\n  savePredictions = \"final\"\n)\n\nNext we train the model:\n\n# Train Random Forest\nset.seed(123)\nrf_model &lt;- train(\n  Outcome ~ .,\n  data = Trainingdata,\n  method = \"rf\",\n  trControl = RFcv,\n  metric = \"ROC\",\n  tuneLength = 5           \n)\n\n\n# Model summary\nprint(rf_model)\n\n\nPlot your model fit. How does your model improve when you add 10, 20, 30, etc. predictors?\n\n\n# Best parameters\nrf_model$bestTune\n\n# Plot performance\nplot(rf_model)\n\n\nUse your test set to evaluate your model performance. How does the Random Forest compare to the Elastic Net regression?\nExtract the predictive variables with the greatest importance from your fit.\n\n\nvarImpOut &lt;- varImp(rf_model)\n\nvarImpOut$importance\n\n\nMake a logistic regression using the same dataset (you already have your train data, test data, y_train and y_test). How do the results of Elastic Net regression and Random Forest compare to the output of your glm.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 5B: Models and Model Evaluation in R"
    ]
  },
  {
    "objectID": "exercises/exercise5A.html",
    "href": "exercises/exercise5A.html",
    "title": "Exercise 5A: Intro to Regression in R",
    "section": "",
    "text": "In this exercise you will fit and interpret simple models.\n\nLoad packages\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggfortify)\nlibrary(factoextra)",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 5A: Intro to Regression in R"
    ]
  },
  {
    "objectID": "exercises/exercise5A.html#introduction",
    "href": "exercises/exercise5A.html#introduction",
    "title": "Exercise 5A: Intro to Regression in R",
    "section": "",
    "text": "In this exercise you will fit and interpret simple models.\n\nLoad packages\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggfortify)\nlibrary(factoextra)",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 5A: Intro to Regression in R"
    ]
  },
  {
    "objectID": "exercises/exercise5A.html#part-1-linear-regression",
    "href": "exercises/exercise5A.html#part-1-linear-regression",
    "title": "Exercise 5A: Intro to Regression in R",
    "section": "Part 1: Linear regression",
    "text": "Part 1: Linear regression\nWe will use the dataset described below to fit a linear regression model.\n\nLoad the data boston.csv and inspect it.\n\nThis dataset describes conditions surrounding the Boston housing market in the 1970s. Each row describes a zone in the Boston area (so there is more than one house in each row).\nThe columns are:\ncrim - per capita crime rate\nindus - proportion of non-retail businesses\nnox - Nitrogen oxides concentration (air pollution)\nrm - average number of rooms\nneighborhood - the type of neighborhood the zone is in\nmedv - median value per house in 1000s\n\nExplore the data\n\nDoes the datatype of each column fit to it what it describes? Do you need to change any data types?\n\n\n\nMaking a model\n\nSplit the dataset into test and training data. N.B: For any categorical variables in the boston dataset ensure that all levels are represented in both training and test set.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere are many ways split a dataset - an easy way is to use the function sample_frac().\n\n\n\n\nUsing your training data fit a linear model with number of rooms (rm), crime rate (crim) and neighborhood type (neighborhood) as the predictors and the value of a house (medv) as the response variable (y).\nDescribe what information you get from the model summary.\nIf you wanted to know if there is a difference in the value of houses between the Suburban and Urban neighborhood what could you do to the variable neighborhood before modelling?\nFor linear regression there is an assumption that the model residuals (errors) are normally distributed. An easy way visualize this is by simply calling plot() on your model (see below). What do you think based on the plots?\n\n\n#RMSE\npar(mfrow=c(2,2))\nplot(model)\n\n\nNow, use our test set to predict the response medv (median value per house in 1000s).\nEvaluate how well our model performs. There are different ways of doing this but lets use the classic measure of RMSE (Root Mean Square Error). The psedo-code below shows how to calculate the RMSE. A small RMSE (close to zero), indicates a good model.\n\n\n#RMSE\nrmse &lt;- sqrt(mean((y_test - y_pred)^2))\n\n\nMake a scatter plot to visualize how the predicted values fit with the observed values.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot y_test against y_pred.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 5A: Intro to Regression in R"
    ]
  },
  {
    "objectID": "exercises/exercise5A.html#part-2-logistic-regression",
    "href": "exercises/exercise5A.html#part-2-logistic-regression",
    "title": "Exercise 5A: Intro to Regression in R",
    "section": "Part 2: Logistic regression",
    "text": "Part 2: Logistic regression\nFor this part we will use the joined diabetes data since it has a categorical outcome (Diabetes yes or no). We will not use the oral Glucose measurements as predictors since this is literally how you define diabetes, so we’re loading the joined dataset we created in exercise 1, e.g. ‘diabetes_join.xlsx’ or what you have named it. N.B if you did not manage to finish making this dataset or forgot to save it, you can find a copy here: ../out/diabetes_join.Rdata. Navigate to the file in the file window of Rstudio and click on it. Click “Yes” to confirm that the file can be loaded in your environment and check that it has happened.\nAs the outcome we are studying, Diabetes, is categorical variable we will perform logistic regression. We select serum calcium levels (Serum_ca2), BMI and smoking habits (Smoker) as predictive variables.\n\nRead in the Diabetes dataset.\nLogistic regression does not allow for any missing values so first ensure you do not have NAs in your dataframe. Ensure that your outcome variable Diabetes is a factor.\nSplit your data into training and test data. Take care that the two classes of the outcome variable are represented in both training and test data, and at similar ratios.\nFit a logistic regression model with Serum_ca2, BMI and Smoker as predictors and Diabetes as outcome, using your training data.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nglm(…, family = ‘binomial’)\n\n\n\n\nCheck the model summary and try to determine whether you could potentially drop one or more of your variables? If so, make this alternative model (model2) and compare it to the original model. Is there a significant loss/gain, i.e. better fit when including the serum calcium levels as predictor?\nNow, use your model to predict Diabetes class based on your test set. What does the output of the prediction mean?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\npredict(... , type ='response')\n\n\n\n\nLets evaluate the performance of our model. As we are performing classification, measures such as mse/rmse will not work, instead we will calculate the accuracy. In order to get the accuracy you must first convert our predictions into Diabetes class labels (e.g. 0 or 1).\n\n\ncaret::confusionMatrix(y_pred, y_test)",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 5A: Intro to Regression in R"
    ]
  },
  {
    "objectID": "exercises/exercise5A.html#part-3-k-means-clustering",
    "href": "exercises/exercise5A.html#part-3-k-means-clustering",
    "title": "Exercise 5A: Intro to Regression in R",
    "section": "Part 3: K-Means Clustering",
    "text": "Part 3: K-Means Clustering\nIn this part we will run K-means clustering. To mix it up a bit we will work with a new dataset from patients with kidney disease. The dataset contains approximately 20 biological measures (variables) collected across 400 patients. The outcome is the classification variable which denotes whether a person suffers from ckd=chronic kidney disease ckd or not notckd.\n        age     -   age\n        bp      -   blood pressure\n        rbc     -   red blood cells\n        pc      -   pus cell\n        pcc     -   pus cell clumps\n        ba      -   bacteria\n        bgr     -   blood glucose random\n        bu      -   blood urea\n        sc      -   serum creatinine\n        sod     -   sodium\n        pot     -   potassium\n        hemo    -   hemoglobin\n        pcv     -   packed cell volume\n        wc      -   white blood cell count\n        rc      -   red blood cell count\n        htn     -   hypertension\n        dm      -   diabetes mellitus\n        cad     -   coronary artery disease\n        appet   -   appetite\n        pe      -   pedal edema\n        ane     -   anemia\n        class   -   classification  \n\nLoad in the dataset named kidney_disease.Rdata.\nBefore running K-means clustering please remove rows with any missing values across all variables in your dataset - yes, you will lose quite a lot of rows. Consider which columns you can use and if you have to do anything to them before clustering?\nRun the k-means clustering algorithm with 4 centers on the data. Look at the clusters you have generated.\nVisualize the results of your clustering. Do 4 clusters seems like a good fit for our data in the first two dimensions (Dim1 and Dim2)? How about if you have a look at Dim3 or Dim4?\nInvestigate the best number of clusters for this dataset. Use the silhouette metric.\nRe-do the clustering (plus visualization) with the optimal number of clusters.\nNow, try to figure out what the two clusters might represent. There are different ways to do this, but one easy way would be to simply compare the clusters IDs from the Kmeans output with one or more of the categorical variables from the dataset. You could use count() or table() for this.\nThe withiness measure (within cluster variance/spread) is much larger for one cluster then the other, what biological reason could there be for that?",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 5A: Intro to Regression in R"
    ]
  },
  {
    "objectID": "exercises/exercise4B.html",
    "href": "exercises/exercise4B.html",
    "title": "Exercise 4B: Scripting in R - Functions",
    "section": "",
    "text": "In this exercise you will practice creating and applying user-defined functions.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 4B: Functions"
    ]
  },
  {
    "objectID": "exercises/exercise4B.html#getting-started",
    "href": "exercises/exercise4B.html#getting-started",
    "title": "Exercise 4B: Scripting in R - Functions",
    "section": "Getting started",
    "text": "Getting started\nLoad libraries and the joined diabetes data set.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 4B: Functions"
    ]
  },
  {
    "objectID": "exercises/exercise4B.html#user-defined-functions",
    "href": "exercises/exercise4B.html#user-defined-functions",
    "title": "Exercise 4B: Scripting in R - Functions",
    "section": "User defined Functions",
    "text": "User defined Functions\n\nCreate a function named calculate_risk_score().\n\nIt should accept the following parameters:\n\nBloodPressure\nBMI\nSmoking\nPhysicalActivity\n\nInitiate the risk score with a value of 0 and calculate the final risk score with the following rules:\n\nif BloodPressure is &gt; 90, add 0.5\nif Smoking is ‘Smoker’, add 1\nif BMI is &gt; 30, add 1. If BMI &gt; 40, add 2\nif PhysicalActivity is &gt; 110, substract 1\n\nThe function should return the final risk score. Test it with some different inputs to verify that it works according to the rules detailed above.\n\nAdd a check to your function whether the supplied parameters are numeric, except for Smoking which should be a factor or a character (you can check that too if you want to). Test that your check works correctly.\nIn a for-loop, apply your calculate_risk_score to each row in diabetes_glucose. Remember to use the appropriate column, i.e. diabetes_glucose$BloodPressure should be the argument for BloodPressure in the function. Add the calculated risk score to diabetes_glucose as a column Risk_score.\nNow, run calculate_risk_score on each row in diabetes_glucose by using mapply instead. Confirm that your result is the same.\nCreate an R script file to contain your functions. Copy your functions there and remove them from your global environment with rm(list=\"name_of_your_function\"). Now source the function R script in your quarto document and test that the functions work.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 4B: Functions"
    ]
  },
  {
    "objectID": "exercises/exercise4B.html#extra-exercises",
    "href": "exercises/exercise4B.html#extra-exercises",
    "title": "Exercise 4B: Scripting in R - Functions",
    "section": "Extra exercises",
    "text": "Extra exercises\nThese exercises will ask you to first perform some tidyverse operations. Once you succeeded, you should abstract your code into a function.\nWe start by unnesting diabetes_glucose so you get back the Measurement and Glucose columns:\ne1. Calculate the mean of Glucose (mmol/L) for each measuring time point across all patients. You should obtain 3 values, one for 0, one for 60 and one for 120. e2. Now we would like to stratify this mean further by a second variable, Sex. We would like to obtain 6 means: 0_female, 0_male, 60_female, 60_male, 120_female, 120_male.\ne3. Now we would like to abstract the name of the second column. Imagine we would like glucose measurement means per marriage status or per workplace instead of per sex. Create a variable column &lt;- 'Sex' and redo what you did above but using column instead of the literal name of the column (Sex). This works in the same way as when plotting with a variable instead of the literal column name. Have a look at presentation 4A if you don’t remember how to do it.\ne4. Now, make your code into a function calc_mean_meta() that can calculate glucose measurement means based on different meta data columns. It should be called like so calc_mean_meta('Sex') and give you the same result as before. Try it on other metadata columns in diabetes_glucose_unnest too!",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 4B: Functions"
    ]
  },
  {
    "objectID": "presentations/presentation2.html",
    "href": "presentations/presentation2.html",
    "title": "Presentation 2: Summary Statistics and Data Wrangling",
    "section": "",
    "text": "Now that we’ve cleaned our data, it’s time to dig deeper into the actual contents of our dataset. Lets do some Exploratory Data Analysis (EDA).",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "presentations/presentation2.html#load-packages-and-data",
    "href": "presentations/presentation2.html#load-packages-and-data",
    "title": "Presentation 2: Summary Statistics and Data Wrangling",
    "section": "Load Packages and Data",
    "text": "Load Packages and Data\nLet’s begin by loading the packages we’ll need for data wrangling and plotting:\n\nlibrary(tidyverse)\nlibrary(factoextra)\n\nNext, bring in the cleaned dataset we prepared earlier:\n\nload(\"../data/Ovarian_comb_clean.RData\")\n\n# Check the structure and first few rows\nclass(df_comb)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndf_comb %&gt;% slice(1:5)\n\n# A tibble: 5 × 54\n  unique_patient_ID sample_recode primarysite summarygrade tumorstage substage\n  &lt;chr&gt;             &lt;chr&gt;         &lt;fct&gt;       &lt;fct&gt;        &lt;fct&gt;      &lt;chr&gt;   \n1 TCGA-20-0987      Tumor         ov          high         3          c       \n2 TCGA-23-1031      Tumor         ov          high         4          &lt;NA&gt;    \n3 TCGA-24-0979      Tumor         ov          high         4          &lt;NA&gt;    \n4 TCGA-23-1117      Tumor         ov          high         3          c       \n5 TCGA-23-1021      Tumor         ov          high         4          &lt;NA&gt;    \n# ℹ 48 more variables: grade &lt;fct&gt;, age_at_initial_path_diagn &lt;int&gt;,\n#   days_to_tumor_recurrence &lt;int&gt;, recurrence_status &lt;fct&gt;,\n#   days_to_death &lt;int&gt;, vital_status &lt;fct&gt;, percent_normal_cells &lt;int&gt;,\n#   percent_stromal_cells &lt;int&gt;, percent_tumor_cells &lt;int&gt;, batch &lt;fct&gt;,\n#   percent_not_cancer_cells &lt;int&gt;, stage &lt;chr&gt;, dominant_cell_type &lt;chr&gt;,\n#   group &lt;chr&gt;, COL10A1 &lt;dbl&gt;, COL11A1 &lt;dbl&gt;, COL11A2 &lt;dbl&gt;, COL13A1 &lt;dbl&gt;,\n#   COL14A1 &lt;dbl&gt;, COL15A1 &lt;dbl&gt;, COL16A1 &lt;dbl&gt;, COL17A1 &lt;dbl&gt;, …\n\ndf_comb &lt;- df_comb %&gt;%\n  select(-percent_not_cancer_cells)",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "presentations/presentation2.html#data-overview-and-ggplot2-recap",
    "href": "presentations/presentation2.html#data-overview-and-ggplot2-recap",
    "title": "Presentation 2: Summary Statistics and Data Wrangling",
    "section": "Data Overview and ggplot2 Recap",
    "text": "Data Overview and ggplot2 Recap\nDepending on how we want to investigate the variables, there are many ways in EDA toolkit to explore variables:\n\nUse histograms or boxplots for single variables\nUse scatterplots or barplots to check relationships between variables\n\nLet’s revisit some of the variables. This is also a good chance to refresh your ggplot2 skills. If you need a detailed refresher, refer to the From Excel to R: Presentation 3.\n\n# Distribution of tumor cell percentage\nggplot(df_comb, \n       aes(x = percent_tumor_cells)) +\n  geom_histogram(bins = 30, fill=\"#482878FF\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n# Count with barplot\nggplot(df_comb, \n       aes(x = grade, \n           fill = grade)) +\n  geom_bar() +\n  theme_bw() + \n  scale_fill_viridis_d(na.value = \"gray90\")\n\n\n\n\n\n\n\n\n\n# Tumor percentage by summary grade\nggplot(df_comb, \n       aes(x = grade, \n           y = percent_tumor_cells, \n           fill = grade)) +\n  geom_boxplot() +\n  theme_bw() + \n  scale_fill_viridis_d(na.value = \"gray90\")\n\nWarning: Removed 22 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThese simple plots give us an initial overview of the distribution of some of the variables - on their own and stratified by groups.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "presentations/presentation2.html#formats-long-and-wide",
    "href": "presentations/presentation2.html#formats-long-and-wide",
    "title": "Presentation 2: Summary Statistics and Data Wrangling",
    "section": "Formats: long and wide",
    "text": "Formats: long and wide\nNow, let’s say we want to create a single plot and compare several COL gene expression levels across different categories.\nTo plot and analyze more effectively, we need to reshape the data into long format, where each row represents a single observation:\nThe data can be reformatted to long format using pivot_longer():\n\ndf_comb_long &lt;- df_comb %&gt;% \n  pivot_longer(cols = starts_with('COL1'),\n               names_to = \"gene\",\n               values_to = \"value\")\n\ndf_comb_long %&gt;% select(unique_patient_ID, age_at_initial_path_diagn, gene, value) %&gt;% head() \n\n# A tibble: 6 × 4\n  unique_patient_ID age_at_initial_path_diagn gene    value\n  &lt;chr&gt;                                 &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 TCGA-20-0987                             61 COL10A1  3.66\n2 TCGA-20-0987                             61 COL11A1  3.20\n3 TCGA-20-0987                             61 COL11A2  3.65\n4 TCGA-20-0987                             61 COL13A1  4.07\n5 TCGA-20-0987                             61 COL14A1  4.05\n6 TCGA-20-0987                             61 COL15A1  4.98\n\n\n\n# one line per gene per person\nnrow(df_comb)\n\n[1] 578\n\nnrow(df_comb_long)\n\n[1] 6936\n\n\n\nThe Long Format is ggplot’s Best Friend\nWith the reshaped df_comb_long, we can now create one combined plot that shows distributions for all genes in a single ggplot call. More context: add color-stratification by summarygrade and compare distributions side-by-side:\n\nggplot(na.omit(df_comb_long), \n       aes(x = gene, y = value, fill = summarygrade)) +\n  geom_boxplot() +\n  scale_fill_viridis_d() + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\nWant histograms for all genes?\n\nggplot(df_comb_long, \n       aes(x = value, fill = gene)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(vars(gene), nrow = 3) + \n  scale_fill_viridis_d() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot gives us a histogram for each gene, all in one go.\n\nNo need to write separate plots manually.\nMuch easier to compare variables side-by-side.\n\n\n\nPivot back into wide format\nThe pivot_wider function is used to transform data to wide format.\n\ndf_comb_wide &lt;- df_comb_long %&gt;% \n  pivot_wider(names_from = gene, \n              values_from = value)\n\nhead(df_comb_wide)\nhead(df_comb)",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "presentations/presentation2.html#exploring-factors-categorical-variables",
    "href": "presentations/presentation2.html#exploring-factors-categorical-variables",
    "title": "Presentation 2: Summary Statistics and Data Wrangling",
    "section": "Exploring Factors (Categorical Variables)",
    "text": "Exploring Factors (Categorical Variables)\nWhen working with factor variables, we often want to know:\n\nWhich levels (categories) exist\nWhether groups are balanced\nHow missing data overlaps\n\nLet’s have a look at the vital_status variable. It looks fairly balanced.\n\ntable(df_comb$vital_status, useNA = \"ifany\")\n\n\ndeceased   living     &lt;NA&gt; \n     290      270       18 \n\n\nNow, let’s see how the vital_status levels are distributed across the tumorstage levels.\n\ntable(df_comb$vital_status, df_comb$stage, useNA = \"ifany\")\n\n          \n           1-b 1-c 2-b 2-c 3-b 3-c 4-NA NA-NA\n  deceased   1   2   2   5  12 214   52     2\n  living     2  11   2  18  12 191   32     2\n  &lt;NA&gt;       0   0   0   0   0   7    0    11\n\n\nSome stage categories could be merged for more power and clearer groups:\n\n# Relabel factor levels\ndf_comb &lt;- df_comb %&gt;%\n  mutate(stage = fct_recode(stage,\n                            \"1\" = \"1-b\",\n                            \"1\" = \"1-c\",\n                            \"2\" = \"2-b\",\n                            \"2\" = \"2-c\",\n                            \"4\" = \"4-NA\",\n                            NULL = \"NA-NA\"))\n\nLet’s see how other categorical variables relate to vital_status:\n\n# Plot faceted bar plots colored by vital_status\n\ndf_comb %&gt;% \n  select(where(is.factor), vital_status) %&gt;% \n  drop_na() %&gt;% \n  pivot_longer(cols = -vital_status, names_to = \"variable\", values_to = \"value\") %&gt;% \n  ggplot(aes(x = value, fill = vital_status)) +\n  geom_bar() +\n  facet_wrap(vars(variable), scales = \"free_x\") +\n  scale_fill_viridis_d() +\n  theme_minimal() \n\n\n\n\n\n\n\n\nAfter reviewing our categorical variables, we observed the following:\n\nTumorstage and grade are unbalanced. With certain stages and grades (e.g., 1 or 4) having low sample counts. This limits our ability to make reliable comparisons across all detailed levels.\nBatch is evenly represented, which reduces concerns about batch effects.\nVital_status is nicely balanced making it well-suited for comparisons and modeling.\n\nThese insights help us design our downstream analysis in a statistically sound and interpretable way.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "presentations/presentation2.html#summary-statistics",
    "href": "presentations/presentation2.html#summary-statistics",
    "title": "Presentation 2: Summary Statistics and Data Wrangling",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nIt’s important to make sure our variables are well-behaved and ready for downstream analysis and modeling.\nFor this, let’s say we want to compute the mean of several columns. A basic (but tedious) approach might look like this:\n\ndf_comb %&gt;%\n  summarise(mean_COL10A1 = mean(COL10A1),\n            mean_COL11A1 = mean(COL11A1),\n            mean_COL13A1 = mean(COL13A1),\n            mean_COL17A1 = mean(COL17A1),\n            mean_age_at_diagn = mean(age_at_initial_path_diagn))\n\n# A tibble: 1 × 5\n  mean_COL10A1 mean_COL11A1 mean_COL13A1 mean_COL17A1 mean_age_at_diagn\n         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;             &lt;dbl&gt;\n1         5.20         6.38         5.34         3.10                NA\n\n\nThis works, but we need to name every column we want to apply summarise to. It’s verbose and error-prone — especially if you have dozens of variables.\nInstead of looking at individual variable, we’ll introduce some tidyverse helper functions: across(), where()and starts_with() - that make summarizing variables much more efficient and scalable — so you don’t have to write repetitive code for every column.\nThese helpers are useful when we want to apply a functions, i.e. summarise() or mutate() to several columns.\n\nUsing across() and everything() to select columns\nLets select the columns which we want to apply summarise across in a dynamic fashion:\n\ndf_comb %&gt;%\n  summarise(across(.cols = everything(), # Columns to run fuction on \n                   .fns = mean)) %&gt;% # Function \n  select(15:25)\n\n# A tibble: 1 × 11\n  percent_tumor_cells batch stage dominant_cell_type group COL10A1 COL11A1\n                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;              &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1                  NA    NA    NA                 NA    NA    5.20    6.38\n# ℹ 4 more variables: COL11A2 &lt;dbl&gt;, COL13A1 &lt;dbl&gt;, COL14A1 &lt;dbl&gt;,\n#   COL15A1 &lt;dbl&gt;\n\n\n\n\nUsing where() and starts_with() to select numeric columns\nWe will probably not want to calculate means on non-numeric columns, so let’s select only numeric columns. For that we need another helper caller where() that lets us select columns based on their properties, like data type.\n\ndf_comb %&gt;%\n  summarise(across(.cols = where(fn = is.numeric), \n                   .fns = mean)) %&gt;% \n  select(5:15)\n\n# A tibble: 1 × 11\n  percent_stromal_cells percent_tumor_cells COL10A1 COL11A1 COL11A2 COL13A1\n                  &lt;dbl&gt;               &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1                    NA                  NA    5.20    6.38    3.55    5.34\n# ℹ 5 more variables: COL14A1 &lt;dbl&gt;, COL15A1 &lt;dbl&gt;, COL16A1 &lt;dbl&gt;,\n#   COL17A1 &lt;dbl&gt;, COL18A1 &lt;dbl&gt;\n\n\nWe can use starts_with() to select only columns starting with ‘COL’:\n\n# Columns that start with \"COL\"\ndf_comb %&gt;%\n  summarise(across(starts_with('COL'), mean))\n\n# A tibble: 1 × 34\n  COL10A1 COL11A1 COL11A2 COL13A1 COL14A1 COL15A1 COL16A1 COL17A1 COL18A1\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    5.20    6.38    3.55    5.34    5.01    5.89    4.95    3.10    7.26\n# ℹ 25 more variables: COL19A1 &lt;dbl&gt;, COL1A1 &lt;dbl&gt;, COL1A2 &lt;dbl&gt;,\n#   COL21A1 &lt;dbl&gt;, COL2A1 &lt;dbl&gt;, COL3A1 &lt;dbl&gt;, COL4A1 &lt;dbl&gt;, COL4A2 &lt;dbl&gt;,\n#   COL4A3 &lt;dbl&gt;, COL4A3BP &lt;dbl&gt;, COL4A4 &lt;dbl&gt;, COL4A5 &lt;dbl&gt;, COL4A6 &lt;dbl&gt;,\n#   COL5A1 &lt;dbl&gt;, COL5A2 &lt;dbl&gt;, COL5A3 &lt;dbl&gt;, COL6A1 &lt;dbl&gt;, COL6A2 &lt;dbl&gt;,\n#   COL6A3 &lt;dbl&gt;, COL7A1 &lt;dbl&gt;, COL8A1 &lt;dbl&gt;, COL8A2 &lt;dbl&gt;, COL9A1 &lt;dbl&gt;,\n#   COL9A2 &lt;dbl&gt;, COL9A3 &lt;dbl&gt;\n\n\nThese helpers make your code cleaner, more scalable, and easier to maintain. They can be used to select columns in tidyverse, also outside of across().\n\n\nsummarise() becomes more powerful!\nSo far, we’ve only applied a single function. But why stop at the mean? What if you want multiple statistics like mean, SD, min, and max - all in one go?\nWith across(), you can pass a list of functions:\n\ndf_comb %&gt;%\n  summarise(across(.cols = starts_with(\"COL\"), \n                   .fns = list(mean, sd, min, max)))\n\n# A tibble: 1 × 136\n  COL10A1_1 COL10A1_2 COL10A1_3 COL10A1_4 COL11A1_1 COL11A1_2 COL11A1_3\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1      5.20      1.78      2.79      10.3      6.38      2.62      2.76\n# ℹ 129 more variables: COL11A1_4 &lt;dbl&gt;, COL11A2_1 &lt;dbl&gt;, COL11A2_2 &lt;dbl&gt;,\n#   COL11A2_3 &lt;dbl&gt;, COL11A2_4 &lt;dbl&gt;, COL13A1_1 &lt;dbl&gt;, COL13A1_2 &lt;dbl&gt;,\n#   COL13A1_3 &lt;dbl&gt;, COL13A1_4 &lt;dbl&gt;, COL14A1_1 &lt;dbl&gt;, COL14A1_2 &lt;dbl&gt;,\n#   COL14A1_3 &lt;dbl&gt;, COL14A1_4 &lt;dbl&gt;, COL15A1_1 &lt;dbl&gt;, COL15A1_2 &lt;dbl&gt;,\n#   COL15A1_3 &lt;dbl&gt;, COL15A1_4 &lt;dbl&gt;, COL16A1_1 &lt;dbl&gt;, COL16A1_2 &lt;dbl&gt;,\n#   COL16A1_3 &lt;dbl&gt;, COL16A1_4 &lt;dbl&gt;, COL17A1_1 &lt;dbl&gt;, COL17A1_2 &lt;dbl&gt;,\n#   COL17A1_3 &lt;dbl&gt;, COL17A1_4 &lt;dbl&gt;, COL18A1_1 &lt;dbl&gt;, COL18A1_2 &lt;dbl&gt;, …\n\n\nThis gives you one wide row per column, with new columns like COL10A1_1, COL11A1_2, etc. A bit cryptic, right?\nLet’s clean it up by naming the functions and columns:\n\ngene_summary &lt;- df_comb %&gt;%\n  summarise(across(.cols = starts_with(\"COL\"), \n                   .fns = list(mean = mean, \n                               sd = sd, \n                               min = min, \n                               max = max),\n                   .names = \"{.col}-{.fn}\"))\n\ngene_summary\n\n# A tibble: 1 × 136\n  `COL10A1-mean` `COL10A1-sd` `COL10A1-min` `COL10A1-max` `COL11A1-mean`\n           &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1           5.20         1.78          2.79          10.3           6.38\n# ℹ 131 more variables: `COL11A1-sd` &lt;dbl&gt;, `COL11A1-min` &lt;dbl&gt;,\n#   `COL11A1-max` &lt;dbl&gt;, `COL11A2-mean` &lt;dbl&gt;, `COL11A2-sd` &lt;dbl&gt;,\n#   `COL11A2-min` &lt;dbl&gt;, `COL11A2-max` &lt;dbl&gt;, `COL13A1-mean` &lt;dbl&gt;,\n#   `COL13A1-sd` &lt;dbl&gt;, `COL13A1-min` &lt;dbl&gt;, `COL13A1-max` &lt;dbl&gt;,\n#   `COL14A1-mean` &lt;dbl&gt;, `COL14A1-sd` &lt;dbl&gt;, `COL14A1-min` &lt;dbl&gt;,\n#   `COL14A1-max` &lt;dbl&gt;, `COL15A1-mean` &lt;dbl&gt;, `COL15A1-sd` &lt;dbl&gt;,\n#   `COL15A1-min` &lt;dbl&gt;, `COL15A1-max` &lt;dbl&gt;, `COL16A1-mean` &lt;dbl&gt;, …\n\n\nMuch better! Now the column names are readable and include both the variable and the statistic.\nBut still not your preferred format? You can probably pivot your way out of that!\n\ngene_summary %&gt;%\n  pivot_longer(cols = everything(), \n               names_to = c(\"gene\", \"statistic\"), \n               names_sep = \"-\") %&gt;%\n  pivot_wider(names_from = statistic, \n              values_from = value)\n\n# A tibble: 34 × 5\n   gene     mean    sd   min   max\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 COL10A1  5.20 1.78   2.79 10.3 \n 2 COL11A1  6.38 2.62   2.76 11.4 \n 3 COL11A2  3.55 0.260  3.09  5.41\n 4 COL13A1  5.34 1.38   2.57 10.3 \n 5 COL14A1  5.01 1.06   3.28  9.53\n 6 COL15A1  5.89 1.02   3.42  8.68\n 7 COL16A1  4.95 0.931  3.21  8.03\n 8 COL17A1  3.10 0.214  2.71  5.57\n 9 COL18A1  7.26 0.896  2.58 10.2 \n10 COL19A1  2.75 0.150  2.40  3.31\n# ℹ 24 more rows\n\n\nNow you get a long format table with one row per variable and all your stats in columns — clean, tidy, and ready for interpretation.\n\n\n\n\n\n\n\nThe anonymous function: ~ and .\nWe promised to get back to handling the NAs when doing summary stats. Let’s just add the na.rm=TRUE argument. To not have too many things going on at once we’ll only do mean() for now:\n\ndf_comb %&gt;%\n  summarise(across(.cols = where(is.numeric), \n                   .fns = list(mean = mean(na.rm = TRUE)),\n                   .names = \"{.col}-{.fn}\"))\n\nError in `summarise()`:\nℹ In argument: `across(...)`.\nCaused by error in `mean.default()`:\n! argument \"x\" is missing, with no default\n\n\nBrrrtt! We may not.\nWhy doesn’t this work?\nWhen you pass functions directly into across() using the shorthand syntax (mean, sd, etc.), you’re only allowed to use the bare function with default arguments. You will also notice that we didn’t use brackets after their names, which is part of using the function shorthanded. Once you try to add something like na.rm = TRUE, the shorthandness breaks.\nTo pass arguments to a function that is called inside another function (i.e. calling mean inside summarise), we need to use what’s called an anonymous function. Don’t worry — it’s not as scary as it sounds.\nIt is written as a ~ and looks like this:\n\ndf_comb %&gt;%\n  summarise(across(.cols = where(is.numeric), \n                   .fns = list(mean = ~ mean(., na.rm = TRUE)),\n                   .names = \"{.col}-{.fn}\"))\n\n# A tibble: 1 × 40\n  `age_at_initial_path_diagn-mean` days_to_tumor_recurren…¹ `days_to_death-mean`\n                             &lt;dbl&gt;                    &lt;dbl&gt;                &lt;dbl&gt;\n1                             59.7                     624.                1010.\n# ℹ abbreviated name: ¹​`days_to_tumor_recurrence-mean`\n# ℹ 37 more variables: `percent_normal_cells-mean` &lt;dbl&gt;,\n#   `percent_stromal_cells-mean` &lt;dbl&gt;, `percent_tumor_cells-mean` &lt;dbl&gt;,\n#   `COL10A1-mean` &lt;dbl&gt;, `COL11A1-mean` &lt;dbl&gt;, `COL11A2-mean` &lt;dbl&gt;,\n#   `COL13A1-mean` &lt;dbl&gt;, `COL14A1-mean` &lt;dbl&gt;, `COL15A1-mean` &lt;dbl&gt;,\n#   `COL16A1-mean` &lt;dbl&gt;, `COL17A1-mean` &lt;dbl&gt;, `COL18A1-mean` &lt;dbl&gt;,\n#   `COL19A1-mean` &lt;dbl&gt;, `COL1A1-mean` &lt;dbl&gt;, `COL1A2-mean` &lt;dbl&gt;, …\n\n\nLet’s break it down:\n\n~ to define the function. ~ mean(., na.rm = TRUE) tells R:\n“for each column, compute the mean, ignoring NAs”\n. is a placeholder for the current column being operated or ‘the data previously referred to’. We need to use the . because mean when called as a proper function needs to have an argument (a vector of numbers) to work on.\n\n\n\nMultiple Summary Statistics with na.rm = TRUE\nNow let’s compute several stats per column, all with na.rm = TRUE. Here we chose just the integer columns:\n\nstats &lt;- df_comb %&gt;%\n  summarise(across(.cols = where(is.integer),\n                   .fns = list(mean = ~ mean(., na.rm = TRUE),  \n                               sd = ~ sd(., na.rm = TRUE), \n                               min = ~ min(., na.rm = TRUE), \n                               max = ~ max(., na.rm = TRUE)),\n                   .names = \"{.col}-{.fn}\")) %&gt;%\n  # add reformating\n  pivot_longer(cols = everything(), \n               names_to = c(\"variable\", \"statistic\"), \n               names_sep = \"-\") %&gt;%\n  mutate(value = round(value, 1)) %&gt;%\n  pivot_wider(names_from = statistic, \n              values_from = value) \n\nprint(stats)\n\n# A tibble: 6 × 5\n  variable                    mean    sd   min   max\n  &lt;chr&gt;                      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 age_at_initial_path_diagn   59.7  11.6    26    89\n2 days_to_tumor_recurrence   624.  608.      8  5480\n3 days_to_death             1010.  803.      8  5480\n4 percent_normal_cells         2.4   6.7     0    55\n5 percent_stromal_cells       12.8  11.9     0    70\n6 percent_tumor_cells         80.6  15.4     0   100\n\n\nThis produces a tidy table with one row per gene and columns for mean, sd, min, and max.\nThe helper functions can be used in other tidyverse operation such as mutate and select.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "presentations/presentation2.html#handling-outliers",
    "href": "presentations/presentation2.html#handling-outliers",
    "title": "Presentation 2: Summary Statistics and Data Wrangling",
    "section": "Handling Outliers",
    "text": "Handling Outliers\nNow that we’ve explored summary statistics, it’s time to consider outliers — data points that differ markedly from the rest.\nLet’s use our tidyverse skills to calculate some basic stats and visualize potential outliers.\n\nstats &lt;- stats %&gt;% \n  mutate(thr_upper = mean + 2 * sd,\n         thr_lower = mean - 2 * sd) %&gt;%\n  pivot_longer(cols = c(thr_lower, thr_upper), names_to = \"thr_type\", values_to = \"threshold\")\n\nprint(stats)\n\n# A tibble: 12 × 7\n   variable                    mean    sd   min   max thr_type  threshold\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 age_at_initial_path_diagn   59.7  11.6    26    89 thr_lower      36.5\n 2 age_at_initial_path_diagn   59.7  11.6    26    89 thr_upper      82.9\n 3 days_to_tumor_recurrence   624.  608.      8  5480 thr_lower    -593. \n 4 days_to_tumor_recurrence   624.  608.      8  5480 thr_upper    1840. \n 5 days_to_death             1010.  803.      8  5480 thr_lower    -596. \n 6 days_to_death             1010.  803.      8  5480 thr_upper    2615. \n 7 percent_normal_cells         2.4   6.7     0    55 thr_lower     -11  \n 8 percent_normal_cells         2.4   6.7     0    55 thr_upper      15.8\n 9 percent_stromal_cells       12.8  11.9     0    70 thr_lower     -11  \n10 percent_stromal_cells       12.8  11.9     0    70 thr_upper      36.6\n11 percent_tumor_cells         80.6  15.4     0   100 thr_lower      49.8\n12 percent_tumor_cells         80.6  15.4     0   100 thr_upper     111. \n\n\n\ndf_comb_longer &lt;- df_comb %&gt;% \n  pivot_longer(cols = where(is.integer),\n               names_to = \"variable\",\n               values_to = \"value\")\n\nggplot(df_comb_longer, \n       aes(x = value)) +\n  geom_histogram(bins = 30, fill=\"#482878FF\") +\n  geom_vline(data = stats, \n             aes(xintercept = threshold),\n             color = \"red\", linetype = \"dashed\") +\n  facet_wrap(vars(variable), ncol = 3, scales = \"free\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nSometimes, visualizing values in relation to other variables can make potential outliers easier to spot. Below, you’ll notice that a patient with 0% stromal cells and 0% tumor cells likely represents an outlier:\n\n# Bivariate scatter plot colored by stromal cell percentage\nggplot(df_comb, \n       aes(x = percent_tumor_cells, \n           y = percent_stromal_cells, \n           color = percent_normal_cells)) +\n  geom_point() + \n  scale_color_viridis_c() +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe probably want to remove patients where both percent_tumor_cells and percent_stromal_cells are 0%.\n\ndf_comb %&gt;%\n  filter(percent_tumor_cells == 0 & percent_stromal_cells == 0)\n\n# A tibble: 7 × 53\n  unique_patient_ID sample_recode primarysite summarygrade tumorstage substage\n  &lt;chr&gt;             &lt;chr&gt;         &lt;fct&gt;       &lt;fct&gt;        &lt;fct&gt;      &lt;chr&gt;   \n1 TCGA-24-1103      Tumor         ov          high         3          c       \n2 TCGA-23-1119      Tumor         ov          high         3          c       \n3 TCGA-23-1107      Tumor         ov          high         4          &lt;NA&gt;    \n4 TCGA-23-1120      Tumor         ov          high         3          c       \n5 TCGA-23-1121      Tumor         ov          high         3          c       \n6 TCGA-23-1123      Tumor         ov          high         3          c       \n7 TCGA-23-1124      Tumor         ov          high         3          c       \n# ℹ 47 more variables: grade &lt;fct&gt;, age_at_initial_path_diagn &lt;int&gt;,\n#   days_to_tumor_recurrence &lt;int&gt;, recurrence_status &lt;fct&gt;,\n#   days_to_death &lt;int&gt;, vital_status &lt;fct&gt;, percent_normal_cells &lt;int&gt;,\n#   percent_stromal_cells &lt;int&gt;, percent_tumor_cells &lt;int&gt;, batch &lt;fct&gt;,\n#   stage &lt;fct&gt;, dominant_cell_type &lt;chr&gt;, group &lt;chr&gt;, COL10A1 &lt;dbl&gt;,\n#   COL11A1 &lt;dbl&gt;, COL11A2 &lt;dbl&gt;, COL13A1 &lt;dbl&gt;, COL14A1 &lt;dbl&gt;, COL15A1 &lt;dbl&gt;,\n#   COL16A1 &lt;dbl&gt;, COL17A1 &lt;dbl&gt;, COL18A1 &lt;dbl&gt;, COL19A1 &lt;dbl&gt;, COL1A1 &lt;dbl&gt;, …\n\ndf_comb &lt;- df_comb %&gt;%\n  filter(\n    (percent_tumor_cells != 0 | is.na(percent_tumor_cells) & \n            percent_stromal_cells != 0) | is.na(percent_stromal_cells))",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "presentations/presentation2.html#sample-vs-variable-outliers",
    "href": "presentations/presentation2.html#sample-vs-variable-outliers",
    "title": "Presentation 2: Summary Statistics and Data Wrangling",
    "section": "Sample vs variable outliers",
    "text": "Sample vs variable outliers\nThere are many ways to detect and handle outliers. Instead of checking each variable separately, we can also look at all samples across multiple variables to spot unusual patterns.\nOne useful approach for datasets with many numeric or integer variables is to create a dendrogram using hierarchical clustering. This helps reveal samples that stand out from the rest.\nLet´s try it, first, we select all integer columns and drop rows with missing values (they don’t help clustering).\n\ndf_int &lt;- df_comb %&gt;%\n  select(where(is.integer)) %&gt;%\n  drop_na()\n\ndf_int %&gt;% head()\n\n# A tibble: 6 × 6\n  age_at_initial_path_diagn days_to_tumor_recurrence days_to_death\n                      &lt;int&gt;                    &lt;int&gt;         &lt;int&gt;\n1                        61                      442           701\n2                        60                      574           574\n3                        53                      428          1264\n4                        78                       61            61\n5                        74                      870           789\n6                        73                       68            84\n# ℹ 3 more variables: percent_normal_cells &lt;int&gt;, percent_stromal_cells &lt;int&gt;,\n#   percent_tumor_cells &lt;int&gt;\n\n\nNext, we scale the data (details in Presentation 3), calculate pairwise distances between all pairs of scaled variables, and run hierarchical clustering to build a dendrogram.\nThis tree-like plot helps reveal samples that cluster far from the rest — potential outliers.\n\n# Euclidean pairwise distances\ndf_int_dist &lt;- df_int %&gt;% \n  mutate(across(everything(), scale)) %&gt;%\n  dist(., method = 'euclidean')\n\n# Hierarchical clustering with Ward's distance metric\nhclust_int &lt;- hclust(df_int_dist, method = 'average')\n\n\nIf the plot is hard to see:\n\nCopy-paste the code into the console to open it in the Plots pane.\nClick Export → Copy to Clipboard… and enlarge it as needed.\n\nOr save it as a high-resolution file and inspect it outside RStudio.\n\n\n# Plot the result of clustering\n# png(\"dendrogram_plot.png\", width = 2000, height = 500)\n# plot(hclust_int, main = \"Clustering based on scaled integer values\", cex = 0.7)\n# dev.off()\nfviz_dend(hclust_int, h = 10)\n\n\n\n\n\n\n\n\nFrom the dendrogram, we can see a few samples that branch off far from the main cluster — these long branch lengths (Height) may indicate outliers.\nLet’s identify samples with unusually large heights and inspect them:\n\n# Set a high threshold (top 0.1%)\n#threshold &lt;- quantile(hclust_int$height, 0.99)\nthreshold &lt;- 6\n# Find their order in the clustering\nord_inx &lt;- hclust_int$order[hclust_int$height &gt; threshold]\nord_inx\n\n[1] 180  97\n\n# See which samples they are\ndf_int %&gt;% slice(ord_inx)\n\n# A tibble: 2 × 6\n  age_at_initial_path_diagn days_to_tumor_recurrence days_to_death\n                      &lt;int&gt;                    &lt;int&gt;         &lt;int&gt;\n1                        56                      213          1354\n2                        51                      192           192\n# ℹ 3 more variables: percent_normal_cells &lt;int&gt;, percent_stromal_cells &lt;int&gt;,\n#   percent_tumor_cells &lt;int&gt;\n\n\nNext, we’ll bring back our summary statistics table to see whether these measurements fall outside what’s typical for the dataset.\n\nstats\n\n# A tibble: 12 × 7\n   variable                    mean    sd   min   max thr_type  threshold\n   &lt;chr&gt;                      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 age_at_initial_path_diagn   59.7  11.6    26    89 thr_lower      36.5\n 2 age_at_initial_path_diagn   59.7  11.6    26    89 thr_upper      82.9\n 3 days_to_tumor_recurrence   624.  608.      8  5480 thr_lower    -593. \n 4 days_to_tumor_recurrence   624.  608.      8  5480 thr_upper    1840. \n 5 days_to_death             1010.  803.      8  5480 thr_lower    -596. \n 6 days_to_death             1010.  803.      8  5480 thr_upper    2615. \n 7 percent_normal_cells         2.4   6.7     0    55 thr_lower     -11  \n 8 percent_normal_cells         2.4   6.7     0    55 thr_upper      15.8\n 9 percent_stromal_cells       12.8  11.9     0    70 thr_lower     -11  \n10 percent_stromal_cells       12.8  11.9     0    70 thr_upper      36.6\n11 percent_tumor_cells         80.6  15.4     0   100 thr_lower      49.8\n12 percent_tumor_cells         80.6  15.4     0   100 thr_upper     111. \n\n\nNow we can compare these flagged samples to our summary statistics to understand why they stand out - and decide if they should be removed.\nFor these samples:\n\nAge at diagnosis and days to death are within a normal range.\nHowever, percent stromal cells and percent tumor cells are unusually low.\nThese samples have uncharacteristically high normal cell proportions, which is extreme for this dataset.\n\nGiven that they show extreme values across several key variables, it’s reasonable to treat them as outliers and remove them to avoid bias in our analysis.\n\ndf_comb &lt;- df_comb %&gt;%\n  filter(!(\n      (percent_normal_cells == 35 & days_to_death == 1354) |\n      (percent_normal_cells == 30 & days_to_death == 192)\n  ) |\n    is.na(percent_normal_cells) | is.na(days_to_death))\n\n\nBe cautious when removing data: Outlier removal should always be guided by domain knowledge and clear justification. Removing too much — or for the wrong reasons — can distort your analysis.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "presentations/presentation2.html#handling-missing-data",
    "href": "presentations/presentation2.html#handling-missing-data",
    "title": "Presentation 2: Summary Statistics and Data Wrangling",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data\nMissing data is common in most real-world datasets and can significantly affect the quality of our analysis. During EDA, it’s essential to identify, understand, and properly handle missing values to avoid biased or misleading conclusions.\nLet’s inspect rows with the most missing values:\n\ndf_comb %&gt;%\n  mutate(na_count = rowSums(is.na(.))) %&gt;%\n  arrange(desc(na_count)) %&gt;%\n  slice_head(n = 10)\n\n# A tibble: 10 × 54\n   unique_patient_ID sample_recode primarysite summarygrade tumorstage substage\n   &lt;chr&gt;             &lt;chr&gt;         &lt;fct&gt;       &lt;fct&gt;        &lt;fct&gt;      &lt;chr&gt;   \n 1 TCGA-01-0630      Healthy       &lt;NA&gt;        &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n 2 TCGA-01-0631      Healthy       &lt;NA&gt;        &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n 3 TCGA-01-0633      Healthy       &lt;NA&gt;        &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n 4 TCGA-01-0636      Healthy       &lt;NA&gt;        &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n 5 TCGA-01-0637      Healthy       &lt;NA&gt;        &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n 6 TCGA-01-0628      Healthy       &lt;NA&gt;        &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n 7 TCGA-01-0639      Healthy       &lt;NA&gt;        &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n 8 TCGA-01-0642      Healthy       &lt;NA&gt;        &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n 9 TCGA-04-1353      Tumor         &lt;NA&gt;        &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n10 TCGA-36-2539      Tumor         &lt;NA&gt;        &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n# ℹ 48 more variables: grade &lt;fct&gt;, age_at_initial_path_diagn &lt;int&gt;,\n#   days_to_tumor_recurrence &lt;int&gt;, recurrence_status &lt;fct&gt;,\n#   days_to_death &lt;int&gt;, vital_status &lt;fct&gt;, percent_normal_cells &lt;int&gt;,\n#   percent_stromal_cells &lt;int&gt;, percent_tumor_cells &lt;int&gt;, batch &lt;fct&gt;,\n#   stage &lt;fct&gt;, dominant_cell_type &lt;chr&gt;, group &lt;chr&gt;, COL10A1 &lt;dbl&gt;,\n#   COL11A1 &lt;dbl&gt;, COL11A2 &lt;dbl&gt;, COL13A1 &lt;dbl&gt;, COL14A1 &lt;dbl&gt;, COL15A1 &lt;dbl&gt;,\n#   COL16A1 &lt;dbl&gt;, COL17A1 &lt;dbl&gt;, COL18A1 &lt;dbl&gt;, COL19A1 &lt;dbl&gt;, COL1A1 &lt;dbl&gt;, …\n\n\nLooking at our missing values, we see that most missingness comes from samples labeled as Healthy(sample_recode).\nThis makes sense: Healthy tissue lacks tumor-specific details like tumor stage or percent tumor cells. This is an example of MAR (Missing At Random) — the missingness is related to an observed category (healthy vs. tumor tissue).\nSince our focus is on tumor samples, not healthy tissue (few samples), we can remove these samples to clean up our data and avoid noise.\n\ndf_comb &lt;- df_comb %&gt;% filter(sample_recode == \"Tumor\")\n\n# Let’s inspect rows with the most missing values again.\ndf_comb %&gt;%\n  mutate(na_count = rowSums(is.na(.))) %&gt;%\n  arrange(desc(na_count)) %&gt;%\n  slice_head(n = 10)\n\n# A tibble: 10 × 54\n   unique_patient_ID sample_recode primarysite summarygrade tumorstage substage\n   &lt;chr&gt;             &lt;chr&gt;         &lt;fct&gt;       &lt;fct&gt;        &lt;fct&gt;      &lt;chr&gt;   \n 1 TCGA-04-1353      Tumor         &lt;NA&gt;        &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n 2 TCGA-36-2539      Tumor         &lt;NA&gt;        &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n 3 TCGA-42-2593      Tumor         ov          &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n 4 TCGA-04-1341      Tumor         ov          high         &lt;NA&gt;       &lt;NA&gt;    \n 5 TCGA-57-1994      Tumor         ov          &lt;NA&gt;         &lt;NA&gt;       &lt;NA&gt;    \n 6 TCGA-24-1104      Tumor         ov          high         4          &lt;NA&gt;    \n 7 TCGA-23-1118      Tumor         ov          high         3          c       \n 8 TCGA-24-1105      Tumor         ov          high         3          c       \n 9 TCGA-23-1113      Tumor         ov          high         4          &lt;NA&gt;    \n10 TCGA-13-0755      Tumor         ov          high         4          &lt;NA&gt;    \n# ℹ 48 more variables: grade &lt;fct&gt;, age_at_initial_path_diagn &lt;int&gt;,\n#   days_to_tumor_recurrence &lt;int&gt;, recurrence_status &lt;fct&gt;,\n#   days_to_death &lt;int&gt;, vital_status &lt;fct&gt;, percent_normal_cells &lt;int&gt;,\n#   percent_stromal_cells &lt;int&gt;, percent_tumor_cells &lt;int&gt;, batch &lt;fct&gt;,\n#   stage &lt;fct&gt;, dominant_cell_type &lt;chr&gt;, group &lt;chr&gt;, COL10A1 &lt;dbl&gt;,\n#   COL11A1 &lt;dbl&gt;, COL11A2 &lt;dbl&gt;, COL13A1 &lt;dbl&gt;, COL14A1 &lt;dbl&gt;, COL15A1 &lt;dbl&gt;,\n#   COL16A1 &lt;dbl&gt;, COL17A1 &lt;dbl&gt;, COL18A1 &lt;dbl&gt;, COL19A1 &lt;dbl&gt;, COL1A1 &lt;dbl&gt;, …\n\n\nThere are two main approaches when deciding what to do:\n\nRemove rows with missing values\nImpute values\n\nIn our case:\nWe’ll replace numeric missing values with the median of each variable. We’ll replace missing factor levels with the most common category.\n\ndf_comb &lt;- df_comb %&gt;%\n  mutate(across(\n    c(\n      percent_normal_cells,\n      percent_stromal_cells,\n      percent_tumor_cells,\n      age_at_initial_path_diagn,\n      days_to_death,\n      days_to_tumor_recurrence\n    ),\n    ~ if_else(is.na(.x), as.integer(median(.x, na.rm = TRUE)), as.integer(.x))\n  ))\n\ndf_comb &lt;- df_comb %&gt;%\n  mutate(across(\n    c(stage, grade),\n    ~ fct_explicit_na(.x, na_level = names(sort(table(.x), decreasing = TRUE))[1])\n  ))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(...)`.\nCaused by warning:\n! `fct_explicit_na()` was deprecated in forcats 1.0.0.\nℹ Please use `fct_na_value_to_level()` instead.\n\n\nWe’ll keep the remaining NAs as-is for now.\n\nHowever always consider the impact of missing data. Even after imputing, missing data can cause uncertainty and bias so understands the result with caution.\n\n\n\nIn Summary\nWe conducted an initial exploratory data analysis (EDA) and, overall, the dataset looks clean and well-structured.\n\nReshaped data — switched between wide and long formats to easily plot multiple variables together.\nChecked factors — reviewed category balance, merged sparse groups, and visualized how factors relate.\nGenerated and inspected summary statistics — used tidyverse helpers to calculate means, standard deviations, and more for many columns at once, confirming that values align with biological expectations.\nDetected and removed outliers — used clustering to identify and exclude extreme or inconsistent samples with unusually high normal cell content.\nHandled missing data thoughtfully — removed irrelevant samples and imputed remaining gaps where appropriate.\n\n\n\nKey Takeaways\n\nLong format is your best friend when plotting or modeling.\nUse across() and helpers to write clean, scalable summaries.\nVisual checks are as important as statistical checks.\nEDA is not optional — it’s the foundation of trustworthy analysis.\nKeep your workflow reproducible and tidy for future modeling stages.\n\n\nsave(df_comb, file=\"../data/Ovarian_comb_prep_Col.RData\")",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 2: Summary Statistics and Data Wrangling"
    ]
  },
  {
    "objectID": "presentations/presentation1.html",
    "href": "presentations/presentation1.html",
    "title": "Presentation 1: Data Cleaning",
    "section": "",
    "text": "In this section, we’ll focus on cleaning a dataset. We’ll primarily use functions from the tidyverse package, but in some cases base R offers quick and effective alternatives.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 1: Data Cleaning"
    ]
  },
  {
    "objectID": "presentations/presentation1.html#load-packages",
    "href": "presentations/presentation1.html#load-packages",
    "title": "Presentation 1: Data Cleaning",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(tidyverse)\nlibrary(ggforce) # extension of ggplot2\n# unload MASS since it has select function we don't want to use.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 1: Data Cleaning"
    ]
  },
  {
    "objectID": "presentations/presentation1.html#load-data",
    "href": "presentations/presentation1.html#load-data",
    "title": "Presentation 1: Data Cleaning",
    "section": "Load data",
    "text": "Load data\nWe’ll work with two datasets:\n\ndf_ov: clinical information for ovarian cancer patients.\ndf_exp: gene expression data for collagen genes.\n\nWe use the load() function, which automatically imports an R object - let’s have a look at it.\n\n# .RData can contain several objects \nload(\"../data/Ovarian_data_Col.RData\")\n\nclass(df_ov)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndim(df_ov)\n\n[1] 578  23\n\ndf_ov %&gt;% head() \n\n# A tibble: 6 × 23\n  alt_sample_name    unique_patient_ID sample_type histological_type primarysite\n  &lt;chr&gt;              &lt;chr&gt;             &lt;chr&gt;       &lt;chr&gt;             &lt;chr&gt;      \n1 TCGA-20-0987-01A-… TCGA-20-0987      tumor       ser               ov         \n2 TCGA-23-1031-01A-… TCGA-23-1031      unknown     ser               ov         \n3 TCGA-24-0979-01A-… TCGA-24-0979      tumor       ser               ov         \n4 TCGA-23-1117-01A-… TCGA-23-1117      unknown     ser               ov         \n5 TCGA-23-1021-01B-… TCGA-23-1021      unknown     ser               ov         \n6 TCGA-04-1337-01A-… TCGA-04-1337      tumor       ser               ov         \n# ℹ 18 more variables: arrayedsite &lt;lgl&gt;, summarygrade &lt;chr&gt;, tumorstage &lt;int&gt;,\n#   substage &lt;chr&gt;, grade &lt;int&gt;, age_at_initial_path_diagn &lt;int&gt;,\n#   days_to_tumor_recurrence &lt;int&gt;, recurrence_status &lt;chr&gt;,\n#   days_to_death &lt;int&gt;, vital_status &lt;chr&gt;, os_binary &lt;lgl&gt;,\n#   relapse_binary &lt;lgl&gt;, site_of_tumor_first_recurrence &lt;chr&gt;,\n#   primary_therapy_outcome_success &lt;chr&gt;, percent_normal_cells &lt;int&gt;,\n#   percent_stromal_cells &lt;int&gt;, percent_tumor_cells &lt;int&gt;, batch &lt;dbl&gt;\n\nclass(df_exp)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndim(df_exp)\n\n[1] 578  35\n\ndf_exp %&gt;% head()\n\n# A tibble: 6 × 35\n  unique_patient_ID COL10A1 COL11A1 COL11A2 COL13A1 COL14A1 COL15A1 COL16A1\n  &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 TCGA-20-0987         3.66    3.20    3.65    4.07    4.05    4.98    3.57\n2 TCGA-23-1031         3.49    6.82    3.83    5.32    5.11    4.32    4.57\n3 TCGA-24-0979         3.43    3.43    3.59    3.92    5.86    5.89    4.82\n4 TCGA-23-1117         7.17   10.7     3.48    7.18    7.31    7.07    7.48\n5 TCGA-23-1021         6.41    4.06    3.63    8.60    5.42    4.27    5.66\n6 TCGA-04-1337         7.63   10.6     3.58    7.78    5.62    7.97    7.53\n# ℹ 27 more variables: COL17A1 &lt;dbl&gt;, COL18A1 &lt;dbl&gt;, COL19A1 &lt;dbl&gt;,\n#   COL1A1 &lt;dbl&gt;, COL1A2 &lt;dbl&gt;, COL21A1 &lt;dbl&gt;, COL2A1 &lt;dbl&gt;, COL3A1 &lt;dbl&gt;,\n#   COL4A1 &lt;dbl&gt;, COL4A2 &lt;dbl&gt;, COL4A3 &lt;dbl&gt;, COL4A3BP &lt;dbl&gt;, COL4A4 &lt;dbl&gt;,\n#   COL4A5 &lt;dbl&gt;, COL4A6 &lt;dbl&gt;, COL5A1 &lt;dbl&gt;, COL5A2 &lt;dbl&gt;, COL5A3 &lt;dbl&gt;,\n#   COL6A1 &lt;dbl&gt;, COL6A2 &lt;dbl&gt;, COL6A3 &lt;dbl&gt;, COL7A1 &lt;dbl&gt;, COL8A1 &lt;dbl&gt;,\n#   COL8A2 &lt;dbl&gt;, COL9A1 &lt;dbl&gt;, COL9A2 &lt;dbl&gt;, COL9A3 &lt;dbl&gt;\n\n\n\nBasics\nWe can get a little reminder of base R and tidyverse functions and explore some basic data characteristics.\nTake a look at categorical and numerical variables, access elements in list.\n\n# output: vector\ndf_ov$vital_status %&gt;% \n  head()\n\n[1] \"deceased\" \"deceased\" \"deceased\" \"deceased\" \"deceased\" \"deceased\"\n\n# output: one-column-tibble \ndf_ov %&gt;%\n  select(vital_status) %&gt;% \n  head()\n\n# A tibble: 6 × 1\n  vital_status\n  &lt;chr&gt;       \n1 deceased    \n2 deceased    \n3 deceased    \n4 deceased    \n5 deceased    \n6 deceased    \n\n\nGet a vector the tidyverse way:\n\ndf_ov %&gt;%\n  select(vital_status) %&gt;% \n  pull() %&gt;% \n  head()\n\n[1] \"deceased\" \"deceased\" \"deceased\" \"deceased\" \"deceased\" \"deceased\"\n\n\n\n\nCheck for Missing Values\nCheck which columns contain NA values and how many:\n\ndf_ov %&gt;% \n  is.na() %&gt;% \n  colSums() %&gt;% \n  barplot(las=2, cex.names=0.6) # baseR barplot since we are plotting a vector\n\n\n\n\n\n\n\n\nRemove rows with missing data (careful - this can reduce sample size too much):\n\ndf_ov %&gt;% \n  drop_na()\n\n# A tibble: 0 × 23\n# ℹ 23 variables: alt_sample_name &lt;chr&gt;, unique_patient_ID &lt;chr&gt;,\n#   sample_type &lt;chr&gt;, histological_type &lt;chr&gt;, primarysite &lt;chr&gt;,\n#   arrayedsite &lt;lgl&gt;, summarygrade &lt;chr&gt;, tumorstage &lt;int&gt;, substage &lt;chr&gt;,\n#   grade &lt;int&gt;, age_at_initial_path_diagn &lt;int&gt;,\n#   days_to_tumor_recurrence &lt;int&gt;, recurrence_status &lt;chr&gt;,\n#   days_to_death &lt;int&gt;, vital_status &lt;chr&gt;, os_binary &lt;lgl&gt;,\n#   relapse_binary &lt;lgl&gt;, site_of_tumor_first_recurrence &lt;chr&gt;, …",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 1: Data Cleaning"
    ]
  },
  {
    "objectID": "presentations/presentation1.html#data-wrangling-and-cleaning",
    "href": "presentations/presentation1.html#data-wrangling-and-cleaning",
    "title": "Presentation 1: Data Cleaning",
    "section": "Data wrangling and cleaning",
    "text": "Data wrangling and cleaning\nLet’s do some cleanup\n\nDrop Non-Informative Variables\nDrop Variables with Too Many Missing Values\nStandardize Categorical Data\nFix Variable Types\nRecode and Reorder Factors\nCreate New Variables from existing ones\n\n\nStep 1: Drop Non-Informative Variables\nLet’s check a few categorical variables and remove any that are redundant or irrelevant. If they don’t provide useful or well-curated information, we can drop them:\n\ndf_ov$histological_type %&gt;% table()\n\n.\nser \n568 \n\n\n\ndf_ov &lt;- df_ov %&gt;% \n  select(!histological_type)\n\n\n\nStep 2: Drop Variables with Too Many Missing Values\nIf a variable has more than ~20% missing data, it probably won’t be useful. Let’s remove those:\n~ mean(is.na(.)) &lt; threshold explained: ~ = “Define a function” . is a placeholder for the input (in this case, a column of the data frame) mean(is.na(.)) calculates the proportion of missing values &lt; threshold returns TRUE or FALSE — which where() uses to decide if a column should be kept\n\nthreshold &lt;- 0.2 # 20% threshold\n\ndf_ov &lt;- df_ov %&gt;% \n  select(where(~mean(is.na(.)) &lt; threshold))\n\ndim(df_ov)\n\n[1] 578  17\n\n\nThis keeps only the variables that are mostly complete.\n\n\nStep 3: Standardize Categorical Variables\nCounting the number of occurrences for each level of a categorical variable.\nHere we count how many dead or alive individuals we have. This is useful for checking balance across groups or categories.\n\ndf_ov %&gt;% \n  select(vital_status) %&gt;% \n  table(useNA = 'ifany') #%&gt;% barplot()\n\nvital_status\n deceased deceased     living   living        NA       &lt;NA&gt; \n      280        10       261         9         1        17 \n\n\nLooks like we have two groups of deceased and living. Let’s have a closer look at the unique values of this variable.\n\ndf_ov$vital_status %&gt;% \n  unique()\n\n[1] \"deceased\"  \"living\"    \"deceased \" \"living \"   NA          \"NA \"      \n\n\nWhitespace includes spaces, newlines, and other blank characters in text. It can cause errors or inconsistencies in data, so removing unnecessary whitespace is an important step in cleaning data. Sometimes, hidden characters or spaces cause misgrouped levels.\nUse the str_trim() function to remove whitespace.\n\nstr_trim(\" Hello World!   \")\n\n[1] \"Hello World!\"\n\n\nTrim whitespace and standardize:\nLike other function, the str_trim() function can also be used inside the mutate() function to alter the dataframe.\n\ndf_ov &lt;- df_ov %&gt;% \n  mutate(vital_status = str_trim(vital_status))\n\nAccessing the unique values of the vital_status column.\n\ndf_ov$vital_status %&gt;% \n  unique()\n\n[1] \"deceased\" \"living\"   NA         \"NA\"      \n\n\nChange “NA” to actual NA.\n\ndf_ov &lt;- df_ov %&gt;%\n  mutate(vital_status = na_if(vital_status, \"NA\"))\n\nAfter clean up we only have one group of deceased, living, and NA - perfect!\n\ndf_ov$vital_status %&gt;% \n  unique()\n\n[1] \"deceased\" \"living\"   NA        \n\n\n\n\nStep 4: Fix Variable Types\nSome variables are stored as character or numeric when they should actually be factors (i.e., categorical variables). Convert character/numeric columns to factors:\n\n# One by one...\ndf_ov$sample_type &lt;- as.factor(df_ov$sample_type)\ndf_ov$primarysite &lt;- as.factor(df_ov$primarysite)\n\n# In one go:\ncols_to_factor &lt;- c('sample_type', 'primarysite', 'summarygrade',\n                    'recurrence_status', 'vital_status', \n                    'tumorstage', 'grade', 'batch')\n\ndf_ov &lt;- df_ov %&gt;%\n  mutate(across(.cols = cols_to_factor, \n                .fns = as.factor))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = cols_to_factor, .fns = as.factor)`.\nCaused by warning:\n! Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(cols_to_factor)\n\n  # Now:\n  data %&gt;% select(all_of(cols_to_factor))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\nstr(df_ov)\n\ntibble [578 × 17] (S3: tbl_df/tbl/data.frame)\n $ alt_sample_name          : chr [1:578] \"TCGA-20-0987-01A-02R-0434-01\" \"TCGA-23-1031-01A-01R-0434-01\" \"TCGA-24-0979-01A-01R-0434-01\" \"TCGA-23-1117-01A-02R-0434-01\" ...\n $ unique_patient_ID        : chr [1:578] \"TCGA-20-0987\" \"TCGA-23-1031\" \"TCGA-24-0979\" \"TCGA-23-1117\" ...\n $ sample_type              : Factor w/ 3 levels \"healthy\",\"tumor\",..: 2 3 2 3 3 2 2 2 2 2 ...\n $ primarysite              : Factor w/ 2 levels \"other\",\"ov\": 2 2 2 2 2 2 2 2 2 2 ...\n $ summarygrade             : Factor w/ 4 levels \"HIGH\",\"high\",..: 2 1 2 2 2 4 2 2 2 2 ...\n $ tumorstage               : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 3 4 4 3 4 3 3 4 3 3 ...\n $ substage                 : chr [1:578] \"c\" NA NA \"c\" ...\n $ grade                    : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 3 3 3 3 3 2 3 3 3 3 ...\n $ age_at_initial_path_diagn: int [1:578] 61 60 53 42 45 78 74 73 45 45 ...\n $ days_to_tumor_recurrence : int [1:578] 442 574 428 418 1446 61 870 68 2616 797 ...\n $ recurrence_status        : Factor w/ 2 levels \"norecurrence\",..: 2 1 2 2 1 1 2 1 1 2 ...\n $ days_to_death            : int [1:578] 701 574 1264 1013 1446 61 789 84 2616 816 ...\n $ vital_status             : Factor w/ 2 levels \"deceased\",\"living\": 1 1 1 1 1 1 2 1 2 2 ...\n $ percent_normal_cells     : int [1:578] 16 8 0 0 0 0 12 15 NA 0 ...\n $ percent_stromal_cells    : int [1:578] 1 5 25 NA 0 36 2 5 NA NA ...\n $ percent_tumor_cells      : int [1:578] 72 80 72 NA NA 64 84 72 NA NA ...\n $ batch                    : Factor w/ 7 levels \"11\",\"13\",\"15\",..: 2 2 2 2 2 2 2 2 2 2 ...\n\n\n\n\nStep 5: Re-code and Reorder Factors\nSometimes we want to set the order of factor levels manually, especially if there’s a natural or meaningful order (e.g., “low” → “high”): This helps when plotting or interpreting regression coefficients.\n\n# Recode factor \ndf_ov$summarygrade %&gt;% table()\n\n.\nHIGH high  LOW  low \n  24  456    4   71 \n\ndf_ov &lt;- df_ov %&gt;% \n  mutate(summarygrade = fct_recode(summarygrade, \"high\" = \"HIGH\"),\n         summarygrade = fct_recode(summarygrade, \"low\" = \"LOW\"))\n\ndf_ov$summarygrade %&gt;% table()\n\n.\nhigh  low \n 480   75 \n\n# Reorder levels \nlevels(df_ov$summarygrade) # high, low\n\n[1] \"high\" \"low\" \n\ndf_ov$summarygrade &lt;- factor(df_ov$summarygrade, levels = c('low', 'high'))\n\nlevels(df_ov$summarygrade) # low, high\n\n[1] \"low\"  \"high\"\n\n\n\n\nStep 6: Creating New Variables\nNow, we would like to create a new data column by extracting specific information or combining other columns.\nAdd a column combining other columns into one:\n\ndf_ov &lt;- df_ov %&gt;% \n  mutate(percent_not_cancer_cells = percent_stromal_cells + percent_normal_cells)\n\nFor character vectors you can use functions paste and paste0 . The paste function concatenates two strings into one. This one is from baseR.\n\npaste('TCGA', 'Data', sep = ' ')\n\n[1] \"TCGA Data\"\n\n\n\ndf_ov &lt;- df_ov %&gt;% \n  mutate(stage = paste(tumorstage, substage, sep = '-'))\n\nunique(df_ov$stage)\n\n[1] \"3-c\"   \"4-NA\"  \"2-b\"   \"NA-NA\" \"1-b\"   \"3-b\"   \"2-c\"   \"1-c\"  \n\n\nCreate logical variables with ifelse. Adding a column with content based on a condition with ifelse.\n\ndf_ov &lt;- df_ov %&gt;% \n  mutate(dominant_cell_type = ifelse(percent_not_cancer_cells &gt; percent_tumor_cells, \"not_cancer_cells\", \"cancer_cells\"))\n\ntable(df_ov$dominant_cell_type)\n\n\n    cancer_cells not_cancer_cells \n             536                9 \n\n\nExtension of ifelse: case_when\n\ndf_ov &lt;- df_ov %&gt;% \n  mutate(group = case_when(age_at_initial_path_diagn &lt; 60 & summarygrade == \"low\" ~ \"Low grade - Young\",\n                           age_at_initial_path_diagn &lt; 60 & summarygrade == \"high\" ~ \"High grade - Young\",\n                           age_at_initial_path_diagn &gt;= 60 & summarygrade == \"low\" ~ \"Low grade - Mature\",\n                           age_at_initial_path_diagn &gt;= 60 & summarygrade == \"high\" ~ \"High grade - Mature\",\n                           .default = NA))\n\ntable(df_ov$group)\n\n\nHigh grade - Mature  High grade - Young  Low grade - Mature   Low grade - Young \n                233                 247                  31                  44",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 1: Data Cleaning"
    ]
  },
  {
    "objectID": "presentations/presentation1.html#string-manipulation",
    "href": "presentations/presentation1.html#string-manipulation",
    "title": "Presentation 1: Data Cleaning",
    "section": "String manipulation",
    "text": "String manipulation\nSometimes the information you need is hidden inside longer text strings or encoded in specific patterns — especially in genomics or clinical datasets. In this example we notice that sample type information is unknown for a lot of patients. Since we are TCGA experts, we know that this information is actually encoded in the full sample name TCGA-20-0987-01A-02R-0434-01. But How do we get it out.\nTo extract and work with this kind of information, we can use what is called a Regular Expression (Regex) and string manipulation tools.\n\nRegex: Regular expression\nRegular expressions (regex) are used to search for patterns in text strings. Regular expressions are super flexible and very powerful, BUT they can be slightly confusing at times…\nInstead of looking for an exact word like cat, a regex lets you search for things like:\n\n“Any word that starts with ‘c’ and ends with ‘t’”\n“Any sequence of digits”\n“A word that may or may not have a certain letter”\n“A word containing a specific delimiter, e.g. a dash, as comma, etc.”\n\nIt might look a little weird (lots of slashes, dots, and symbols), but once you learn a few basics, it’s incredibly useful! And the good thing is that ChatCPT is very good at making regular expressions for you!\n\n\n\n\n\nWe will do string manipulation the tidyverse way: using the stringr package where all the functions starts with str_. These functions often has a parameter called pattern which can take a regular expression as an argument. Let’s start with some simple examples.\n\n\nExamples\n\ndf_ov$alt_sample_name %&gt;% head(n = 1)\n\n[1] \"TCGA-20-0987-01A-02R-0434-01\"\n\n\nLet’s say we want to extract part of the sample name. We can use the str_split_i function to split a string on a given character and access the i’th element.\n\nstr_split_i('TCGA-20-0987-01A-02R-0434-01', pattern = '-', i = 2)\n\n[1] \"20\"\n\n\nThis grabs the 4th segment in the name string, which might contain the sample code We can apply this to a whole column using mutate() and create a new variable with the extracted values.\n\ndf_ov &lt;- df_ov %&gt;% \n  mutate(sample_code = str_split_i(alt_sample_name, pattern = '-', i = 4))\n\ntable(df_ov$sample_code)\n\n\n01A 01B 01C 01D 11A \n539  27   2   1   8 \n\n\nWe know that 01 encodes primary tumor while 11 encodes healthy tissue. We can use str_detect() to see if a sub-string contains the 01 or 11.\n\nstr_detect(df_ov$sample_code, '01') %&gt;% table(useNA = 'ifany')\n\n.\nFALSE  TRUE  &lt;NA&gt; \n    8   569     1 \n\n\n\nstr_detect(df_ov$sample_code, '11') %&gt;% table(useNA = 'ifany')\n\n.\nFALSE  TRUE  &lt;NA&gt; \n  569     8     1 \n\n\nAdd a column that records TRUE if sample is a primary tumor and place it after the Sample_type column.\n'01.{0,1}' explained:\n01 — matches the characters ‘0’ followed immediately by ‘1’.\n. — matches any single character (A, B, C, X, !, except a newline by default).\n{0,1} — is a quantifier, meaning: match the preceding element (the dot .) zero or one time.\n\ndf_ov &lt;- df_ov %&gt;% \n  mutate(sample_code_manual = str_detect(sample_code, '01.{0,1}'), \n         .after = sample_type)\n\ndf_ov %&gt;% select(sample_code, sample_code_manual) %&gt;% slice(100:110)\n\n# A tibble: 11 × 2\n   sample_code sample_code_manual\n   &lt;chr&gt;       &lt;lgl&gt;             \n 1 01A         TRUE              \n 2 01A         TRUE              \n 3 01A         TRUE              \n 4 01A         TRUE              \n 5 01A         TRUE              \n 6 11A         FALSE             \n 7 11A         FALSE             \n 8 01A         TRUE              \n 9 01A         TRUE              \n10 01A         TRUE              \n11 11A         FALSE             \n\n\nWe can also replace text patterns:\n\nstr_replace_all(string = 'TCGA', pattern = 'A', replacement = 'aaaaaaaaaa')\n\n[1] \"TCGaaaaaaaaaa\"\n\n\n\ndf_ov &lt;- df_ov %&gt;% \n  mutate(sample_recode = str_replace_all(sample_code, '01.', 'Tumor'),\n         sample_recode = str_replace_all(sample_recode, '11A', 'Healthy'),\n         .after = sample_type)\n\ndf_ov %&gt;% select(alt_sample_name, sample_code, sample_code_manual, sample_recode) %&gt;% tail()\n\n# A tibble: 6 × 4\n  alt_sample_name              sample_code sample_code_manual sample_recode\n  &lt;chr&gt;                        &lt;chr&gt;       &lt;lgl&gt;              &lt;chr&gt;        \n1 TCGA-24-1852-01A-01R-0808-01 01A         TRUE               Tumor        \n2 TCGA-29-1692-01B-01R-0808-01 01B         TRUE               Tumor        \n3 TCGA-13-1817-01A-01R-0808-01 01A         TRUE               Tumor        \n4 TCGA-61-1916-01A-01R-0808-01 01A         TRUE               Tumor        \n5 TCGA-29-1704-01B-01R-0808-01 01B         TRUE               Tumor        \n6 TCGA-13-1819-01A-01R-0808-01 01A         TRUE               Tumor        \n\ndf_ov &lt;- df_ov %&gt;% \n  select(!c(sample_code_manual, sample_code, alt_sample_name, sample_type))\n\ndf_ov %&gt;% dim()\n\n[1] 578  20\n\n\nNicely done!\nAt this point, we have:\n\nDropped Non-Informative Variables\nDropped Variables with Too Many Missing Values\nStandardized Categorical Data\nFixed Variable Types\nRecoded and Reordered Factors\nCreated New Variables from existing ones",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 1: Data Cleaning"
    ]
  },
  {
    "objectID": "presentations/presentation1.html#joining-dataframes",
    "href": "presentations/presentation1.html#joining-dataframes",
    "title": "Presentation 1: Data Cleaning",
    "section": "Joining Dataframes",
    "text": "Joining Dataframes\nOften we have information stored in more than one table, and we want to merge these data together into a single dataset. For example, here we would like to join our df_ov and df_exp data table by IDs.\nLet’s make a subset of each of the datasets to have a better overview of what is going on.\n\ndf_exp_subset &lt;- df_exp %&gt;% arrange(unique_patient_ID) %&gt;% slice(1:5) %&gt;% select(1:5)\ndf_ov_subset &lt;- df_ov %&gt;% arrange(unique_patient_ID) %&gt;% slice(3:7) %&gt;% select(1:3, vital_status)\n\ndf_exp_subset$unique_patient_ID %in% df_ov_subset$unique_patient_ID %&gt;% table()\n\n.\nFALSE  TRUE \n    2     3 \n\n\nA quick recap of join types from dplyr:\n\nfull_join(): all rows from both\ninner_join(): only matched rows\nleft_join(): all from left, matched from right\nright_join(): all from right, matched from left\n\n\nFull Join\nA full join keeps everything — all rows from both df_ov_subset and df_exp_subset. If there’s no match, missing values (NA) are filled in.\n\ndf_ov_subset %&gt;% \n  full_join(df_exp_subset, by = \"unique_patient_ID\")\n\n# A tibble: 7 × 8\n  unique_patient_ID sample_recode primarysite vital_status COL10A1 COL11A1\n  &lt;chr&gt;             &lt;chr&gt;         &lt;fct&gt;       &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 TCGA-01-0631      Healthy       &lt;NA&gt;        &lt;NA&gt;            3.64    4.90\n2 TCGA-01-0633      Healthy       &lt;NA&gt;        &lt;NA&gt;            3.50    2.97\n3 TCGA-01-0636      Healthy       &lt;NA&gt;        &lt;NA&gt;            3.28    2.93\n4 TCGA-01-0637      Healthy       &lt;NA&gt;        &lt;NA&gt;           NA      NA   \n5 TCGA-01-0639      Healthy       &lt;NA&gt;        &lt;NA&gt;           NA      NA   \n6 TCGA-01-0628      &lt;NA&gt;          &lt;NA&gt;        &lt;NA&gt;            2.99    3.00\n7 TCGA-01-0630      &lt;NA&gt;          &lt;NA&gt;        &lt;NA&gt;            3.63    2.76\n# ℹ 2 more variables: COL11A2 &lt;dbl&gt;, COL13A1 &lt;dbl&gt;\n\n# N row:\n# union(df_exp_subset$unique_patient_ID, df_ov_subset$unique_patient_ID) %&gt;% length()\n\n\n\nInner Join\nAn inner join keeps only the rows that appear in both data frames. So if an ID exists in one but not the other, it’s dropped.\n\ndf_ov_subset %&gt;% \n  inner_join(df_exp_subset, by = \"unique_patient_ID\")\n\n# A tibble: 3 × 8\n  unique_patient_ID sample_recode primarysite vital_status COL10A1 COL11A1\n  &lt;chr&gt;             &lt;chr&gt;         &lt;fct&gt;       &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 TCGA-01-0631      Healthy       &lt;NA&gt;        &lt;NA&gt;            3.64    4.90\n2 TCGA-01-0633      Healthy       &lt;NA&gt;        &lt;NA&gt;            3.50    2.97\n3 TCGA-01-0636      Healthy       &lt;NA&gt;        &lt;NA&gt;            3.28    2.93\n# ℹ 2 more variables: COL11A2 &lt;dbl&gt;, COL13A1 &lt;dbl&gt;\n\n# N row:\n# intersect(df_exp_subset$unique_patient_ID, df_ov_subset$unique_patient_ID) %&gt;% length()\n\n\n\nLeft Join\nA left join keeps all rows from df_ov_subset, and matches info from df_exp_subset wherever possible. Unmatched rows from df_ov_subset get NA for the new columns.\n\ndf_ov_subset %&gt;% \n  left_join(df_exp_subset, by = \"unique_patient_ID\")\n\n# A tibble: 5 × 8\n  unique_patient_ID sample_recode primarysite vital_status COL10A1 COL11A1\n  &lt;chr&gt;             &lt;chr&gt;         &lt;fct&gt;       &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 TCGA-01-0631      Healthy       &lt;NA&gt;        &lt;NA&gt;            3.64    4.90\n2 TCGA-01-0633      Healthy       &lt;NA&gt;        &lt;NA&gt;            3.50    2.97\n3 TCGA-01-0636      Healthy       &lt;NA&gt;        &lt;NA&gt;            3.28    2.93\n4 TCGA-01-0637      Healthy       &lt;NA&gt;        &lt;NA&gt;           NA      NA   \n5 TCGA-01-0639      Healthy       &lt;NA&gt;        &lt;NA&gt;           NA      NA   \n# ℹ 2 more variables: COL11A2 &lt;dbl&gt;, COL13A1 &lt;dbl&gt;\n\n# N row:\n# df_ov_subset$unique_patient_ID %&gt;% length()\n\n\n\nRight Join\nA right join is the opposite: it keeps all rows from df_ov_subset and adds matching data from df_exp_subset wherever it can.\n\ndf_ov_subset %&gt;% \n  right_join(df_exp_subset, by = \"unique_patient_ID\")\n\n# A tibble: 5 × 8\n  unique_patient_ID sample_recode primarysite vital_status COL10A1 COL11A1\n  &lt;chr&gt;             &lt;chr&gt;         &lt;fct&gt;       &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 TCGA-01-0631      Healthy       &lt;NA&gt;        &lt;NA&gt;            3.64    4.90\n2 TCGA-01-0633      Healthy       &lt;NA&gt;        &lt;NA&gt;            3.50    2.97\n3 TCGA-01-0636      Healthy       &lt;NA&gt;        &lt;NA&gt;            3.28    2.93\n4 TCGA-01-0628      &lt;NA&gt;          &lt;NA&gt;        &lt;NA&gt;            2.99    3.00\n5 TCGA-01-0630      &lt;NA&gt;          &lt;NA&gt;        &lt;NA&gt;            3.63    2.76\n# ℹ 2 more variables: COL11A2 &lt;dbl&gt;, COL13A1 &lt;dbl&gt;\n\n# N row:\n# df_exp_subset$unique_patient_ID %&gt;% length()\n\n\n\nLet’s join our datasets\nWe will join the two full datasets.\n\ndf_comb &lt;- df_ov %&gt;% \n  left_join(df_exp, by = \"unique_patient_ID\")\n\n\n\nRecap\nBy now, we have:\n\nLoaded and explored clinical and expression datasets\nCleaned whitespace, fixed types, dropped unnecessary variables\nCreated new columns through logic and string operations\nMerged the clinical and expression data for downstream analysis\nYour data is now prepped and ready for modeling, visualization, or more advanced statistical analysis.\n\nYour dataset df_comb is now ready for summary statistics and exploratory data analysis (EDA).\n\n# save(df_comb, file=\"../data/Ovarian_comb_clean.RData\")",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 1: Data Cleaning"
    ]
  },
  {
    "objectID": "presentations/presentation4A.html",
    "href": "presentations/presentation4A.html",
    "title": "Presentation 4A: Scripting in R - Conditions and For-loops",
    "section": "",
    "text": "In this section we will learn more about flow control and how to make more complex code constructs in R.\nlibrary(tidyverse)",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 4A: Conditions and For-loops"
    ]
  },
  {
    "objectID": "presentations/presentation4A.html#if-else-statments",
    "href": "presentations/presentation4A.html#if-else-statments",
    "title": "Presentation 4A: Scripting in R - Conditions and For-loops",
    "section": "If-else statments",
    "text": "If-else statments\nIf-else statements are essential if you want your program to do different things depending on a condition. Here we see how to code them in R.\nFirst define some variables.\n\nnum1 &lt;- 8\nnum2 &lt;- 5\n\nNow that we have variables, we can test logical statement between them: Is num1 larger than num2? The result of a logical statement is always one of either TRUE or FALSE:\n\nnum1 &gt; num2\n\n[1] TRUE\n\n\nIs num1 smaller than num2?\n\nnum1 &lt; num2\n\n[1] FALSE\n\n\nWe use logical statements inside an if statement to define a condition.\n\nif (num1 &gt; num2){\n  statement &lt;- paste(num1, 'is larger than', num2)\n}\n\nprint(statement)\n\n[1] \"8 is larger than 5\"\n\n\nWe can add an else if to test multiple conditions. else is what applies when all previous checks where FALSE.\nNow we have three possible outcomes:\n\n#try with different values for num2\nnum2 &lt;- 10\n\nif (num1 &gt; num2){\n  statement &lt;- paste(num1, 'is larger than', num2)\n} else if (num1 &lt; num2) {\n  statement &lt;- paste(num1, 'is smaller than', num2)\n} else {\n  statement &lt;- paste(num1, 'is equal to', num2)\n} \n\nprint(statement)\n\n[1] \"8 is smaller than 10\"\n\n\n\nAnd and or operations\nCheck values of num1 and num2\n\nnum1\n\n[1] 8\n\nnum2\n\n[1] 10\n\n# num1 &lt;- 14\n# num2 &lt;- 12\n\nYou can give multiple conditions and check if both of them are true with the & (and-operation).\n\nif (num1 &lt; 10 & num2 &lt; 10) {\n  print('Both numbers are lower than 10')\n} else {\n  print('Both numbers are not lower than 10')\n}\n\n[1] \"Both numbers are not lower than 10\"\n\n\nYou can also check if either one or the other is true with the | (or-operation).\n\nif (num1 &lt; 10 | num2 &lt; 10) {\n  print('One or both of the numbers are lower than 10.')\n} else {\n  print('None of the numbers are not lower than 10')\n}\n\n[1] \"One or both of the numbers are lower than 10.\"\n\n\nWhen you use complex conditions it can be necessary to use parenthesis to show what should be evaluated first.\nIn the example below, we first evaluate whether num1 or num2 is larger than 10, and we will receive a TRUE or FALSE answer. We then combine that with the condition that num3 shoud be exactly 10.\n\n# num1 &lt;- 8\n# num2 &lt;- 5\n\nnum3 &lt;- 10\n\n\nif ((num1 &lt; 10 | num2 &lt; 10) & num3 == 10) {\n  print('Yes')\n} else {\n  print('No')\n}\n\n[1] \"Yes\"",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 4A: Conditions and For-loops"
    ]
  },
  {
    "objectID": "presentations/presentation4A.html#for-loops",
    "href": "presentations/presentation4A.html#for-loops",
    "title": "Presentation 4A: Scripting in R - Conditions and For-loops",
    "section": "For-loops",
    "text": "For-loops\n\nDefining a for loop\nMany functions in R are already vectorized, i.e.\n\ndf &lt;- tibble(num1 = 1:10)\ndf\n\n# A tibble: 10 × 1\n    num1\n   &lt;int&gt;\n 1     1\n 2     2\n 3     3\n 4     4\n 5     5\n 6     6\n 7     7\n 8     8\n 9     9\n10    10\n\ndf$num2 &lt;- df$num1 * 10\ndf\n\n# A tibble: 10 × 2\n    num1  num2\n   &lt;int&gt; &lt;dbl&gt;\n 1     1    10\n 2     2    20\n 3     3    30\n 4     4    40\n 5     5    50\n 6     6    60\n 7     7    70\n 8     8    80\n 9     9    90\n10    10   100\n\n\nThe above code applies * 10 to each element of column num1 without us having to invoke a loop.\nBut sometimes we want to iterate over the elements manually because the situation requires it. For that case we can use a for loop.\nWe first define a list containing both numeric and character elements.\n\nlist1 &lt;- list(1, 2, 6, 3, 2, 'hello', 'world', 'yes', 7, 8, 12, 15)\n\nTo loop through list1, we define a loop variable (here called element), which takes the value of each item in the vector, one at a time.\n\nfor (element in list1) {\n  print(element)\n}\n\n[1] 1\n[1] 2\n[1] 6\n[1] 3\n[1] 2\n[1] \"hello\"\n[1] \"world\"\n[1] \"yes\"\n[1] 7\n[1] 8\n[1] 12\n[1] 15\n\n\nThe loop variable name is arbitrary - you can call it anything. For example, we can use THIS_VARIABLE and get the same result. Point is, it does not matter what you call the variable, just avoid overwriting an important variable of your script.\n\nfor (THIS_VARIABLE in list1) {\n  print(THIS_VARIABLE)\n}\n\n[1] 1\n[1] 2\n[1] 6\n[1] 3\n[1] 2\n[1] \"hello\"\n[1] \"world\"\n[1] \"yes\"\n[1] 7\n[1] 8\n[1] 12\n[1] 15\n\n\nAfter you loop through a vector or a list, the value of the loop variable is always the last element of your vector. The variable is hence a global variable.\n\nTHIS_VARIABLE\n\n[1] 15\n\n\n\n\nLoop control\nThere are two loop control statements we can use to: next and break\nnext jumps to the next iteration. Here, we print every element in list1 and when the element is ‘hello’ we jump to the next iteration.\n\nfor (element in list1) {\n  if(element == 'hello'){\n    next\n  }\n  \n  print(element)\n}\n\n[1] 1\n[1] 2\n[1] 6\n[1] 3\n[1] 2\n[1] \"world\"\n[1] \"yes\"\n[1] 7\n[1] 8\n[1] 12\n[1] 15\n\n\nbreak ends the loop before finishing. Here, we print every element in list1 and when the element is ‘hello’ we break (end) the loop.\n\nfor (element in list1) {\n  if(element == 'hello'){\n    break\n  }\n  \n  print(element)\n}\n\n[1] 1\n[1] 2\n[1] 6\n[1] 3\n[1] 2\n\n\n\n\nWhich data constructs are iterable in R?\nVectors:\n\nmy_vector &lt;- c(1, 2, 3, 4, 5)\nfor (elem in my_vector) {\n  print(elem)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nLists:\n\nmy_list &lt;- list(a = 1, b = \"Hello\", c = TRUE)\nfor (elem in my_list) {\n  print(elem)\n}\n\n[1] 1\n[1] \"Hello\"\n[1] TRUE\n\n\nDataframes and tibbles:\n\nmy_df &lt;- data.frame(A = 1:3, B = c(\"X\", \"Y\", \"Z\"))\nmy_df\n\n  A B\n1 1 X\n2 2 Y\n3 3 Z\n\n#column-wise\n\nfor (col in my_df) {\n  print(col)\n}\n\n[1] 1 2 3\n[1] \"X\" \"Y\" \"Z\"\n\n\nFor row-wise iteration you can for example use the row index:\n\nfor (i in 1:nrow(my_df)) {\n  print(i)\n  #print row i\n  print(my_df[i,])\n}\n\n[1] 1\n  A B\n1 1 X\n[1] 2\n  A B\n2 2 Y\n[1] 3\n  A B\n3 3 Z\n\n\n\n\nPlotting in loops\nCreate data\n\nplot_data_1 &lt;- tibble(Name = c('Marie', 'Marie', 'Emma', 'Sofie', 'Sarah', 'Sofie', 'Hannah', 'Lise', 'Emma'),\n                      Class = c('1.A', '1.A', '1.A', '1.A', '1.B', '1.B', '1.B', '1.C', '1.C'),\n                      Food = c('Lasagna', 'Pizza', 'Pizza', 'Bruger', 'Lasagna', 'Lasagna', 'Lasagna', 'Burger', 'Lasagna'),\n                      Age = c(6, 6, 6, 6, 6, 5, 7, 6, 6))\n\nhead(plot_data_1, n = 2)\n\n# A tibble: 2 × 4\n  Name  Class Food      Age\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Marie 1.A   Lasagna     6\n2 Marie 1.A   Pizza       6\n\n\nBarplot of each variable.\n\nggplot(plot_data_1, \n       aes(x = Name)) + \n  geom_bar()\n\n\n\n\n\n\n\nggplot(plot_data_1, \n       aes(x = Class)) + \n  geom_bar()\n\n\n\n\n\n\n\n# and so on ...\n\nLet’s do it in a for loop!\nFirst, let’s check that the variables we are interested in are iterated correctly.\n\nfor (col in colnames(plot_data_1)){\n  print(col)\n}\n\n[1] \"Name\"\n[1] \"Class\"\n[1] \"Food\"\n[1] \"Age\"\n\n\nGreat! Now, let’s add the plot function to our for loop.\n\nfor (col in colnames(plot_data_1)){\n  p &lt;- ggplot(plot_data_1, \n       aes(x = col)) + \n  geom_bar()\n  \n  print(p)\n}\n\nWarning in geom_bar(): All aesthetics have length 1, but the data has 9 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nWarning in geom_bar(): All aesthetics have length 1, but the data has 9 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nWarning in geom_bar(): All aesthetics have length 1, but the data has 9 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nWarning in geom_bar(): All aesthetics have length 1, but the data has 9 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nThat was not what we wanted…\nWrap the !!sym() function around the string formatted column name when passing it in aesthetic.\n\nsym() turns a string into a symbol (column reference)\n!! unquotes the symbol to use it in aes()\n\n\nfor (col in colnames(plot_data_1)){\n  p &lt;- ggplot(plot_data_1, \n       aes(x = !!sym(col))) + \n  geom_bar()\n  \n  print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n!!sym(col) should also be used for other tidyverse operations (filter, select, …) where you pass the column names in string format.\n\n\nIf-else in loops\nWe can now use what we have learned to loop through our list1 and multiply all numeric values with 10:\n\n#to remember contents:\nlist1\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 3\n\n[[5]]\n[1] 2\n\n[[6]]\n[1] \"hello\"\n\n[[7]]\n[1] \"world\"\n\n[[8]]\n[1] \"yes\"\n\n[[9]]\n[1] 7\n\n[[10]]\n[1] 8\n\n[[11]]\n[1] 12\n\n[[12]]\n[1] 15\n\n\n\nfor (element in list1) {\n  if (is.numeric(element)){\n    statement &lt;- paste(element, 'times 10 is', element*10)\n  } else {\n    statement &lt;- paste(element, 'is not a number!')\n  }\n  print(statement)\n}\n\n[1] \"1 times 10 is 10\"\n[1] \"2 times 10 is 20\"\n[1] \"6 times 10 is 60\"\n[1] \"3 times 10 is 30\"\n[1] \"2 times 10 is 20\"\n[1] \"hello is not a number!\"\n[1] \"world is not a number!\"\n[1] \"yes is not a number!\"\n[1] \"7 times 10 is 70\"\n[1] \"8 times 10 is 80\"\n[1] \"12 times 10 is 120\"\n[1] \"15 times 10 is 150\"\n\n\nNote: that this does not work with a vector, i.e. vec &lt;- c(1,2,'hello') because vectors can only contain one data type so all elements of vec are characters.\n\nSave results\nLet’s say we want to save the non-numeric values. We can do this in a list.\nInitiate list for saving non-numeric values.\n\nnon_numeric_values &lt;- list()\n\nAppend the non-numeric values to the list.\n\nfor (element in list1) {\n  if (is.numeric(element)){\n    next\n  } else {\n    non_numeric_values &lt;- append(non_numeric_values, element)\n  }\n}\n\nView list.\n\nnon_numeric_values\n\n[[1]]\n[1] \"hello\"\n\n[[2]]\n[1] \"world\"\n\n[[3]]\n[1] \"yes\"",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 4A: Conditions and For-loops"
    ]
  },
  {
    "objectID": "presentations/presentation4A.html#while-loop",
    "href": "presentations/presentation4A.html#while-loop",
    "title": "Presentation 4A: Scripting in R - Conditions and For-loops",
    "section": "While-loop",
    "text": "While-loop\nWhile-loops are not commonly used in R since they are ineffective in this language. We will demonstrate how they work since they are common in other programming languages.\nSay we want to create a list where each element is the letter “A” followed by a number, starting from 0 and going up to 99 We can do this using a while-loop that keeps appending “A_” to a list until the count reaches 100:\nFirst, let’s initially the counter count that will count the number of times we go through the while-loop.\n\ncount &lt;- 0\nlist2 &lt;- list()\n\nNow, we can make our while-loop. In each iteration we add 1 to the counter.\n\nwhile (count &lt; 100) {\n  list2 &lt;- append(list2, paste('A_', count, sep = ''))\n  count = count + 1\n}\n\nCheck count\n\ncount\n\n[1] 100\n\n\nCheck length of list as well as the first and the last element.\n\nlength(list2) \n\n[1] 100\n\nlist2[[1]]\n\n[1] \"A_0\"\n\nlist2[[length(list2)]]\n\n[1] \"A_99\"\n\n\nThey way you would do it in R:\n\nlist3 &lt;- paste('A_', 1:100, sep = '') %&gt;% as.list()\n# list3",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 4A: Conditions and For-loops"
    ]
  },
  {
    "objectID": "presentations/presentation5B.html",
    "href": "presentations/presentation5B.html",
    "title": "Presentation 5B: Modelling in R",
    "section": "",
    "text": "Elastic Net regression is part of the family of penalized regressions, which also includes Ridge regression and LASSO regression. Penalized regressions are useful when dealing with many predictors, as they help eliminate less informative ones while retaining the important predictors, making them ideal for high-dimensional datasets.\nIf you are interested in knowing more about penalized regressions, you can have a look at this excellent tutorial from Datacamp.\nIn linear regression, we estimate the relationship between predictors \\(X\\) and an outcome \\(y\\) using parameters \\(β\\), chosen to minimize the residual sum of squares (RSS). Two key properties of these \\(β\\) estimates are bias (the difference between the true parameter and the estimate) and variance (how much the estimates vary across different samples). While OLS (Ordinary Least Squares) gives unbiased estimates, it can suffer from high variance - especially when predictors are numerous or highly correlated — leading to poor predictions.\nTo address this, we can introduce regularization, which slightly biases the estimates in exchange for reduced variance and improved predictive performance. This trade-off is essential: as model complexity grows, variance increases and bias decreases, so regularization helps find a better balance between the two.\n\n\n\n\n\nRidge Regression = L2 penalty (adds the sum of the squares of the coefficients to the loss function). It discourages large coefficients by penalizing their squared magnitudes, shrinking them towards zero. This reduces overfitting while keeping all variables in the model.\nLasso Regression = L1 penalty (adds the sum of the absolute values of the coefficients to the loss function). This penalty encourages sparsity, causing some coefficients to become exactly zero for large \\(λ\\), thereby performing variable selection.\nElastic Net combines L1 and L2 penalties to balance variable selection and coefficient shrinkage. One of the key advantages of Elastic Net over other types of penalized regression is its ability to handle multicollinearity and situations where the number of predictors exceeds the number of observations.\nLambda, \\((λ)\\), controls the strength of the penalty. As \\(λ\\) increases, variance decreases and bias increases, raising the key question: how much bias are we willing to accept to reduce variance?\n\n\n\n\n\n\n\nLoad the R packages needed for analysis:\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(randomForest)\n\nFor this exercise we will use a dataset from patients with Heart Disease. Information on the columns in the dataset can be found here.\n\nHD &lt;- read_csv(\"../data/HeartDisease.csv\")\n\nhead(HD)\n\n# A tibble: 6 × 14\n    age   sex chestPainType restBP  chol fastingBP restElecCardio maxHR\n  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1    52     1             0    125   212         0              1   168\n2    53     1             0    140   203         1              0   155\n3    70     1             0    145   174         0              1   125\n4    61     1             0    148   203         0              1   161\n5    62     0             0    138   294         1              1   106\n6    58     0             0    100   248         0              0   122\n# ℹ 6 more variables: exerciseAngina &lt;dbl&gt;, STdepEKG &lt;dbl&gt;,\n#   slopePeakExST &lt;dbl&gt;, nMajorVessels &lt;dbl&gt;, DefectType &lt;dbl&gt;,\n#   heartDisease &lt;dbl&gt;\n\n\nFirstly, let’s convert some of the variables that are encoded as numeric datatypes but should be factors:\n\nfacCols &lt;- c(\"sex\", \n             \"chestPainType\", \n             \"fastingBP\", \n             \"restElecCardio\", \n             \"exerciseAngina\", \n             \"slopePeakExST\", \n             \"DefectType\", \n             \"heartDisease\")\n\n\nHD &lt;- HD %&gt;% \n  mutate_if(names(.) %in% facCols, as.factor) \n\nhead(HD)\n\n# A tibble: 6 × 14\n    age sex   chestPainType restBP  chol fastingBP restElecCardio maxHR\n  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;          &lt;dbl&gt;\n1    52 1     0                125   212 0         1                168\n2    53 1     0                140   203 1         0                155\n3    70 1     0                145   174 0         1                125\n4    61 1     0                148   203 0         1                161\n5    62 0     0                138   294 1         1                106\n6    58 0     0                100   248 0         0                122\n# ℹ 6 more variables: exerciseAngina &lt;fct&gt;, STdepEKG &lt;dbl&gt;,\n#   slopePeakExST &lt;fct&gt;, nMajorVessels &lt;dbl&gt;, DefectType &lt;fct&gt;,\n#   heartDisease &lt;fct&gt;\n\n\nLet’s do some summary statistics to have a look at the variables we have in our dataset. Firstly, the numeric columns. We can get a quick overview of variable distributions and ranges with some histograms.\n\n# Reshape data to long format for ggplot2\nlong_data &lt;- HD %&gt;% \n  dplyr::select(where(is.numeric)) %&gt;%\n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"value\")\n\nhead(long_data)\n\n# A tibble: 6 × 2\n  variable      value\n  &lt;chr&gt;         &lt;dbl&gt;\n1 age              52\n2 restBP          125\n3 chol            212\n4 maxHR           168\n5 STdepEKG          1\n6 nMajorVessels     2\n\n\n\n# Plot histograms for each numeric variable in one grid\nggplot(long_data, \n       aes(x = value)) +\n  geom_histogram(binwidth = 0.5, fill = \"#9395D3\", color ='grey30') +\n  facet_wrap(vars(variable), scales = \"free\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn opposition to hypothesis tests and classic linear regression, penalized regression has no assumption that predictors, nor model residuals, must be normally distributed so we do not have to test that, yeah! However, it does still assume that the relationship between predictors and the outcome is linear and that observations are independent from one another.\nImportantly, penalized regression can be sensitive to large differences in the range of numeric/integer variables and it does not like missing values, so lets remove missing (if any) and scale our numeric variables.\nQuestion: What happens to the data when it is scaled?\n\nHD_EN &lt;- HD %&gt;%\n  drop_na(.) \n\n\nHD_EN &lt;-HD %&gt;%\n  mutate(across(where(is.numeric), scale))\n\nhead(HD_EN)\n\n# A tibble: 6 × 14\n  age[,1] sex   chestPainType restBP[,1] chol[,1] fastingBP restElecCardio\n    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;              &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;         \n1  -0.268 1     0                 -0.377  -0.659  0         1             \n2  -0.158 1     0                  0.479  -0.833  1         0             \n3   1.72  1     0                  0.764  -1.40   0         1             \n4   0.724 1     0                  0.936  -0.833  0         1             \n5   0.834 0     0                  0.365   0.930  1         1             \n6   0.393 0     0                 -1.80    0.0388 0         0             \n# ℹ 7 more variables: maxHR &lt;dbl[,1]&gt;, exerciseAngina &lt;fct&gt;,\n#   STdepEKG &lt;dbl[,1]&gt;, slopePeakExST &lt;fct&gt;, nMajorVessels &lt;dbl[,1]&gt;,\n#   DefectType &lt;fct&gt;, heartDisease &lt;fct&gt;\n\n\nNow, let’s check the balance of the categorical/factor variables.\n\nHD_EN %&gt;%\n  dplyr::select(where(is.factor)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n  count(Variable, Level, name = \"Count\")\n\n# A tibble: 22 × 3\n   Variable       Level Count\n   &lt;chr&gt;          &lt;fct&gt; &lt;int&gt;\n 1 DefectType     0         7\n 2 DefectType     1        64\n 3 DefectType     2       544\n 4 DefectType     3       410\n 5 chestPainType  0       497\n 6 chestPainType  1       167\n 7 chestPainType  2       284\n 8 chestPainType  3        77\n 9 exerciseAngina 0       680\n10 exerciseAngina 1       345\n# ℹ 12 more rows\n\n# OR\n\n# cat_cols &lt;- HD_EN %&gt;% dplyr::select(where(is.factor)) %&gt;% colnames()\n# \n# for (col in cat_cols){\n#   print(col)\n#   print(table(HD_EN[[col]]))\n# }\n\nFrom our count table above we see that variables DefectType, chestPainType, restElecCardio, and slopePeakExST are unbalanced. Especially DefectType and restElecCardio are problematic with only 7 and 15 observations for one of the factor levels.\nTo avoid issues when modelling, we will filter out these observations and re-level these two variables.\n\nHD_EN &lt;- HD_EN %&gt;% \n  filter(DefectType != 0 & restElecCardio !=2) %&gt;%\n  mutate(DefectType = as.factor(as.character(DefectType)), \n         restElecCardio = as.factor(as.character(restElecCardio)))\n\nWe will use heartDisease (0 = no, 1 = yes) as the outcome.\nWe split our dataset into train and test set, we will keep 70% of the data in the training set and take out 30% for the test set. To keep track of our train and test samples, we will make an ID variable. Importantly, afterwards we must again ensure that all levels of each factor variable are represented in both sets.\n\n# Add ID column\nHD_EN &lt;- HD_EN %&gt;% \n  mutate(ID = paste0(\"ID\", 1:nrow(HD_EN)))\n\n# Set seed\nset.seed(123)\n\n# Training set\ntrain &lt;- HD_EN %&gt;% \n  sample_frac(0.70) \n\n# Check group levels\ntrain %&gt;%\n  dplyr::select(where(is.factor)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n  count(Variable, Level, name = \"Count\")\n\n# A tibble: 20 × 3\n   Variable       Level Count\n   &lt;chr&gt;          &lt;fct&gt; &lt;int&gt;\n 1 DefectType     1        42\n 2 DefectType     2       374\n 3 DefectType     3       286\n 4 chestPainType  0       330\n 5 chestPainType  1       120\n 6 chestPainType  2       196\n 7 chestPainType  3        56\n 8 exerciseAngina 0       478\n 9 exerciseAngina 1       224\n10 fastingBP      0       601\n11 fastingBP      1       101\n12 heartDisease   0       333\n13 heartDisease   1       369\n14 restElecCardio 0       346\n15 restElecCardio 1       356\n16 sex            0       209\n17 sex            1       493\n18 slopePeakExST  0        50\n19 slopePeakExST  1       323\n20 slopePeakExST  2       329\n\n\n\n# Test set\ntest  &lt;- anti_join(HD_EN, train, by = 'ID') \n\n# Check group levels\ntest %&gt;%\n  dplyr::select(where(is.factor)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n  count(Variable, Level, name = \"Count\")\n\n# A tibble: 20 × 3\n   Variable       Level Count\n   &lt;chr&gt;          &lt;fct&gt; &lt;int&gt;\n 1 DefectType     1        18\n 2 DefectType     2       163\n 3 DefectType     3       120\n 4 chestPainType  0       151\n 5 chestPainType  1        47\n 6 chestPainType  2        82\n 7 chestPainType  3        21\n 8 exerciseAngina 0       192\n 9 exerciseAngina 1       109\n10 fastingBP      0       253\n11 fastingBP      1        48\n12 heartDisease   0       150\n13 heartDisease   1       151\n14 restElecCardio 0       148\n15 restElecCardio 1       153\n16 sex            0        89\n17 sex            1       212\n18 slopePeakExST  0        20\n19 slopePeakExST  1       144\n20 slopePeakExST  2       137\n\n\nAfter dividing into train and test set, we pull out the outcome variable heartDisease into its own vector for both datasets, we name these: y_train and y_test.\n\ny_train &lt;- train %&gt;%\n  pull(heartDisease)\n\ny_test &lt;- test %&gt;% \n  pull(heartDisease)\n\nNext, we remove the outcome variable heartDisease from the train and test set, as well as ID as we should obviously not use this for training or testing.\n\ntrain &lt;- train %&gt;% \n  dplyr::select(!c(ID, heartDisease))\n\ntest &lt;- test %&gt;% \n  dplyr::select(!c(ID, heartDisease))\n\nAnother benefit of a regularized regression, such as Elastic Net, is that this type of model can accommodate a categorical or a numeric variable as outcome, and handle a mix of these types as predictors, making it super flexible.\nHowever, if you do use categorical variables as your predictors within the glmnet() function, you need to ensure that your variables are Dummy-coded (one-hot encoded). Dummy-coding means that categorical levels are converted into binary numeric indicators. You can do this ‘manually’, but there is a super easy way to do it with model.matrix() as shown below.\nLet’s create the model matrix needed for input to glmnet() and cv.glmnet() functions:\n\nmodTrain &lt;- model.matrix(~ .- 1, data = train)\nhead(modTrain)\n\n          age sex0 sex1 chestPainType1 chestPainType2 chestPainType3\n1  0.28282315    1    0              0              0              0\n2  0.61350040    1    0              0              0              1\n3 -1.37056311    0    1              0              1              0\n4 -0.04785411    1    0              0              1              0\n5 -0.48875711    0    1              0              0              0\n6  0.83395190    0    1              0              1              0\n       restBP       chol fastingBP1 restElecCardio1       maxHR exerciseAngina1\n1 -0.66289286  2.0933271          0               1  0.60358256               1\n2  1.04975673 -0.1162960          0               1  0.95132211               0\n3 -0.09200966 -1.2792555          0               1  0.03850579               0\n4 -1.34795270  0.4070358          0               0  0.77745234               0\n5  0.70722681 -0.8916023          0               0 -1.00471285               1\n6 -0.09200966 -0.2907399          0               1 -0.13536398               0\n    STdepEKG slopePeakExST1 slopePeakExST2 nMajorVessels DefectType2\n1 -0.4012688              0              1    -0.7316143           1\n2 -0.1459612              0              1    -0.7316143           1\n3 -0.9118839              0              1    -0.7316143           1\n4 -0.9118839              0              1    -0.7316143           1\n5 -0.1459612              1              0    -0.7316143           0\n6  0.6199615              1              0     2.1787531           0\n  DefectType3\n1           0\n2           0\n3           0\n4           0\n5           1\n6           1\n\nmodTest &lt;- model.matrix(~ .- 1,, data = test)\n\nLet’s create your Elastic Net Regression model with glmnet().\nThe parameter \\(α\\) essentially tells glmnet() whether we are performing Ridge Regression (\\(α\\) = 0), LASSO regression (\\(α\\) = 1) or Elastic Net regression (0 &lt; \\(α\\) &lt; 1 ). Furthermore, like for logistic regression we must specify if our outcome is; binominal, multinomial, gaussian, etc.\n\nEN_model &lt;- glmnet(modTrain, y_train, alpha = 0.5, family = \"binomial\")\n\nAs you may recall from the first part of this presentation, penalized regressions have a hyperparameter, \\(λ\\), which determines the strength of shrinkage of the coefficients. We can use k-fold cross-validation to find the value of lambda that minimizes the cross-validated prediction error. For classification problems such as the one we have here the prediction error is usually binominal deviance, which is related to log-loss (a measure of how well predicted probabilities match true class labels).\nLet’s use cv.glmnet() to attain the best value of the hyperparameter lambda (\\(λ\\)). We should remember to set a seed for reproducible results.\n\nset.seed(123)\ncv_model &lt;- cv.glmnet(modTrain, y_train, alpha = 0.5, family = \"binomial\")\n\nWe can plot all the values of lambda tested during cross validation by calling plot() on the output of your cv.glmnet() and we can extract the best lambda value from the cv.glmnet() model and save it as an object.\n\nplot(cv_model)\n\n\n\n\n\n\n\nbestLambda &lt;- cv_model$lambda.min\n\nThe plot shows how the model’s prediction error changes with different values of lambda, \\(λ\\). It helps to identify the largest \\(λ\\) we can choose before the penalty starts to hurt performance - too much shrinkage can remove important predictors, increasing prediction error.\n\nbestLambda\n\n[1] 0.003892802\n\nlog(bestLambda)\n\n[1] -5.548626\n\n\nTime to see how well our model performs. Let’s predict if a individual is likely to have heart disease using our model and our test set.\n\ny_pred &lt;- predict(EN_model, s = bestLambda, newx = modTest, type = 'class')\n\nJust like for the logistic regression model we can calculate the accuracy of the prediction by comparing it to y_test with confusionMatrix().\n\ny_pred &lt;- as.factor(y_pred)\n\ncaret::confusionMatrix(y_pred, y_test)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 124  21\n         1  26 130\n                                         \n               Accuracy : 0.8439         \n                 95% CI : (0.7978, 0.883)\n    No Information Rate : 0.5017         \n    P-Value [Acc &gt; NIR] : &lt;2e-16         \n                                         \n                  Kappa : 0.6877         \n                                         \n Mcnemar's Test P-Value : 0.5596         \n                                         \n            Sensitivity : 0.8267         \n            Specificity : 0.8609         \n         Pos Pred Value : 0.8552         \n         Neg Pred Value : 0.8333         \n             Prevalence : 0.4983         \n         Detection Rate : 0.4120         \n   Detection Prevalence : 0.4817         \n      Balanced Accuracy : 0.8438         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nOur model performs relatively well with a Balanced Accuracy of 0.84.\nJust like with linear or logistic regression, we can pull out the coefficients (weights) from our model to asses which variable(s) are the most explanatory for heart disease. We use the function coef() for this.\n\ncoeffs &lt;- coef(EN_model, s = bestLambda)\n\ncoeffs\n\n19 x 1 sparse Matrix of class \"dgCMatrix\"\n                         s1\n(Intercept)      0.22782401\nage             -0.05823254\nsex0             0.75259034\nsex1            -0.70972944\nchestPainType1   1.26049789\nchestPainType2   1.78481019\nchestPainType3   1.55254904\nrestBP          -0.39726763\nchol            -0.25270564\nfastingBP1       0.26701756\nrestElecCardio1  0.48761187\nmaxHR            0.46555870\nexerciseAngina1 -0.53368076\nSTdepEKG        -0.48693148\nslopePeakExST1  -0.94379010\nslopePeakExST2   0.05576804\nnMajorVessels   -0.75202020\nDefectType2      0.00917183\nDefectType3     -1.23099327\n\n\nFirst of all we see that none of our explanatory variables have been penalized so much that they have been removed, although some like age contribute very little to the model.\nLet’s order the coefficients by size and plot them to get an easy overview. First we do a little data wrangling to set up the dataset.\n\ncoeffsDat &lt;- as.data.frame(as.matrix(coeffs)) %&gt;% \n  rownames_to_column(var = 'VarName') %&gt;%\n  arrange(desc(s1)) %&gt;%  \n  filter(!str_detect(VarName,\"(Intercept)\")) %&gt;% \n  mutate(VarName = factor(VarName, levels=VarName))\n\nNow we can make a bar plot to visualize our results.\n\n# Plot\nggplot(coeffsDat, aes(x = VarName, y = s1)) +\n  geom_bar(stat = \"identity\", fill = \"#9395D3\") +\n  coord_flip() +\n  labs(title = \"Feature Importance for Elastic Net\", \n       x = \"Features\", \n       y = \"Absolute Coefficients\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nFrom the coefficients above it seem like cheat pain of any type (0 vs 1, 2 or 3) is a strong predictor of the outcome, e.g. heart disease. In opposition, having a DefectType3 significantly lowers the predicted probability of belonging to the event class (1 = Heart Disease). Specifically, it decreases the log-odds of belonging to class 1.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 5B: Models and Model Evaluation in R"
    ]
  },
  {
    "objectID": "presentations/presentation5B.html#part-1-penalized-regression",
    "href": "presentations/presentation5B.html#part-1-penalized-regression",
    "title": "Presentation 5B: Modelling in R",
    "section": "",
    "text": "Elastic Net regression is part of the family of penalized regressions, which also includes Ridge regression and LASSO regression. Penalized regressions are useful when dealing with many predictors, as they help eliminate less informative ones while retaining the important predictors, making them ideal for high-dimensional datasets.\nIf you are interested in knowing more about penalized regressions, you can have a look at this excellent tutorial from Datacamp.\nIn linear regression, we estimate the relationship between predictors \\(X\\) and an outcome \\(y\\) using parameters \\(β\\), chosen to minimize the residual sum of squares (RSS). Two key properties of these \\(β\\) estimates are bias (the difference between the true parameter and the estimate) and variance (how much the estimates vary across different samples). While OLS (Ordinary Least Squares) gives unbiased estimates, it can suffer from high variance - especially when predictors are numerous or highly correlated — leading to poor predictions.\nTo address this, we can introduce regularization, which slightly biases the estimates in exchange for reduced variance and improved predictive performance. This trade-off is essential: as model complexity grows, variance increases and bias decreases, so regularization helps find a better balance between the two.\n\n\n\n\n\nRidge Regression = L2 penalty (adds the sum of the squares of the coefficients to the loss function). It discourages large coefficients by penalizing their squared magnitudes, shrinking them towards zero. This reduces overfitting while keeping all variables in the model.\nLasso Regression = L1 penalty (adds the sum of the absolute values of the coefficients to the loss function). This penalty encourages sparsity, causing some coefficients to become exactly zero for large \\(λ\\), thereby performing variable selection.\nElastic Net combines L1 and L2 penalties to balance variable selection and coefficient shrinkage. One of the key advantages of Elastic Net over other types of penalized regression is its ability to handle multicollinearity and situations where the number of predictors exceeds the number of observations.\nLambda, \\((λ)\\), controls the strength of the penalty. As \\(λ\\) increases, variance decreases and bias increases, raising the key question: how much bias are we willing to accept to reduce variance?\n\n\n\n\n\n\n\nLoad the R packages needed for analysis:\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(randomForest)\n\nFor this exercise we will use a dataset from patients with Heart Disease. Information on the columns in the dataset can be found here.\n\nHD &lt;- read_csv(\"../data/HeartDisease.csv\")\n\nhead(HD)\n\n# A tibble: 6 × 14\n    age   sex chestPainType restBP  chol fastingBP restElecCardio maxHR\n  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1    52     1             0    125   212         0              1   168\n2    53     1             0    140   203         1              0   155\n3    70     1             0    145   174         0              1   125\n4    61     1             0    148   203         0              1   161\n5    62     0             0    138   294         1              1   106\n6    58     0             0    100   248         0              0   122\n# ℹ 6 more variables: exerciseAngina &lt;dbl&gt;, STdepEKG &lt;dbl&gt;,\n#   slopePeakExST &lt;dbl&gt;, nMajorVessels &lt;dbl&gt;, DefectType &lt;dbl&gt;,\n#   heartDisease &lt;dbl&gt;\n\n\nFirstly, let’s convert some of the variables that are encoded as numeric datatypes but should be factors:\n\nfacCols &lt;- c(\"sex\", \n             \"chestPainType\", \n             \"fastingBP\", \n             \"restElecCardio\", \n             \"exerciseAngina\", \n             \"slopePeakExST\", \n             \"DefectType\", \n             \"heartDisease\")\n\n\nHD &lt;- HD %&gt;% \n  mutate_if(names(.) %in% facCols, as.factor) \n\nhead(HD)\n\n# A tibble: 6 × 14\n    age sex   chestPainType restBP  chol fastingBP restElecCardio maxHR\n  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;          &lt;dbl&gt;\n1    52 1     0                125   212 0         1                168\n2    53 1     0                140   203 1         0                155\n3    70 1     0                145   174 0         1                125\n4    61 1     0                148   203 0         1                161\n5    62 0     0                138   294 1         1                106\n6    58 0     0                100   248 0         0                122\n# ℹ 6 more variables: exerciseAngina &lt;fct&gt;, STdepEKG &lt;dbl&gt;,\n#   slopePeakExST &lt;fct&gt;, nMajorVessels &lt;dbl&gt;, DefectType &lt;fct&gt;,\n#   heartDisease &lt;fct&gt;\n\n\nLet’s do some summary statistics to have a look at the variables we have in our dataset. Firstly, the numeric columns. We can get a quick overview of variable distributions and ranges with some histograms.\n\n# Reshape data to long format for ggplot2\nlong_data &lt;- HD %&gt;% \n  dplyr::select(where(is.numeric)) %&gt;%\n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"value\")\n\nhead(long_data)\n\n# A tibble: 6 × 2\n  variable      value\n  &lt;chr&gt;         &lt;dbl&gt;\n1 age              52\n2 restBP          125\n3 chol            212\n4 maxHR           168\n5 STdepEKG          1\n6 nMajorVessels     2\n\n\n\n# Plot histograms for each numeric variable in one grid\nggplot(long_data, \n       aes(x = value)) +\n  geom_histogram(binwidth = 0.5, fill = \"#9395D3\", color ='grey30') +\n  facet_wrap(vars(variable), scales = \"free\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn opposition to hypothesis tests and classic linear regression, penalized regression has no assumption that predictors, nor model residuals, must be normally distributed so we do not have to test that, yeah! However, it does still assume that the relationship between predictors and the outcome is linear and that observations are independent from one another.\nImportantly, penalized regression can be sensitive to large differences in the range of numeric/integer variables and it does not like missing values, so lets remove missing (if any) and scale our numeric variables.\nQuestion: What happens to the data when it is scaled?\n\nHD_EN &lt;- HD %&gt;%\n  drop_na(.) \n\n\nHD_EN &lt;-HD %&gt;%\n  mutate(across(where(is.numeric), scale))\n\nhead(HD_EN)\n\n# A tibble: 6 × 14\n  age[,1] sex   chestPainType restBP[,1] chol[,1] fastingBP restElecCardio\n    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;              &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;         \n1  -0.268 1     0                 -0.377  -0.659  0         1             \n2  -0.158 1     0                  0.479  -0.833  1         0             \n3   1.72  1     0                  0.764  -1.40   0         1             \n4   0.724 1     0                  0.936  -0.833  0         1             \n5   0.834 0     0                  0.365   0.930  1         1             \n6   0.393 0     0                 -1.80    0.0388 0         0             \n# ℹ 7 more variables: maxHR &lt;dbl[,1]&gt;, exerciseAngina &lt;fct&gt;,\n#   STdepEKG &lt;dbl[,1]&gt;, slopePeakExST &lt;fct&gt;, nMajorVessels &lt;dbl[,1]&gt;,\n#   DefectType &lt;fct&gt;, heartDisease &lt;fct&gt;\n\n\nNow, let’s check the balance of the categorical/factor variables.\n\nHD_EN %&gt;%\n  dplyr::select(where(is.factor)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n  count(Variable, Level, name = \"Count\")\n\n# A tibble: 22 × 3\n   Variable       Level Count\n   &lt;chr&gt;          &lt;fct&gt; &lt;int&gt;\n 1 DefectType     0         7\n 2 DefectType     1        64\n 3 DefectType     2       544\n 4 DefectType     3       410\n 5 chestPainType  0       497\n 6 chestPainType  1       167\n 7 chestPainType  2       284\n 8 chestPainType  3        77\n 9 exerciseAngina 0       680\n10 exerciseAngina 1       345\n# ℹ 12 more rows\n\n# OR\n\n# cat_cols &lt;- HD_EN %&gt;% dplyr::select(where(is.factor)) %&gt;% colnames()\n# \n# for (col in cat_cols){\n#   print(col)\n#   print(table(HD_EN[[col]]))\n# }\n\nFrom our count table above we see that variables DefectType, chestPainType, restElecCardio, and slopePeakExST are unbalanced. Especially DefectType and restElecCardio are problematic with only 7 and 15 observations for one of the factor levels.\nTo avoid issues when modelling, we will filter out these observations and re-level these two variables.\n\nHD_EN &lt;- HD_EN %&gt;% \n  filter(DefectType != 0 & restElecCardio !=2) %&gt;%\n  mutate(DefectType = as.factor(as.character(DefectType)), \n         restElecCardio = as.factor(as.character(restElecCardio)))\n\nWe will use heartDisease (0 = no, 1 = yes) as the outcome.\nWe split our dataset into train and test set, we will keep 70% of the data in the training set and take out 30% for the test set. To keep track of our train and test samples, we will make an ID variable. Importantly, afterwards we must again ensure that all levels of each factor variable are represented in both sets.\n\n# Add ID column\nHD_EN &lt;- HD_EN %&gt;% \n  mutate(ID = paste0(\"ID\", 1:nrow(HD_EN)))\n\n# Set seed\nset.seed(123)\n\n# Training set\ntrain &lt;- HD_EN %&gt;% \n  sample_frac(0.70) \n\n# Check group levels\ntrain %&gt;%\n  dplyr::select(where(is.factor)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n  count(Variable, Level, name = \"Count\")\n\n# A tibble: 20 × 3\n   Variable       Level Count\n   &lt;chr&gt;          &lt;fct&gt; &lt;int&gt;\n 1 DefectType     1        42\n 2 DefectType     2       374\n 3 DefectType     3       286\n 4 chestPainType  0       330\n 5 chestPainType  1       120\n 6 chestPainType  2       196\n 7 chestPainType  3        56\n 8 exerciseAngina 0       478\n 9 exerciseAngina 1       224\n10 fastingBP      0       601\n11 fastingBP      1       101\n12 heartDisease   0       333\n13 heartDisease   1       369\n14 restElecCardio 0       346\n15 restElecCardio 1       356\n16 sex            0       209\n17 sex            1       493\n18 slopePeakExST  0        50\n19 slopePeakExST  1       323\n20 slopePeakExST  2       329\n\n\n\n# Test set\ntest  &lt;- anti_join(HD_EN, train, by = 'ID') \n\n# Check group levels\ntest %&gt;%\n  dplyr::select(where(is.factor)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n  count(Variable, Level, name = \"Count\")\n\n# A tibble: 20 × 3\n   Variable       Level Count\n   &lt;chr&gt;          &lt;fct&gt; &lt;int&gt;\n 1 DefectType     1        18\n 2 DefectType     2       163\n 3 DefectType     3       120\n 4 chestPainType  0       151\n 5 chestPainType  1        47\n 6 chestPainType  2        82\n 7 chestPainType  3        21\n 8 exerciseAngina 0       192\n 9 exerciseAngina 1       109\n10 fastingBP      0       253\n11 fastingBP      1        48\n12 heartDisease   0       150\n13 heartDisease   1       151\n14 restElecCardio 0       148\n15 restElecCardio 1       153\n16 sex            0        89\n17 sex            1       212\n18 slopePeakExST  0        20\n19 slopePeakExST  1       144\n20 slopePeakExST  2       137\n\n\nAfter dividing into train and test set, we pull out the outcome variable heartDisease into its own vector for both datasets, we name these: y_train and y_test.\n\ny_train &lt;- train %&gt;%\n  pull(heartDisease)\n\ny_test &lt;- test %&gt;% \n  pull(heartDisease)\n\nNext, we remove the outcome variable heartDisease from the train and test set, as well as ID as we should obviously not use this for training or testing.\n\ntrain &lt;- train %&gt;% \n  dplyr::select(!c(ID, heartDisease))\n\ntest &lt;- test %&gt;% \n  dplyr::select(!c(ID, heartDisease))\n\nAnother benefit of a regularized regression, such as Elastic Net, is that this type of model can accommodate a categorical or a numeric variable as outcome, and handle a mix of these types as predictors, making it super flexible.\nHowever, if you do use categorical variables as your predictors within the glmnet() function, you need to ensure that your variables are Dummy-coded (one-hot encoded). Dummy-coding means that categorical levels are converted into binary numeric indicators. You can do this ‘manually’, but there is a super easy way to do it with model.matrix() as shown below.\nLet’s create the model matrix needed for input to glmnet() and cv.glmnet() functions:\n\nmodTrain &lt;- model.matrix(~ .- 1, data = train)\nhead(modTrain)\n\n          age sex0 sex1 chestPainType1 chestPainType2 chestPainType3\n1  0.28282315    1    0              0              0              0\n2  0.61350040    1    0              0              0              1\n3 -1.37056311    0    1              0              1              0\n4 -0.04785411    1    0              0              1              0\n5 -0.48875711    0    1              0              0              0\n6  0.83395190    0    1              0              1              0\n       restBP       chol fastingBP1 restElecCardio1       maxHR exerciseAngina1\n1 -0.66289286  2.0933271          0               1  0.60358256               1\n2  1.04975673 -0.1162960          0               1  0.95132211               0\n3 -0.09200966 -1.2792555          0               1  0.03850579               0\n4 -1.34795270  0.4070358          0               0  0.77745234               0\n5  0.70722681 -0.8916023          0               0 -1.00471285               1\n6 -0.09200966 -0.2907399          0               1 -0.13536398               0\n    STdepEKG slopePeakExST1 slopePeakExST2 nMajorVessels DefectType2\n1 -0.4012688              0              1    -0.7316143           1\n2 -0.1459612              0              1    -0.7316143           1\n3 -0.9118839              0              1    -0.7316143           1\n4 -0.9118839              0              1    -0.7316143           1\n5 -0.1459612              1              0    -0.7316143           0\n6  0.6199615              1              0     2.1787531           0\n  DefectType3\n1           0\n2           0\n3           0\n4           0\n5           1\n6           1\n\nmodTest &lt;- model.matrix(~ .- 1,, data = test)\n\nLet’s create your Elastic Net Regression model with glmnet().\nThe parameter \\(α\\) essentially tells glmnet() whether we are performing Ridge Regression (\\(α\\) = 0), LASSO regression (\\(α\\) = 1) or Elastic Net regression (0 &lt; \\(α\\) &lt; 1 ). Furthermore, like for logistic regression we must specify if our outcome is; binominal, multinomial, gaussian, etc.\n\nEN_model &lt;- glmnet(modTrain, y_train, alpha = 0.5, family = \"binomial\")\n\nAs you may recall from the first part of this presentation, penalized regressions have a hyperparameter, \\(λ\\), which determines the strength of shrinkage of the coefficients. We can use k-fold cross-validation to find the value of lambda that minimizes the cross-validated prediction error. For classification problems such as the one we have here the prediction error is usually binominal deviance, which is related to log-loss (a measure of how well predicted probabilities match true class labels).\nLet’s use cv.glmnet() to attain the best value of the hyperparameter lambda (\\(λ\\)). We should remember to set a seed for reproducible results.\n\nset.seed(123)\ncv_model &lt;- cv.glmnet(modTrain, y_train, alpha = 0.5, family = \"binomial\")\n\nWe can plot all the values of lambda tested during cross validation by calling plot() on the output of your cv.glmnet() and we can extract the best lambda value from the cv.glmnet() model and save it as an object.\n\nplot(cv_model)\n\n\n\n\n\n\n\nbestLambda &lt;- cv_model$lambda.min\n\nThe plot shows how the model’s prediction error changes with different values of lambda, \\(λ\\). It helps to identify the largest \\(λ\\) we can choose before the penalty starts to hurt performance - too much shrinkage can remove important predictors, increasing prediction error.\n\nbestLambda\n\n[1] 0.003892802\n\nlog(bestLambda)\n\n[1] -5.548626\n\n\nTime to see how well our model performs. Let’s predict if a individual is likely to have heart disease using our model and our test set.\n\ny_pred &lt;- predict(EN_model, s = bestLambda, newx = modTest, type = 'class')\n\nJust like for the logistic regression model we can calculate the accuracy of the prediction by comparing it to y_test with confusionMatrix().\n\ny_pred &lt;- as.factor(y_pred)\n\ncaret::confusionMatrix(y_pred, y_test)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 124  21\n         1  26 130\n                                         \n               Accuracy : 0.8439         \n                 95% CI : (0.7978, 0.883)\n    No Information Rate : 0.5017         \n    P-Value [Acc &gt; NIR] : &lt;2e-16         \n                                         \n                  Kappa : 0.6877         \n                                         \n Mcnemar's Test P-Value : 0.5596         \n                                         \n            Sensitivity : 0.8267         \n            Specificity : 0.8609         \n         Pos Pred Value : 0.8552         \n         Neg Pred Value : 0.8333         \n             Prevalence : 0.4983         \n         Detection Rate : 0.4120         \n   Detection Prevalence : 0.4817         \n      Balanced Accuracy : 0.8438         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nOur model performs relatively well with a Balanced Accuracy of 0.84.\nJust like with linear or logistic regression, we can pull out the coefficients (weights) from our model to asses which variable(s) are the most explanatory for heart disease. We use the function coef() for this.\n\ncoeffs &lt;- coef(EN_model, s = bestLambda)\n\ncoeffs\n\n19 x 1 sparse Matrix of class \"dgCMatrix\"\n                         s1\n(Intercept)      0.22782401\nage             -0.05823254\nsex0             0.75259034\nsex1            -0.70972944\nchestPainType1   1.26049789\nchestPainType2   1.78481019\nchestPainType3   1.55254904\nrestBP          -0.39726763\nchol            -0.25270564\nfastingBP1       0.26701756\nrestElecCardio1  0.48761187\nmaxHR            0.46555870\nexerciseAngina1 -0.53368076\nSTdepEKG        -0.48693148\nslopePeakExST1  -0.94379010\nslopePeakExST2   0.05576804\nnMajorVessels   -0.75202020\nDefectType2      0.00917183\nDefectType3     -1.23099327\n\n\nFirst of all we see that none of our explanatory variables have been penalized so much that they have been removed, although some like age contribute very little to the model.\nLet’s order the coefficients by size and plot them to get an easy overview. First we do a little data wrangling to set up the dataset.\n\ncoeffsDat &lt;- as.data.frame(as.matrix(coeffs)) %&gt;% \n  rownames_to_column(var = 'VarName') %&gt;%\n  arrange(desc(s1)) %&gt;%  \n  filter(!str_detect(VarName,\"(Intercept)\")) %&gt;% \n  mutate(VarName = factor(VarName, levels=VarName))\n\nNow we can make a bar plot to visualize our results.\n\n# Plot\nggplot(coeffsDat, aes(x = VarName, y = s1)) +\n  geom_bar(stat = \"identity\", fill = \"#9395D3\") +\n  coord_flip() +\n  labs(title = \"Feature Importance for Elastic Net\", \n       x = \"Features\", \n       y = \"Absolute Coefficients\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nFrom the coefficients above it seem like cheat pain of any type (0 vs 1, 2 or 3) is a strong predictor of the outcome, e.g. heart disease. In opposition, having a DefectType3 significantly lowers the predicted probability of belonging to the event class (1 = Heart Disease). Specifically, it decreases the log-odds of belonging to class 1.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 5B: Models and Model Evaluation in R"
    ]
  },
  {
    "objectID": "presentations/presentation5B.html#part-2-random-forest",
    "href": "presentations/presentation5B.html#part-2-random-forest",
    "title": "Presentation 5B: Modelling in R",
    "section": "Part 2: Random Forest",
    "text": "Part 2: Random Forest\nIn this section, we will train a Random Forest (RF) model using the same dataset and outcome as above. Random Forest is a simple ensemble machine learning method that builds multiple decision trees and combines their predictions to improve accuracy and robustness. By averaging the results of many trees, it reduces overfitting and increases generalization, making it particularly effective for complex, non-linear relationships. One of its key strengths is its ability to handle large datasets with many features, while also providing insights into feature importance.\n\n\n\n\n\nWhy do we want to try a RF? Unlike linear, logistic, or elastic net regression, RF does not assume a linear relationship between predictors and the outcome — it can naturally capture non-linear patterns and complex interactions between variables.\nAnother advantage is that RF considers one predictor at a time when splitting, making it robust to differences in variable scales and allowing it to handle categorical variables directly, without requiring dummy coding.\nThe downside to a is RF model is that it typically require a reasonably large sample size to perform well and can be less interpretable compared to regression-based approaches.\n\nRF Model\nLuckily, we already have a good understanding of our dataset so we won’t spend time on exploratory data analysis. We have the data loaded already:\n\nhead(HD)\n\n# A tibble: 6 × 14\n    age sex   chestPainType restBP  chol fastingBP restElecCardio maxHR\n  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;          &lt;dbl&gt;\n1    52 1     0                125   212 0         1                168\n2    53 1     0                140   203 1         0                155\n3    70 1     0                145   174 0         1                125\n4    61 1     0                148   203 0         1                161\n5    62 0     0                138   294 1         1                106\n6    58 0     0                100   248 0         0                122\n# ℹ 6 more variables: exerciseAngina &lt;fct&gt;, STdepEKG &lt;dbl&gt;,\n#   slopePeakExST &lt;fct&gt;, nMajorVessels &lt;dbl&gt;, DefectType &lt;fct&gt;,\n#   heartDisease &lt;fct&gt;\n\n\nFor RF there is no need to scale numeric predictors or dummy code categorical predictors However, we do need to covert the outcome variable heartDisease from binary (0 or 1) to a category name (this is required by the function we will use for random forest later).\n\n# Mutate outcome to category and add ID column for splitting\nHD_RF &lt;- HD %&gt;% \n  mutate(heartDisease = as.factor(as.character(ifelse(heartDisease == 1, \"yesHD\", \"noHD\")))) %&gt;%\n  mutate(ID = paste0(\"W\", 1:nrow(HD)))\n\nhead(HD_RF$heartDisease)\n\n[1] noHD  noHD  noHD  noHD  noHD  yesHD\nLevels: noHD yesHD\n\n\nThen, we can split our dataset into training and test. You may notice that in opposition to the elastic net regression above, we keep the outcome variable in the dataset. Weather you need to keep it in or must pull it out depends on the R-package (functions) you are using.\n\n# Set seed\nset.seed(123)\n\n# Training set\ntrain &lt;- HD_RF %&gt;% \n  sample_frac(0.70) \n\n# Test set\ntest  &lt;- anti_join(HD_RF, train, by = 'ID') \n\n\n\n# Remove the ID, which we do not want to use for training.\ntrain &lt;- train %&gt;% \n  dplyr::select(-ID)\n\ntest &lt;- test %&gt;% \n  dplyr::select(-ID)\n\nNow let’s set up a Random Forest model with cross-validation - this way we do not overfit our model. The R-package caret has a very versatile function trainControl() which can be used with a range of resampling methods including bootstrapping, out-of-bag error, and leave-one-out cross-validation.\n\nset.seed(123)\n\n# Set up cross-validation: 5-fold CV\nRFcv &lt;- trainControl(\n  method = \"cv\",\n  number = 5,\n  classProbs = TRUE,\n  summaryFunction = twoClassSummary,\n  savePredictions = \"final\"\n)\n\nNow that we have set up parameters for cross validation in the RFcv object above, we can feed it to the train() function from the caret packages. We also specify the training data, the name of the outcome variable, and, importantly, that we want to perform random forest (method = \"rf\") as the train() function can be used for different models.\n\n# Train Random Forest\nset.seed(123)\nrf_model &lt;- train(\n  heartDisease ~ .,\n  data = train,\n  method = \"rf\",\n  trControl = RFcv,\n  metric = \"ROC\",\n  tuneLength = 5          \n)\n\n\n# Model summary\nprint(rf_model)\n\nRandom Forest \n\n718 samples\n 13 predictor\n  2 classes: 'noHD', 'yesHD' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 575, 574, 574, 575, 574 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.9916503  0.9464386  0.9587900\n   6    0.9978648  0.9802817  0.9670852\n  10    0.9976344  0.9830986  0.9670852\n  14    0.9972088  0.9830986  0.9670852\n  19    0.9966214  0.9830986  0.9616058\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 6.\n\n\nNext, we can plot your model fit to see how many explanatory variables significantly contribute to our model.\n\n# Best parameters\nrf_model$bestTune\n\n  mtry\n2    6\n\n# Plot performance\nplot(rf_model)\n\n\n\n\n\n\n\n\nNow, we use the test set to evaluate our model performance.\n\n# Predict class probabilities\ny_pred &lt;- predict(rf_model, newdata = test, type = \"prob\")\ny_pred\n\n     noHD yesHD\n1   0.914 0.086\n2   0.980 0.020\n3   0.928 0.072\n4   0.922 0.078\n5   0.950 0.050\n6   1.000 0.000\n7   0.782 0.218\n8   0.998 0.002\n9   0.882 0.118\n10  0.038 0.962\n11  0.002 0.998\n12  0.014 0.986\n13  0.972 0.028\n14  0.006 0.994\n15  0.000 1.000\n16  0.126 0.874\n17  0.866 0.134\n18  0.950 0.050\n19  0.066 0.934\n20  0.982 0.018\n21  0.048 0.952\n22  0.918 0.082\n23  0.008 0.992\n24  0.912 0.088\n25  0.852 0.148\n26  0.916 0.084\n27  0.990 0.010\n28  0.976 0.024\n29  0.022 0.978\n30  0.706 0.294\n31  0.000 1.000\n32  0.026 0.974\n33  0.998 0.002\n34  0.206 0.794\n35  0.890 0.110\n36  0.002 0.998\n37  0.098 0.902\n38  0.076 0.924\n39  0.932 0.068\n40  0.942 0.058\n41  0.302 0.698\n42  0.996 0.004\n43  1.000 0.000\n44  0.024 0.976\n45  0.000 1.000\n46  0.066 0.934\n47  0.320 0.680\n48  0.730 0.270\n49  0.044 0.956\n50  0.044 0.956\n51  0.990 0.010\n52  0.002 0.998\n53  0.082 0.918\n54  0.022 0.978\n55  0.126 0.874\n56  0.076 0.924\n57  0.112 0.888\n58  0.010 0.990\n59  0.998 0.002\n60  0.084 0.916\n61  0.998 0.002\n62  0.054 0.946\n63  0.286 0.714\n64  0.006 0.994\n65  0.044 0.956\n66  0.060 0.940\n67  0.964 0.036\n68  0.022 0.978\n69  0.002 0.998\n70  0.000 1.000\n71  0.706 0.294\n72  1.000 0.000\n73  0.318 0.682\n74  0.960 0.040\n75  1.000 0.000\n76  0.006 0.994\n77  0.998 0.002\n78  0.988 0.012\n79  0.000 1.000\n80  0.942 0.058\n81  0.260 0.740\n82  0.000 1.000\n83  0.942 0.058\n84  0.036 0.964\n85  0.096 0.904\n86  0.912 0.088\n87  0.006 0.994\n88  0.034 0.966\n89  0.142 0.858\n90  0.976 0.024\n91  0.988 0.012\n92  0.474 0.526\n93  0.034 0.966\n94  0.024 0.976\n95  0.048 0.952\n96  0.984 0.016\n97  0.416 0.584\n98  0.002 0.998\n99  0.068 0.932\n100 0.996 0.004\n101 0.060 0.940\n102 0.902 0.098\n103 0.024 0.976\n104 0.000 1.000\n105 0.002 0.998\n106 0.014 0.986\n107 0.882 0.118\n108 0.202 0.798\n109 0.998 0.002\n110 0.064 0.936\n111 0.028 0.972\n112 0.016 0.984\n113 0.932 0.068\n114 0.958 0.042\n115 0.002 0.998\n116 0.992 0.008\n117 0.968 0.032\n118 0.898 0.102\n119 0.068 0.932\n120 0.026 0.974\n121 0.072 0.928\n122 0.980 0.020\n123 0.038 0.962\n124 1.000 0.000\n125 0.416 0.584\n126 0.072 0.928\n127 0.004 0.996\n128 0.036 0.964\n129 0.990 0.010\n130 0.984 0.016\n131 0.002 0.998\n132 0.008 0.992\n133 0.000 1.000\n134 0.990 0.010\n135 0.024 0.976\n136 0.076 0.924\n137 0.990 0.010\n138 0.000 1.000\n139 0.994 0.006\n140 0.868 0.132\n141 1.000 0.000\n142 0.084 0.916\n143 0.206 0.794\n144 0.036 0.964\n145 0.318 0.682\n146 0.230 0.770\n147 0.964 0.036\n148 0.008 0.992\n149 0.064 0.936\n150 0.002 0.998\n151 0.942 0.058\n152 0.142 0.858\n153 1.000 0.000\n154 0.956 0.044\n155 0.868 0.132\n156 0.980 0.020\n157 0.000 1.000\n158 0.928 0.072\n159 0.982 0.018\n160 0.882 0.118\n161 0.916 0.084\n162 0.286 0.714\n163 0.988 0.012\n164 0.012 0.988\n165 0.024 0.976\n166 0.014 0.986\n167 0.970 0.030\n168 0.230 0.770\n169 0.926 0.074\n170 1.000 0.000\n171 0.942 0.058\n172 0.000 1.000\n173 0.064 0.936\n174 0.976 0.024\n175 0.072 0.928\n176 0.002 0.998\n177 1.000 0.000\n178 0.868 0.132\n179 0.004 0.996\n180 1.000 0.000\n181 0.008 0.992\n182 0.006 0.994\n183 1.000 0.000\n184 0.962 0.038\n185 0.474 0.526\n186 0.938 0.062\n187 0.938 0.062\n188 0.978 0.022\n189 0.928 0.072\n190 0.942 0.058\n191 0.064 0.936\n192 0.964 0.036\n193 1.000 0.000\n194 0.002 0.998\n195 0.890 0.110\n196 0.854 0.146\n197 0.052 0.948\n198 0.000 1.000\n199 0.914 0.086\n200 0.024 0.976\n201 0.064 0.936\n202 0.060 0.940\n203 0.190 0.810\n204 0.998 0.002\n205 0.320 0.680\n206 0.962 0.038\n207 0.922 0.078\n208 0.000 1.000\n209 0.986 0.014\n210 0.002 0.998\n211 0.782 0.218\n212 0.998 0.002\n213 0.002 0.998\n214 0.994 0.006\n215 0.962 0.038\n216 0.004 0.996\n217 0.148 0.852\n218 1.000 0.000\n219 0.008 0.992\n220 0.474 0.526\n221 0.054 0.946\n222 0.016 0.984\n223 0.016 0.984\n224 0.932 0.068\n225 0.982 0.018\n226 0.014 0.986\n227 0.998 0.002\n228 0.982 0.018\n229 0.114 0.886\n230 0.048 0.952\n231 0.866 0.134\n232 0.026 0.974\n233 0.992 0.008\n234 1.000 0.000\n235 0.998 0.002\n236 0.994 0.006\n237 0.854 0.146\n238 1.000 0.000\n239 0.036 0.964\n240 0.730 0.270\n241 0.064 0.936\n242 0.302 0.698\n243 0.992 0.008\n244 0.014 0.986\n245 0.022 0.978\n246 0.002 0.998\n247 0.964 0.036\n248 0.026 0.974\n249 0.202 0.798\n250 0.148 0.852\n251 0.076 0.924\n252 0.988 0.012\n253 0.114 0.886\n254 0.230 0.770\n255 0.706 0.294\n256 0.980 0.020\n257 0.998 0.002\n258 0.976 0.024\n259 0.988 0.012\n260 0.026 0.974\n261 0.416 0.584\n262 0.852 0.148\n263 0.914 0.086\n264 0.060 0.940\n265 0.320 0.680\n266 0.040 0.960\n267 0.880 0.120\n268 0.084 0.916\n269 0.934 0.066\n270 0.988 0.012\n271 0.988 0.012\n272 0.782 0.218\n273 0.230 0.770\n274 0.854 0.146\n275 0.994 0.006\n276 0.004 0.996\n277 0.994 0.006\n278 0.004 0.996\n279 0.902 0.098\n280 0.978 0.022\n281 1.000 0.000\n282 0.014 0.986\n283 0.010 0.990\n284 0.954 0.046\n285 0.034 0.966\n286 0.190 0.810\n287 0.038 0.962\n288 0.974 0.026\n289 0.012 0.988\n290 0.034 0.966\n291 0.976 0.024\n292 0.318 0.682\n293 0.082 0.918\n294 0.032 0.968\n295 0.124 0.876\n296 0.956 0.044\n297 0.918 0.082\n298 0.006 0.994\n299 0.932 0.068\n300 0.260 0.740\n301 0.004 0.996\n302 0.054 0.946\n303 0.932 0.068\n304 0.922 0.078\n305 1.000 0.000\n306 1.000 0.000\n307 0.988 0.012\n\n\n\ny_pred &lt;- as.factor(ifelse(y_pred$yesHD &gt; 0.5, \"yesHD\", \"noHD\"))\n\ncaret::confusionMatrix(y_pred, test$heartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction noHD yesHD\n     noHD   145     0\n     yesHD    0   162\n                                     \n               Accuracy : 1          \n                 95% CI : (0.9881, 1)\n    No Information Rate : 0.5277     \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16  \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n                                     \n            Sensitivity : 1.0000     \n            Specificity : 1.0000     \n         Pos Pred Value : 1.0000     \n         Neg Pred Value : 1.0000     \n             Prevalence : 0.4723     \n         Detection Rate : 0.4723     \n   Detection Prevalence : 0.4723     \n      Balanced Accuracy : 1.0000     \n                                     \n       'Positive' Class : noHD       \n                                     \n\n\nLastly, we can extract the predictive variables with the greatest importance from your fit.\n\nvarImpOut &lt;- varImp(rf_model)\n\nvarImpOut$importance\n\n                   Overall\nage              70.849045\nsex1             23.642161\nchestPainType1    8.087142\nchestPainType2   33.997946\nchestPainType3   16.629339\nrestBP           61.528727\nchol             67.870275\nfastingBP1        7.750244\nrestElecCardio1  15.653588\nrestElecCardio2   0.000000\nmaxHR            95.828681\nexerciseAngina1  58.691710\nSTdepEKG         76.039553\nslopePeakExST1   20.352056\nslopePeakExST2   33.839912\nnMajorVessels    91.600454\nDefectType1       3.171842\nDefectType2     100.000000\nDefectType3      67.485474\n\nvarImportance &lt;- as.data.frame(as.matrix(varImpOut$importance)) %&gt;% \n  rownames_to_column(var = 'VarName') %&gt;%\n  arrange(desc(Overall))\n\nvarImportance\n\n           VarName    Overall\n1      DefectType2 100.000000\n2            maxHR  95.828681\n3    nMajorVessels  91.600454\n4         STdepEKG  76.039553\n5              age  70.849045\n6             chol  67.870275\n7      DefectType3  67.485474\n8           restBP  61.528727\n9  exerciseAngina1  58.691710\n10  chestPainType2  33.997946\n11  slopePeakExST2  33.839912\n12            sex1  23.642161\n13  slopePeakExST1  20.352056\n14  chestPainType3  16.629339\n15 restElecCardio1  15.653588\n16  chestPainType1   8.087142\n17      fastingBP1   7.750244\n18     DefectType1   3.171842\n19 restElecCardio2   0.000000\n\n\nVariable importance is based on how much each variable improves the model’s accuracy across splits. DefectType2 might be involved in important interaction or split the data in a very informative way early in trees.\nIn terms of comparing the outcome of the Random Forest model with the Elastic Net Regression, don’t expect identical top features — they reflect different model assumptions.\nUse both models as complementary tools:\nEN: for interpretable, linear relationships\nRF: for capturing complex patterns and variable interactions\nIf a feature ranks high in both models, it’s a strong signal that the feature is important.\nIf a feature ranks high in one but not the other — explore further: interaction? non-linearity? collinearity?",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 5B: Models and Model Evaluation in R"
    ]
  },
  {
    "objectID": "presentations/presentation5A.html",
    "href": "presentations/presentation5A.html",
    "title": "Presentation 5A: Intro to Regression in R",
    "section": "",
    "text": "In this section we’ll look at how to define and fit regression models in R.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 5A: Intro to Regression in R"
    ]
  },
  {
    "objectID": "presentations/presentation5A.html#load-packages",
    "href": "presentations/presentation5A.html#load-packages",
    "title": "Presentation 5A: Intro to Regression in R",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(readxl)",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 5A: Intro to Regression in R"
    ]
  },
  {
    "objectID": "presentations/presentation5A.html#linear-regression",
    "href": "presentations/presentation5A.html#linear-regression",
    "title": "Presentation 5A: Intro to Regression in R",
    "section": "Linear Regression",
    "text": "Linear Regression\nWe will perform a linear regression using daily cigarettes smoked and exercise level as predictors, \\(X\\), and lived years as the outcome, \\(y\\).",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 5A: Intro to Regression in R"
    ]
  },
  {
    "objectID": "presentations/presentation5A.html#load-data",
    "href": "presentations/presentation5A.html#load-data",
    "title": "Presentation 5A: Intro to Regression in R",
    "section": "Load data",
    "text": "Load data\nIn order to focus on the technical aspects we’ll use a simple toy dataset. It contains the number of cigarettes smoked per day and how long the person lived. It is inspired by this paper if you want to take a look.\n\ndf_smoke &lt;- read_csv('../data/smoking_cat.csv')\ndf_smoke\n\n# A tibble: 100 × 3\n   daily_cigarettes  life exercise\n              &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1                7    76        0\n 2               11    73        0\n 3               27    72        1\n 4               23    71        0\n 5               13    74        0\n 6               11    76        1\n 7               20    71        0\n 8                6    76        1\n 9               23    72        1\n10               32    70        2\n# ℹ 90 more rows\n\n\nWe have daily cigarettes smoked (number) and life (in years). Both of these are numeric variables. We also have a variable named exercise, it seems that this variable might in fact be an ordinal factor variable. Exercise is encoded as a numeric variable, so the first thing we will do is to convert it to a factor.\n\ndf_smoke %&gt;% distinct(exercise)\n\n# A tibble: 3 × 1\n  exercise\n     &lt;dbl&gt;\n1        0\n2        1\n3        2\n\ndf_smoke &lt;- df_smoke %&gt;% \n  mutate(exercise = as.factor(exercise))\n\n\nSplit Data into Training and Test Set\nNow, we will split our data into a test and a training set. There are numerous ways to do this. We here show sample_frac from dplyr:\n\n# Set seed to ensure reproducibility\nset.seed(123)  \n\n# add an ID column to keep track of observations\ndf_smoke$ID &lt;- 1:nrow(df_smoke)\n\ntrain &lt;- df_smoke %&gt;% \n  sample_frac(0.75)\n\nnrow(train)\n\n[1] 75\n\nhead(train)\n\n# A tibble: 6 × 4\n  daily_cigarettes  life exercise    ID\n             &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    &lt;int&gt;\n1               29    72 1           31\n2               16    73 0           79\n3                5    78 1           51\n4                3    77 0           14\n5                4    79 2           67\n6               23    71 1           42\n\n\nAs you can see, the ID’s in train are shuffled and it only has 75 rows since we asked for 75% of the data. Now all we have to do is identify the other 25%, i.e. the observations not in train.\n\n#from df_smoke remove what is in train by checking the ID column\ntest  &lt;- df_smoke %&gt;% \n  filter(!ID %in% train$ID) \n\n# OR\n\ntest  &lt;- anti_join(df_smoke, train, by = 'ID') \n\nnrow(test)\n\n[1] 25\n\nhead(test)\n\n# A tibble: 6 × 4\n  daily_cigarettes  life exercise    ID\n             &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    &lt;int&gt;\n1                7    76 0            1\n2               11    73 0            2\n3               27    72 1            3\n4               32    70 2           10\n5                8    75 0           11\n6               16    75 2           24\n\n\n\n\nDefining the model\nAs stated above, a linear regression model generally has the form of:\n\\[y = β_0 + β_1 * x_i\\]\nWhere we refer to \\(β_0\\) as the intercept and \\(β_1\\) as the coefficient. There will typically be one coefficient for each predictor. The goal of modelling is to estimate the values of \\(β_0\\) and all \\(β_i\\).\nWe need to tell R which of our variables is the outcome, \\(y\\), and which predictors, \\(x_i\\), we want to include in the model. This is referred to in documentation as the model’s formula. Have a look:\n\n#the formula is written like so:\nlm(y ~ x_1 + x_2 + ...)\n#see the help\n?lm\n\nIn our case, \\(y\\) is the number of years lived and we have a two predictors \\(x_1\\) (numeric), the number of cigarettes smoked per day, and \\(x_2\\) (ordinal factor), exercise level (0, 1 or 2). So that will be our model formulation:\n\n#remember to select the training data subset we defined above! \nmodel &lt;- lm(life ~ daily_cigarettes + exercise, data = train)\n\n\n\nModelling results\nBy calling lm we have already trained our model! The return of lm() is, just like the return of prcomp(), a named list.\n\nclass(model)\n\n[1] \"lm\"\n\nnames(model)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"contrasts\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n\n\nLet’s have a look at the results. The summary gives us a lot of information about the model we trained:\n\n# View model summary\nsummary(model)\n\n\nCall:\nlm(formula = life ~ daily_cigarettes + exercise, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.58295 -0.53972 -0.01596  0.53773  1.70257 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      77.582954   0.234237 331.216  &lt; 2e-16 ***\ndaily_cigarettes -0.285521   0.009401 -30.372  &lt; 2e-16 ***\nexercise1         1.095475   0.249402   4.392 3.84e-05 ***\nexercise2         2.372227   0.260427   9.109 1.48e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8578 on 71 degrees of freedom\nMultiple R-squared:  0.9338,    Adjusted R-squared:  0.931 \nF-statistic: 333.7 on 3 and 71 DF,  p-value: &lt; 2.2e-16\n\n\nThe Residuals section summarizes the distribution of the residuals, which is the difference between the actual observed \\(y\\) values and the fitted \\(y\\) values.\nThe Coefficients table shows the estimated values for each coefficient including the intercept, along with their standard errors, t-values, and p-values. These help to determine the significance of each predictor.\nIn the bottom section we have some information about how well the model fits the training data.\nThe Residual Standard Error (RSE) is the standard deviation of the residuals (prediction errors). It tells you, on average, how far the observed values deviate from the regression line.\nThe R-squared value indicates the proportion of variance explained by the model, with the Adjusted R-squared accounting for the number of predictors.\nFinally, the F-statistic and its p-value tests whether the model as a whole explains a significant portion of the variance in the response variable (the outcome, \\(y\\)).\nLets plot the results:\n\npar(mfrow=c(2,2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\nModel interpretation\nPlot to understand the model:\nWhat do these results mean? Our model formulation is:\n\\[life = β_0 + β_1 * cigarettes + β_2 * exercise\\]\nAnd we estimated these values:\n\nmodel$coefficients\n\n     (Intercept) daily_cigarettes        exercise1        exercise2 \n      77.5829543       -0.2855213        1.0954747        2.3722266 \n\n\nTherefore:\n\nThe intercept \\(β_0\\) is the number of years we estimated a person in this dataset will live if they smoke 0 cigarettes and do not exercise. It is 77.6 years.\nThe coefficient of cigarettes per day is -0.29. This means for every 1 unit increase in cigarettes (one additional cigarette per day) the life expectancy decreases by 0.29 years. Similarly for the exercise variable, if you exercise your life expectancy will go up 1.1-2.4 years compared to no exercise, independently of how many cigarettes you smoke.\n\n\n\nModel performance\nWe now use our held out test data to evaluate the model performance. For that we will predict life expectancy for the 25 observations in test and compare with the observed values.\n\n#use the fitted model to make predictions for the test data\ny_pred &lt;- predict(model, test)\n\nLet’s see how the predicted values fit with the observed values.\n\npred &lt;- tibble(pred = y_pred, \n               real = test$life)\n\nggplot(pred, \n       aes(x=real, y=pred)) +\n  geom_point()\n\n\n\n\n\n\n\n\nNot too bad! We usually calculate the root mean square error (rmse) between predictions and the true observed values to numerically evaluate regression performance:\n\nRMSE(pred$real,pred$pred)\n\n[1] 0.8778289\n\n\nOur predictions are on average 0.88 years ‘off’.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 5A: Intro to Regression in R"
    ]
  },
  {
    "objectID": "presentations/presentation5A.html#classification",
    "href": "presentations/presentation5A.html#classification",
    "title": "Presentation 5A: Intro to Regression in R",
    "section": "Classification",
    "text": "Classification\nClassification is the method we use when the outcome variable we are interested in is not continuous, but instead consists of two or more classes.\nIn order to have a categorical outcome, we’ll add a column to our toy data that describes whether the person died before age 75 or not.\n\ndf_smoke &lt;- df_smoke %&gt;%\n  mutate(early_death = factor(ifelse(life &lt; 75, 1, 0))) # Encoding: True/yes = 1, False/no = 0\n\ndf_smoke %&gt;%\n  count(early_death)\n\n# A tibble: 2 × 2\n  early_death     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0              49\n2 1              51\n\n\n\nTraining and Test set with class data\nLet’s remake our training and test data. This time we have classes that we would like to be in the same ratios in training and test set. We must check this is the case!\n\n# Set seed to ensure reproducibility\nset.seed(123)  \n\n#add an ID column to keep track of observations\ndf_smoke$ID &lt;- 1:nrow(df_smoke)\n\ntrain &lt;- df_smoke %&gt;% \n  sample_frac(0.75)\n\ntable(train$early_death)\n\n\n 0  1 \n36 39 \n\ntest  &lt;- anti_join(df_smoke, train, by = 'ID') \n\ntable(test$early_death)\n\n\n 0  1 \n13 12 \n\n\nLuckily for us the division of the outcome variable classes between the train and test set is almost perfect. However, there may be cases where randomly splitting will not give you a balanced distribution. This is likely to happen if one class is much larger than the other(s). In these cases you should split your data in a non-random way, specifically ensuring a balanced train and test set.\nNow let’s perform logistic regression to see whether there is an influence of the number of cigarettes and amount of exercise on the odds of the person dying before 75.\nLogistic regression belongs to the family of generalized linear models. They all look like this:\n\\[ y \\sim \\beta * X \\]\nwith:\n\n\\(y\\) the outcome\n\\(\\beta\\) the coefficient matrix\n\\(X\\) the matrix of predictors\n\\(\\sim\\) the link function\n\nIn a logistic regression model the link function is the logit. In a linear model the link function is the identity function (so ~ becomes =).\n\n\nLogistic regression: Math\nIn order to understand what that means we’ll need a tiny bit of math.\nOur \\(y\\) is either 0 or 1. However we cannot model that, so instead we will model the probability of the outcome being 1: \\(P(earlydeath == 1)\\). Except probabilities are bounded between 0 and 1 which is mathematically difficult to impose (it means all \\(y's\\) have to be between these two values and how are we gonna enforce that?) So instead, we will model the log-odds of early death:\n\\[ y = \\log(\\frac{P(earlydeath == 1)}{1-P(earlydeath == 1)})\\]\nIt may not look like it but we promise you this \\(y\\) is a well behaved number because it can be anywhere between - infinity and + infinity. So therefore our actual model is:\n\\[ \\log(\\frac{P(earlydeath == 1)}{1-P(earlydeath == 1)} = \\beta * X\\]\nAnd if we want to know what this means for the probability of dying early, we just take invert the link function:\n\\[ P(earlydeath == 1) = \\frac{1}{1+ e^{(-y)}} \\]\nWhich serves as the link between what we’re actually interested in (the probability of a person dying early) and what we’re modelling using logit. End of math.\n\n\nModel formulation in R\nSo in order to fit a logistic regression we will use the function for generalized linear models, glm. We will specify that we want logistic regression (using the logit as the link) by setting family = binomial:\n\nmodel_log &lt;- glm(early_death ~ daily_cigarettes + exercise, data = train, family = \"binomial\")\nsummary(model_log)\n\n\nCall:\nglm(formula = early_death ~ daily_cigarettes + exercise, family = \"binomial\", \n    data = train)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)       -9.7171     3.4761  -2.795  0.00518 **\ndaily_cigarettes   0.8361     0.2876   2.907  0.00365 **\nexercise1         -2.8037     1.9692  -1.424  0.15453   \nexercise2         -5.3671     2.8042  -1.914  0.05563 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 103.85  on 74  degrees of freedom\nResidual deviance:  13.59  on 71  degrees of freedom\nAIC: 21.59\n\nNumber of Fisher Scoring iterations: 9\n\n\n\n\nModel interpretation\nWe see from looking at the summary that the coefficient of exercise level 1 and level 2 is not significant. This means that we are not confident that doing any level of exercise has a significant impact on the probability of dying before 75 compared to doing no exercise. This does not mean that there can be no influence, merely that we do not have enough data to detect it if it is there.\nAre you surprised? Exercise level was significant when we modelled the number of years lived with linear regressions.\nWith the number of daily cigarettes predictor we have a high degree of certainty that it influences the probability of dying before 75 (in this dataset!), but what does a coefficient of 0.84 mean?\nWe know that:\n\\[ P(earlydeath == 1) = \\frac{1}{1+ e^{(-y)}} \\]\nand (leaving out the exercise level since it’s not significant):\n\\[ y = \\beta_0 + \\beta_1 * cigs \\]\nSo how does \\(y\\) change as \\(0.84 * cigs\\) becomes larger? Let’s agree that \\(y\\) becomes larger. What does that mean for the probability of dying before 75? Is \\(e^{(-y)}\\) a large number if \\(y\\) is large? Luckily we have a calculator handy:\n\n#exp(b) is e^b in R\n\nexp(-1)\n\n[1] 0.3678794\n\nexp(-10)\n\n[1] 4.539993e-05\n\nexp(-100)\n\n[1] 3.720076e-44\n\n\nWe see that \\(e^{(-y)}\\) becomes increasingly smaller with larger \\(y\\) which means that:\n\\[ P(earlydeath == 1) = \\frac{1}{1+ small} \\sim \\frac{1}{1} \\]\nSo the larger \\(y\\) the smaller \\(e^{(-y)}\\) and the closer we get to \\(P(earlydeath == 1)\\) being 1. That was a lot of math for: If the coefficient is positive you increase the likelihood of getting the outcome, i.e. dying before 75.\n\n\nModel comparison\nNow we know how to fit linear models and interpret the results. But often there are several predictors we could include or not include, so how do we know that one model is better than another?\nThere are several ways to compare models. One is the likelihood ratio test which tests whether adding predictors significantly improves model fit by comparing the log-likelihoods of the two models. Another much used comparison is to look at the AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion). Lower AIC/BIC values generally indicate a better trade-off between model fit and complexity.\nFor example we made this logistic regression model above:\n\nsummary(model_log)\n\n\nCall:\nglm(formula = early_death ~ daily_cigarettes + exercise, family = \"binomial\", \n    data = train)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)       -9.7171     3.4761  -2.795  0.00518 **\ndaily_cigarettes   0.8361     0.2876   2.907  0.00365 **\nexercise1         -2.8037     1.9692  -1.424  0.15453   \nexercise2         -5.3671     2.8042  -1.914  0.05563 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 103.85  on 74  degrees of freedom\nResidual deviance:  13.59  on 71  degrees of freedom\nAIC: 21.59\n\nNumber of Fisher Scoring iterations: 9\n\n\nBut exercise does not have a significant p-value. Perhaps we would have a better model if we only use dialy_cigarettes?\nLet’s compare them:\n\nmodel_reduced &lt;- glm(early_death ~ daily_cigarettes, data = train, family = \"binomial\")\nsummary(model_reduced)\n\n\nCall:\nglm(formula = early_death ~ daily_cigarettes, family = \"binomial\", \n    data = train)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)       -9.3382     2.8813  -3.241  0.00119 **\ndaily_cigarettes   0.6228     0.1958   3.181  0.00147 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 103.852  on 74  degrees of freedom\nResidual deviance:  20.408  on 73  degrees of freedom\nAIC: 24.408\n\nNumber of Fisher Scoring iterations: 8\n\n\nWe can use an anova with the Chi-square (kai-square) test to compare the log-likelihood of the two models. It is most common to compare the less complex model to the more complex model:\n\nanova(model_reduced, model_log, test = 'Chisq')\n\nAnalysis of Deviance Table\n\nModel 1: early_death ~ daily_cigarettes\nModel 2: early_death ~ daily_cigarettes + exercise\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  \n1        73     20.408                       \n2        71     13.590  2   6.8187  0.03306 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value of the Chi-square test tells us if there is evidence that the difference in log-likelihoods is significant. If it is not significant, we do not have evidence that the more complex model is a better fit than the less complex model and we would therefore choose the less complex model with fewer predictors since that gives us more statistical power and less overfitting.\nIn this case the p-value is significant which means that the more complex model, that includes exercises, is a better fit. Do you think this makes sense with the exercises levels not being significant predictors when we looked at summary(model_log).\n\n\nModel Evaluation\nLastly, let’s evaluate our model using the test set, just like we did for the linear regression. First, we will predict the outcome for the test set:\n\ny_pred &lt;- predict(model_log, newdata = test, type = 'response')\n\ny_pred \n\n           1            2            3            4            5            6 \n2.053739e-02 3.727265e-01 9.999569e-01 9.999914e-01 4.614593e-02 1.535095e-01 \n           7            8            9           10           11           12 \n1.704256e-03 9.999998e-01 9.665682e-01 9.999803e-01 2.922587e-03 3.727265e-01 \n          13           14           15           16           17           18 \n6.358678e-03 2.257685e-04 6.901515e-01 7.669379e-02 6.717464e-03 9.999006e-01 \n          19           20           21           22           23           24 \n4.911941e-01 9.999569e-01 9.999679e-01 7.669379e-02 9.987809e-01 1.536393e-02 \n          25 \n6.023842e-05 \n\n\nAs you see, the predictions we get out are not \\(yes\\) or \\(no\\), they are instead a probability (as discussed above), so, we will convert them to class labels.\n\ny_pred &lt;- ifelse(as.numeric(y_pred) &gt;= 0.5, 1, 0) %&gt;% \n  as.factor()\n\ny_pred\n\n [1] 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0\nLevels: 0 1\n\n\nNow we will compare the predicted class with the observed class for the test set. You can do this in different ways, but here we will use the accuracy.\n\\[ Accuracy = \\frac{(True Positives + True Negatives)}{(True Positives + True Negatives + False Positives + False Negatives)}\\]\nYou can calculate the accuracy yourself, or you can use a function like confusionMatrix() from the package caret which also provides you with individual metrics like sensitivity (true positive rate, recall) and specificity (true negative rate).\n\ncaret::confusionMatrix(y_pred, test$early_death)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 13  2\n         1  0 10\n                                          \n               Accuracy : 0.92            \n                 95% CI : (0.7397, 0.9902)\n    No Information Rate : 0.52            \n    P-Value [Acc &gt; NIR] : 2.222e-05       \n                                          \n                  Kappa : 0.8387          \n                                          \n Mcnemar's Test P-Value : 0.4795          \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.8333          \n         Pos Pred Value : 0.8667          \n         Neg Pred Value : 1.0000          \n             Prevalence : 0.5200          \n         Detection Rate : 0.5200          \n   Detection Prevalence : 0.6000          \n      Balanced Accuracy : 0.9167          \n                                          \n       'Positive' Class : 0",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 5A: Intro to Regression in R"
    ]
  },
  {
    "objectID": "presentations/presentation5A.html#kmeans-clustering",
    "href": "presentations/presentation5A.html#kmeans-clustering",
    "title": "Presentation 5A: Intro to Regression in R",
    "section": "Kmeans Clustering",
    "text": "Kmeans Clustering\nClustering is a type of unsupervised learning technique used to group similar data points together based on their features. The goal is to find inherent patterns or structures within the data, e.g. to see whether the data points fall into distinct groups with distinct features or not.\n\nWine dataset\nFor this we will use the wine data set as an example:\n\nlibrary(ContaminatedMixt)\nlibrary(factoextra)\n\nLet’s load in the dataset\n\ndata('wine') #load dataset\ndf_wine &lt;- wine %&gt;% as_tibble() #convert to tibble\ndf_wine\n\n# A tibble: 178 × 14\n   Type   Alcohol Malic   Ash Alcalinity Magnesium Phenols Flavanoids\n   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;int&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 Barolo    14.2  1.71  2.43       15.6       127    2.8        3.06\n 2 Barolo    13.2  1.78  2.14       11.2       100    2.65       2.76\n 3 Barolo    13.2  2.36  2.67       18.6       101    2.8        3.24\n 4 Barolo    14.4  1.95  2.5        16.8       113    3.85       3.49\n 5 Barolo    13.2  2.59  2.87       21         118    2.8        2.69\n 6 Barolo    14.2  1.76  2.45       15.2       112    3.27       3.39\n 7 Barolo    14.4  1.87  2.45       14.6        96    2.5        2.52\n 8 Barolo    14.1  2.15  2.61       17.6       121    2.6        2.51\n 9 Barolo    14.8  1.64  2.17       14          97    2.8        2.98\n10 Barolo    13.9  1.35  2.27       16          98    2.98       3.15\n# ℹ 168 more rows\n# ℹ 6 more variables: Nonflavanoid &lt;dbl&gt;, Proanthocyanins &lt;dbl&gt;, Color &lt;dbl&gt;,\n#   Hue &lt;dbl&gt;, Dilution &lt;dbl&gt;, Proline &lt;int&gt;\n\n\nThis dataset contains 178 wine, each corresponding to one of three different cultivars of wine. It has 13 numerical columns that record different features of the wine.\nWe will try out a popular method, k-means clustering. It works by initializing K centroids and assigning each data point to the nearest centroid. The algorithm then recalculates the centroids as the mean of the points in each cluster, repeating the process until the clusters stabilize. You can see an illustration of the process below. Its weakness is that we need to define the number of centroids, i.e. clusters, beforehand.\n\n\n\n\n\n\n\nRunning k-means\nFor k-means it is very important that the data is numeric and scaled so we will do that before running the algorithm.\n\n# Set seed to ensure reproducibility\nset.seed(123)  \n\n# Pull numeric variables and scale these\nkmeans_df &lt;- df_wine %&gt;%\n  dplyr::select(where(is.numeric)) %&gt;%\n  mutate(across(everything(), scale))\n\nkmeans_df\n\n# A tibble: 178 × 13\n   Alcohol[,1] Malic[,1] Ash[,1] Alcalinity[,1] Magnesium[,1] Phenols[,1]\n         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n 1       1.51    -0.561    0.231         -1.17         1.91         0.807\n 2       0.246   -0.498   -0.826         -2.48         0.0181       0.567\n 3       0.196    0.0212   1.11          -0.268        0.0881       0.807\n 4       1.69    -0.346    0.487         -0.807        0.928        2.48 \n 5       0.295    0.227    1.84           0.451        1.28         0.807\n 6       1.48    -0.516    0.304         -1.29         0.858        1.56 \n 7       1.71    -0.417    0.304         -1.47        -0.262        0.327\n 8       1.30    -0.167    0.888         -0.567        1.49         0.487\n 9       2.25    -0.623   -0.716         -1.65        -0.192        0.807\n10       1.06    -0.883   -0.352         -1.05        -0.122        1.09 \n# ℹ 168 more rows\n# ℹ 7 more variables: Flavanoids &lt;dbl[,1]&gt;, Nonflavanoid &lt;dbl[,1]&gt;,\n#   Proanthocyanins &lt;dbl[,1]&gt;, Color &lt;dbl[,1]&gt;, Hue &lt;dbl[,1]&gt;,\n#   Dilution &lt;dbl[,1]&gt;, Proline &lt;dbl[,1]&gt;\n\n\nKmeans clustering in R is easy, we simply run the kmeans() function:\n\nset.seed(123)  \n\nkmeans_res &lt;- kmeans_df %&gt;%\n  kmeans(centers = 4, nstart = 25)\n\nkmeans_res\n\nK-means clustering with 4 clusters of sizes 28, 56, 49, 45\n\nCluster means:\n     Alcohol       Malic        Ash Alcalinity   Magnesium    Phenols\n1 -0.7869073  0.04195151  0.2157781  0.3683284  0.43818899  0.6543578\n2  0.9580555 -0.37748461  0.1969019 -0.8214121  0.39943022  0.9000233\n3  0.1860184  0.90242582  0.2485092  0.5820616 -0.05049296 -0.9857762\n4 -0.9051690 -0.53898599 -0.6498944  0.1592193 -0.71473842 -0.4537841\n  Flavanoids Nonflavanoid Proanthocyanins      Color        Hue    Dilution\n1  0.5746004   -0.5429201       0.8888549 -0.7346332  0.2830335  0.60628629\n2  0.9848901   -0.6204018       0.5575193  0.2423047  0.4799084  0.76926636\n3 -1.2327174    0.7148253      -0.7474990  0.9857177 -1.1879477 -1.29787850\n4 -0.2408779    0.3315072      -0.4329238 -0.9177666  0.5202140  0.07869143\n     Proline\n1 -0.5169332\n2  1.2184972\n3 -0.3789756\n4 -0.7820425\n\nClustering vector:\n  [1] 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2\n [38] 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 1 4 1 2 4 4 1 4 1 4 1\n [75] 1 4 4 4 1 1 4 4 4 3 1 4 4 4 4 4 4 4 4 1 1 1 1 4 1 1 4 4 1 4 4 4 4 4 4 1 1\n[112] 4 4 4 4 4 4 4 4 4 1 1 1 1 1 4 1 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n\nWithin cluster sum of squares by cluster:\n[1] 307.0966 268.5747 302.9915 289.9515\n (between_SS / total_SS =  49.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nWe can call kmeans_res$centers to inspect the values the centroids. For example the center of cluster 1 is placed at the coordinates -0.79 for Alcohol, 0.04 for Malic Acid, 0.22 for Ash and so on. Since our data has 13 dimensions, i.e. features, the cluster centers also do.\nThis is not super practical if we would like to visually inspect the clustering since we cannot plot in 13 dimensions. How could we solve this?\n\n\nVisualizing k-means results\nWe would like to see where our wine bottles and their clusters lie in a low-dimensional space. This can easily be done using the fviz_cluster()\n\nfviz_cluster(object = kmeans_res, \n             data = kmeans_df,\n             palette = c(\"#2E9FDF\", \"#00AFBB\", \"#E7B800\", \"orchid3\"), \n             geom = \"point\",\n             ellipse.type = \"norm\", \n             ggtheme = theme_bw())\n\n\n\n\n\n\n\n\n\n\nOptimal number of clusters\nThere are several ways to investigate the ideal number of clusters and fviz_nbclust from the factoextra package provides three of them:\nThe so-called elbow method observes how the sum of squared errors (sse) changes as we vary the number of clusters. This is also sometimes referred to as “within sum of square” (wss).\n\nkmeans_df %&gt;%\n  fviz_nbclust(kmeans, method = \"wss\")\n\n\n\n\n\n\n\n\nThe gap statistic compares the within-cluster variation (how compact the clusters are) for different values of K to the expected variation under a null reference distribution (i.e., random clustering).\n\nkmeans_df %&gt;%\n  fviz_nbclust(kmeans, method = \"gap_stat\")\n\n\n\n\n\n\n\n\nBoth of these tell us that there should be three clusters and we also know that there are three cultivars of wine in the dataset. Let’s redo k-means with three centroids.\n\n# Set seed to ensure reproducibility\nset.seed(123)  \n\n#run kmeans\nkmeans_res &lt;- kmeans_df %&gt;%\n  kmeans(centers = 3, nstart = 25)\n\n\n#add updated cluster info to the dataframe\nfviz_cluster(kmeans_res, data = kmeans_df,\n             palette = c(\"#2E9FDF\", \"#00AFBB\", \"#E7B800\", \"orchid3\"), \n             geom = \"point\",\n             ellipse.type = \"norm\", \n             ggtheme = theme_bw())",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 5A: Intro to Regression in R"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Data Science",
    "section": "",
    "text": "This 3 day course is a continuation of our 2 day course FromExceltoR and the material of that course is a prerequisite to this course. If you have not used R since you took the course, please go through the course material again (link to course). R for Data Science is an advanced course in R-programming for researchers at the Faculty of Health and Medical Sciences (SUND), University of Copenhagen. The course is build on code-along presentations and exercises in Quarto documents.\n\nThe course goes through the following topics:\n\nScript formats\nAdvanced tidyverse using real world data (build on material from our introduction to R course, FromExceltoR)\nScripting in R using functions, for-loops, if-else statements.\nModelling in R.\n\nThe material in this repository is for teaching purposes only and not to be distributed commercially.\nFinally… Dear course participants, it would greatly help us if you could complete our feedback form."
  },
  {
    "objectID": "index.html#welcome-to-the-main-page-of-r-for-data-science",
    "href": "index.html#welcome-to-the-main-page-of-r-for-data-science",
    "title": "R for Data Science",
    "section": "",
    "text": "This 3 day course is a continuation of our 2 day course FromExceltoR and the material of that course is a prerequisite to this course. If you have not used R since you took the course, please go through the course material again (link to course). R for Data Science is an advanced course in R-programming for researchers at the Faculty of Health and Medical Sciences (SUND), University of Copenhagen. The course is build on code-along presentations and exercises in Quarto documents.\n\nThe course goes through the following topics:\n\nScript formats\nAdvanced tidyverse using real world data (build on material from our introduction to R course, FromExceltoR)\nScripting in R using functions, for-loops, if-else statements.\nModelling in R.\n\nThe material in this repository is for teaching purposes only and not to be distributed commercially.\nFinally… Dear course participants, it would greatly help us if you could complete our feedback form."
  },
  {
    "objectID": "index.html#program",
    "href": "index.html#program",
    "title": "R for Data Science",
    "section": "Program",
    "text": "Program"
  },
  {
    "objectID": "slides/Quarto_example.html",
    "href": "slides/Quarto_example.html",
    "title": "R for Data Science - How to Quarto",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)"
  },
  {
    "objectID": "slides/Quarto_example.html#load-packages",
    "href": "slides/Quarto_example.html#load-packages",
    "title": "R for Data Science - How to Quarto",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)"
  },
  {
    "objectID": "slides/Quarto_example.html#load-data",
    "href": "slides/Quarto_example.html#load-data",
    "title": "R for Data Science - How to Quarto",
    "section": "Load Data",
    "text": "Load Data\n\ndiabetes &lt;- read_excel('../data/diabetes_clinical_toy_messy.xlsx')"
  },
  {
    "objectID": "slides/Quarto_example.html#inspect-data",
    "href": "slides/Quarto_example.html#inspect-data",
    "title": "R for Data Science - How to Quarto",
    "section": "Inspect Data",
    "text": "Inspect Data\nCheck dimensions of data\n\ndim(diabetes)\n\n[1] 532   9\n\n\nCheck structure of data\n\nstr(diabetes)\n\ntibble [532 × 9] (S3: tbl_df/tbl/data.frame)\n $ ID              : num [1:532] 9046 51676 31112 60182 1665 ...\n $ Sex             : chr [1:532] \"Male\" \"Male\" \"Male\" \"Male\" ...\n $ Age             : num [1:532] 34 25 30 50 27 35 31 52 54 41 ...\n $ BloodPressure   : num [1:532] 84 74 0 80 60 84 78 86 78 90 ...\n $ BMI             : num [1:532] 24.7 22.5 32.3 34.5 26.3 35 43.3 33.3 35.2 39.8 ...\n $ PhysicalActivity: num [1:532] 93 102 75 98 82 58 59 58 74 67 ...\n $ Smoker          : chr [1:532] \"Unknown\" \"Unknown\" \"Former\" \"Unknown\" ...\n $ Diabetes        : num [1:532] 0 0 1 1 0 1 1 1 1 1 ...\n $ Serum_ca2       : num [1:532] 9.8 9.5 9.3 9.4 9 9.3 9.6 9.1 9.3 9.1 ...\n\n\nCheck for NA’s in each column\n\ncolSums(is.na(diabetes))\n\n              ID              Sex              Age    BloodPressure \n               0                0                3                0 \n             BMI PhysicalActivity           Smoker         Diabetes \n               3                0                0                0 \n       Serum_ca2 \n               0"
  },
  {
    "objectID": "slides/Quarto_example.html#exploratory-data-analysis",
    "href": "slides/Quarto_example.html#exploratory-data-analysis",
    "title": "R for Data Science - How to Quarto",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nPlot distribution of BMI\n\ndiabetes %&gt;% \n  ggplot(aes(x = BMI)) + \n  geom_histogram(bins = 10)\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "presentations/named_lists.html",
    "href": "presentations/named_lists.html",
    "title": "R Objects: Named lists",
    "section": "",
    "text": "In our last exercise ‘3B: PCA’ we encountered the object pca_res &lt;- prcomp(df, scale. = TRUE) (or what you have named it). Lets have a deeper look at its structure because this is a type of object you will encounter many times while using R."
  },
  {
    "objectID": "presentations/named_lists.html#creating-a-pca-object",
    "href": "presentations/named_lists.html#creating-a-pca-object",
    "title": "R Objects: Named lists",
    "section": "Creating a PCA object",
    "text": "Creating a PCA object\n\n#build-in dataset: Iris\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n#create PCA object\npca_res &lt;- prcomp(iris[1:4], scale. = TRUE)\n\npca_res is a named list:\n\ntypeof(pca_res)\n\n[1] \"list\"\n\n\nThis is means it has several elements inside it and they are named. You can investigate them by clicking on pca_res in the Environment which will show their name, type and some example values. Lists are great because their elements can have different data types while vectors cannot.\nWe can also list the elements of a named list (or any other named object such as dataframes/tibbles):\n\nnames(pca_res)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n\nElements of a named list are always accessed with the $ symbol:\n\n#the standard deviations of the principal components\npca_res$sdev\n\n[1] 1.7083611 0.9560494 0.3830886 0.1439265\n\n\nElements of a named list (or any list) can themselves be multi-dimensional, such as the coordinates of each data point in the PC space, x:\n\nhead(pca_res$x)\n\n           PC1        PC2         PC3          PC4\n[1,] -2.257141 -0.4784238  0.12727962  0.024087508\n[2,] -2.074013  0.6718827  0.23382552  0.102662845\n[3,] -2.356335  0.3407664 -0.04405390  0.028282305\n[4,] -2.291707  0.5953999 -0.09098530 -0.065735340\n[5,] -2.381863 -0.6446757 -0.01568565 -0.035802870\n[6,] -2.068701 -1.4842053 -0.02687825  0.006586116\n\n\nMany named list objects have a summary:\n\nsummary(pca_res)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\n\nSome named list objects also have a class attribute. Named lists with a class attribute are also referred to as S3 objects. Or the other way around: S3 objects are named lists that have a class.\n\nclass(pca_res)\n\n[1] \"prcomp\"\n\n\nR uses the class to figure out how to process the object, for example inside summary(). So class is about what the object is whereas Type (as in typeof()) is about the structure of an object and how you can interact with it.\nBonus info: A ggplot is also an S3 object. Bet you didn’t know that!"
  },
  {
    "objectID": "presentations/presentation4B.html",
    "href": "presentations/presentation4B.html",
    "title": "Presentation 4B: Scripting in R - Functions",
    "section": "",
    "text": "In this section we will learn more about flow control and how to make more complex code constructs in R.\nlibrary(tidyverse)",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 4B: Functions"
    ]
  },
  {
    "objectID": "presentations/presentation4B.html#user-defined-functions",
    "href": "presentations/presentation4B.html#user-defined-functions",
    "title": "Presentation 4B: Scripting in R - Functions",
    "section": "User defined Functions",
    "text": "User defined Functions\nUser defined functions help us to re-use and structure our code.\nWe will use BMI calculation as an example for this part.\n\n#measurements of one individual\n\nweight_kg &lt;- 70\nheight_m &lt;- 1.80\n\nWe calculate BMI with this formula:\n\nbmi &lt;- weight_kg/height_m^2\nbmi\n\n[1] 21.60494\n\n\nIf we plan to calculate BMI for multiple individuals it is convenient to write the calculation into a function.\n\nFunction name: calculate_bmi.\nFunction parameters: weight_kg and height_m.\nThe return value: bmi.\n\nThe return statement specifies the value that the function will return when called.\n\ncalculate_bmi &lt;- function(weight_kg, height_m){\n  \n  bmi &lt;- weight_kg/height_m^2\n  \n  return(bmi)\n  \n}\n\nWe can now call the function on our previously defined variables.\n\ncalculate_bmi(weight_kg = weight_kg, \n              height_m = height_m)\n\n[1] 21.60494\n\n\nWe can also pass numbers directly to the function.\n\ncalculate_bmi(weight_kg = 100, \n              height_m = 1.90)\n\n[1] 27.70083\n\n\nArgument Order in Function Calls\nIf we specify the parameter names, the order can be changed.\n\ncalculate_bmi(height_m = 1.90, \n              weight_kg = 100)\n\n[1] 27.70083\n\n\nIf we do not specify the parameter names, the arguments will be matched according to the position - so be careful with this.\n\ncalculate_bmi(1.90, \n              100)\n\n[1] 0.00019",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 4B: Functions"
    ]
  },
  {
    "objectID": "presentations/presentation4B.html#combining-function-call-with-if-statement",
    "href": "presentations/presentation4B.html#combining-function-call-with-if-statement",
    "title": "Presentation 4B: Scripting in R - Functions",
    "section": "Combining function call with if-statement",
    "text": "Combining function call with if-statement\nWe can combine user-defined functions with if-else statements, so that the if-else will decide whether we execute the function or not.\n\n#measurements of one individual\nage &lt;- 45\nweight_kg &lt;- 85\nheight_m &lt;- 1.75\n\nIf we want to calculate BMI only for individuals over the age of 18:\n\nif (age &gt;= 18){\n  calculate_bmi(weight_kg, height_m)\n}\n\n[1] 27.7551",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 4B: Functions"
    ]
  },
  {
    "objectID": "presentations/presentation4B.html#combining-function-call-with-for-loops",
    "href": "presentations/presentation4B.html#combining-function-call-with-for-loops",
    "title": "Presentation 4B: Scripting in R - Functions",
    "section": "Combining function call with for-loops",
    "text": "Combining function call with for-loops\nOr we can choose to execute our function once for every element of an iterable, e.g. every row in a dataframe:\n\ndf &lt;- data.frame(row.names = 1:5,\n                 age = c(45, 16, 31, 56, 19),\n                 weight_kg = c(85, 65, 100, 45, 76),\n                 height_m = c(1.75, 1.45, 1.95, 1.51, 1.89))\n\ndf\n\n  age weight_kg height_m\n1  45        85     1.75\n2  16        65     1.45\n3  31       100     1.95\n4  56        45     1.51\n5  19        76     1.89\n\n\nPrint ID, weight, and height of all individuals.\n\nfor (id in rownames(df)){\n\n  weight &lt;- df[id, 'weight_kg']\n\n  height &lt;- df[id, 'height_m']\n\n  print(c(id, weight, height))\n\n}\n\n[1] \"1\"    \"85\"   \"1.75\"\n[1] \"2\"    \"65\"   \"1.45\"\n[1] \"3\"    \"100\"  \"1.95\"\n[1] \"4\"    \"45\"   \"1.51\"\n[1] \"5\"    \"76\"   \"1.89\"\n\n\nCall function to calculate BMI for all individuals.\n\nfor (id in rownames(df)) {\n  \n  weight &lt;- df[id, 'weight_kg']\n  \n  height &lt;- df[id, 'height_m']\n  \n  bmi &lt;- calculate_bmi(weight, height)\n  \n  print(c(id, bmi))\n  \n}\n\n[1] \"1\"                \"27.7551020408163\"\n[1] \"2\"                \"30.9155766944114\"\n[1] \"3\"                \"26.2984878369494\"\n[1] \"4\"                \"19.7359764922591\"\n[1] \"5\"                \"21.2760001119789\"\n\n\n\nCombination of function call, if-statement and for-loops.\nPrint BMI for individuals that are 18 years old or older.\n\nfor (id in rownames(df)) {\n  \n  if (df[id, 'age'] &gt;= 18) {\n    \n    weight &lt;- df[id, 'weight_kg']\n  \n    height &lt;- df[id, 'height_m']\n    \n    bmi &lt;- calculate_bmi(weight, height)\n    \n    print(c(id, bmi))\n\n  } else {\n    \n    print(paste(id, 'is under 18.'))\n    \n  }\n  \n}\n\n[1] \"1\"                \"27.7551020408163\"\n[1] \"2 is under 18.\"\n[1] \"3\"                \"26.2984878369494\"\n[1] \"4\"                \"19.7359764922591\"\n[1] \"5\"                \"21.2760001119789\"\n\n\nAdding BMI to the data frame.\n\nfor (id in rownames(df)){\n  \n  if (df[id, 'age'] &gt;= 18) {\n    \n    weight &lt;- df[id, 'weight_kg']\n  \n    height &lt;- df[id, 'height_m']\n    \n    bmi &lt;- calculate_bmi(weight, height)\n\n  } else {\n    \n    bmi &lt;- NA\n    \n  }\n  \n  df[id, 'bmi'] &lt;- bmi\n  \n}\n\nHave a look at the data frame.\n\ndf\n\n  age weight_kg height_m      bmi\n1  45        85     1.75 27.75510\n2  16        65     1.45       NA\n3  31       100     1.95 26.29849\n4  56        45     1.51 19.73598\n5  19        76     1.89 21.27600",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 4B: Functions"
    ]
  },
  {
    "objectID": "presentations/presentation4B.html#plotting-in-functions",
    "href": "presentations/presentation4B.html#plotting-in-functions",
    "title": "Presentation 4B: Scripting in R - Functions",
    "section": "Plotting in functions",
    "text": "Plotting in functions\nDefine function that creates boxplots\n\nmy_boxplot &lt;- function(dataframe, variable = ''){\n  \n  p &lt;- ggplot(data = dataframe, \n         aes(y = !!sym(variable))) + # Use variable as column reference\n  geom_boxplot(color = 'blue') + \n  theme_bw() + \n  labs(title = paste('Boxplot of', variable)) # Use variable as string\n  \n  return(p)\n  \n}\n\nLook at column names of df\n\ncolnames(df)\n\n[1] \"age\"       \"weight_kg\" \"height_m\"  \"bmi\"      \n\ncolnames(df)[1]\n\n[1] \"age\"\n\n\nRun function on age.\n\nmy_boxplot(dataframe = df, 'age')\n\n\n\n\n\n\n\n\nOr equivalently\n\nmy_boxplot(dataframe = df, colnames(df)[1])",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 4B: Functions"
    ]
  },
  {
    "objectID": "presentations/presentation4B.html#error-handling-in-user-defined-functions",
    "href": "presentations/presentation4B.html#error-handling-in-user-defined-functions",
    "title": "Presentation 4B: Scripting in R - Functions",
    "section": "Error handling in user-defined functions",
    "text": "Error handling in user-defined functions\nCurrently our BMI function accepts all kinds of inputs. However, what happens if we give a negative weight?\n\ncalculate_bmi(weight_kg = -50, height_m = 1.80)\n\n[1] -15.4321\n\n\nWe should require that both weight and height need to be positive values:\n\ncalculate_bmi_2 &lt;- function(weight_kg, height_m) {\n  \n  # Check if weight and height are numeric\n  if (!is.numeric(weight_kg) | !is.numeric(height_m)) {\n    stop(\"Both weight_kg and height_m must be numeric values.\")\n  }\n  \n  # Check if weight and height are positive\n  if (weight_kg &lt;= 0) {\n    stop(\"Weight must be a positive value.\")\n  }\n  if (height_m &lt;= 0) {\n    stop(\"Height must be a positive value.\")\n  }\n  \n  # Calculate BMI\n  bmi &lt;- weight_kg / height_m^2\n  \n  # Check if BMI is within a reasonable range\n  if (bmi &lt; 10 | bmi &gt; 60) {\n    warning(\"The calculated BMI is outside the normal range. Please check your input values.\")\n  }\n  \n  return(bmi)\n  \n}\n\nWhen we try to run calculate_bmi_2 with a negative weight we now receive an error:\n\ncalculate_bmi_2(weight_kg = -50, height_m = 1.80)\n\nWe also added a check whether the calculated BMI is within the normal range:\n\ncalculate_bmi_2(weight_kg = 25, height_m = 1.80)\n\nWarning in calculate_bmi_2(weight_kg = 25, height_m = 1.8): The calculated BMI\nis outside the normal range. Please check your input values.\n\n\n[1] 7.716049\n\n\nRunning calculate_bmi_2 with appropriate inputs:\n\ncalculate_bmi_2(weight_kg = 75, height_m = 1.80)\n\n[1] 23.14815",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 4B: Functions"
    ]
  },
  {
    "objectID": "presentations/presentation4B.html#out-sourcing-functions-to-an-rscript-you-source",
    "href": "presentations/presentation4B.html#out-sourcing-functions-to-an-rscript-you-source",
    "title": "Presentation 4B: Scripting in R - Functions",
    "section": "Out-sourcing functions to an Rscript you source",
    "text": "Out-sourcing functions to an Rscript you source\nIt is cleaner to collect all your functions in one place, and perhaps that place should not be your analysis script. You can instead save your functions in a separate R script and source it inside your analysis script to have access to all your functions without them cluttering your workflow.\nWe have create a file named presentation4_functions.R and copied our two function definitions for calculate_bmi and calculate_bmi_2 into it.\nNow we remove our function definitions from the global environment to demonstrate how to source them from an external file.\n\nrm(list = \"calculate_bmi\", \"calculate_bmi_2\")\n\nBy sourcing a script, all global variables (including functions) in that script will be loaded and appear in the Global environment in the top left corner. Here we source the functions.R script. Check the environment to confirm that the two functions appeared.\n\nsource('./presentation4B.R')\n\nAfter we sourced the functions script the calculate_bmi function can be used just like if it was defined in the main script. If you work on a larger project and write multiple functions, it is best practice to have a function script and source it in your main script.\n\ncalculate_bmi_2(weight_kg = 67, \n                height_m = 1.70)\n\n[1] 23.18339",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 4B: Functions"
    ]
  },
  {
    "objectID": "presentations/presentation4B.html#apply-functions",
    "href": "presentations/presentation4B.html#apply-functions",
    "title": "Presentation 4B: Scripting in R - Functions",
    "section": "Apply functions",
    "text": "Apply functions\nIf you want to run a function multiple times — especially across elements in a vector, matrix, or list — R provides a powerful set of tools called the apply family of functions. These are often used instead of writing explicit for-loops and we will go through three of them here.\n\napply()\nUsed for matrices or 2D arrays. MARGIN = 1: apply function to rows MARGIN = 2: apply function to columns\n\napply(df, 1, sum)   # sum of each row - does not make sense here\n\n       1        2        3        4        5 \n159.5051       NA 159.2485 122.2460 118.1660 \n\napply(df, 2, mean)  # mean of each column\n\n      age weight_kg  height_m       bmi \n    33.40     74.20      1.71        NA \n\n\n\n\nlapply()\nApplies a function to each element of a list (or vector) and returns a list.\n\nlapply(FUN = sum, \n       list(c(1, 2, 3, 4, 6),\n            c(5, 7, 8, 4, 6), \n            c(5, 32, 6, 74, 6536)))\n\n[[1]]\n[1] 16\n\n[[2]]\n[1] 30\n\n[[3]]\n[1] 6653\n\n\n\n\nmapply()\nApplies a function to multiple inputs in parallel — like a for loop across multiple vectors. You can also use mapply as an alternative to calling the function in a for-loop (calculate_bmi_2(weight_kg[i], height_m[i]) for each row).\n\nmapply(FUN = calculate_bmi_2, \n       weight_kg = df$weight_kg, \n       height_m = df$height_m)\n\n[1] 27.75510 30.91558 26.29849 19.73598 21.27600",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 4B: Functions"
    ]
  },
  {
    "objectID": "presentations/s4_objects.html",
    "href": "presentations/s4_objects.html",
    "title": "R Objects: S4 Objects",
    "section": "",
    "text": "Bioconductor packages often use S4 objects to store their data. Some examples:\n\n(Ranged)SummarizedExperiment is an object that holds rectangular matrices of high-throughput omics data (RNA-Seq, ChIP-Seq, ATAC-Seq)\npyholoseq is an objects that stores phylogenetic data such as OTU tables, taxonomy ect for e.g. microbiome and general metagenomics\nDESeqDataSet is an object that stores counts, transformations and metadata for RNAseq analysis. It is subclass of (Ranged)SummarizedExperiment\n\nUnlike the S3 objects (named lists with a class) we looked at before, S4 objects have clearly defined elements called ‘slots’.\nA slot must have:\n\na name\na defined data type\nit may have validation rules\n\nThey are usually accessed by specialized functions. Let’s have a look at a SummarizedExperiment objects.\n\n\n\nlibrary(SummarizedExperiment)\nlibrary(airway)\nlibrary(tidyverse)\n\n#load the airway dataset\ndata(airway, package = 'airway')\n#You can read about the dataset by doing:\n#?airway\n\n\ntypeof(airway)\n\n[1] \"S4\"\n\nclass(airway)\n\n[1] \"RangedSummarizedExperiment\"\nattr(,\"package\")\n[1] \"SummarizedExperiment\"\n\n\nairway is a RangedSummarizedExperiment object. We can get a good overview of the content of Bioconductor S4 objects by just accessing them:\n\nairway\n\nclass: RangedSummarizedExperiment \ndim: 63677 8 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\nThere is only one assay and it is count data. We might have several assays in the same RangedSummarizedExperiment object, i.e. one for raw counts and one for normalized counts or rlog transformed counts.\nOur rows are genes with Ensembl Gene Identifiers (ENSG00000000003, ect) and our columns are samples. There have been 64102 genes measured (look at the dimensions).\n\n\n\nWe generally access slots by functions that have the same name as the slot. The (raw) counts for example are in the assay slot:\n\ncount_matrix &lt;- assay(airway, 'counts')\nhead(count_matrix)\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\nENSG00000000938          0          0          2          0          1\n                SRR1039517 SRR1039520 SRR1039521\nENSG00000000003       1047        770        572\nENSG00000000005          0          0          0\nENSG00000000419        799        417        508\nENSG00000000457        331        233        229\nENSG00000000460         63         76         60\nENSG00000000938          0          0          0\n\nclass(count_matrix)\n\n[1] \"matrix\" \"array\" \n\n\nThe count data is in an array/matrix where each row is a gene and each column is a sample and the gene names are in the row names. They don’t have their own column because unlike dataframes matrices can only have numeric values.\nWe haven’t explicitly worked with this data type before, but many functions in R can be applied to different types of objects so let’s try getting the row sums:\n\nrowSums(count_matrix) %&gt;% head()\n\nENSG00000000003 ENSG00000000005 ENSG00000000419 ENSG00000000457 ENSG00000000460 \n           5935               0            4279            1936             467 \nENSG00000000938 \n              3 \n\n\nWe can see that gene ENSG00000000005 does not have counts in any of our sample (the row sum is 0). How many of such genes are there?\n\nsum(rowSums(count_matrix) == 0)\n\n[1] 30208\n\n\nAbout half of the genes in the catalog have not been seen.\nRemember these are raw counts so we’re not going to do too much with them. If we are going to compare counts across samples or genes we will have to normalize them first. This is a topic for the RNAseq course! (insert shameless self promotion).\n\n\n\nThe metadata slot contains the experimental data underlying the counts:\n\nmetadata(airway)\n\n[[1]]\nExperiment data\n  Experimenter name: Himes BE \n  Laboratory: NA \n  Contact information:  \n  Title: RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells. \n  URL: http://www.ncbi.nlm.nih.gov/pubmed/24926665 \n  PMIDs: 24926665 \n\n  Abstract: A 226 word abstract is available. Use 'abstract' method.\n\n\nWhat we would traditionally describe as metadata in for example an RNAseq experiment is in the colData slot instead:\n\ncolData(airway)\n\nDataFrame with 8 rows and 9 columns\n           SampleName     cell      dex    albut        Run avgLength\n             &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt;   &lt;factor&gt; &lt;integer&gt;\nSRR1039508 GSM1275862  N61311     untrt    untrt SRR1039508       126\nSRR1039509 GSM1275863  N61311     trt      untrt SRR1039509       126\nSRR1039512 GSM1275866  N052611    untrt    untrt SRR1039512       126\nSRR1039513 GSM1275867  N052611    trt      untrt SRR1039513        87\nSRR1039516 GSM1275870  N080611    untrt    untrt SRR1039516       120\nSRR1039517 GSM1275871  N080611    trt      untrt SRR1039517       126\nSRR1039520 GSM1275874  N061011    untrt    untrt SRR1039520       101\nSRR1039521 GSM1275875  N061011    trt      untrt SRR1039521        98\n           Experiment    Sample    BioSample\n             &lt;factor&gt;  &lt;factor&gt;     &lt;factor&gt;\nSRR1039508  SRX384345 SRS508568 SAMN02422669\nSRR1039509  SRX384346 SRS508567 SAMN02422675\nSRR1039512  SRX384349 SRS508571 SAMN02422678\nSRR1039513  SRX384350 SRS508572 SAMN02422670\nSRR1039516  SRX384353 SRS508575 SAMN02422682\nSRR1039517  SRX384354 SRS508576 SAMN02422673\nSRR1039520  SRX384357 SRS508579 SAMN02422683\nSRR1039521  SRX384358 SRS508580 SAMN02422677\n\n\nThis dataframe tells us for each sample whether it was treated with dexamethasone (dex) or albuterol (albut), as well the Run ID, Experiment II and some other identifiers.\nA cool thing about (Ranged)SummarizedExperiment objects is that the $ syntax directly indexes the colData. For example we can get a vector of sample names or whether the samples were treated with dexamethasone like so:\n\n#the $ syntax directly indexes the metadata, i.e.\nairway$SampleName\n\n[1] GSM1275862 GSM1275863 GSM1275866 GSM1275867 GSM1275870 GSM1275871 GSM1275874\n[8] GSM1275875\n8 Levels: GSM1275862 GSM1275863 GSM1275866 GSM1275867 ... GSM1275875\n\nairway$dex\n\n[1] untrt trt   untrt trt   untrt trt   untrt trt  \nLevels: trt untrt\n\n\n\n\n\nAnother neat thing about (Ranged)SummarizedExperiment objects is that they can be subset like matrices.\nLet’s have brief example of a matrix:\n\nmat &lt;- matrix(1:12, nrow = 3, ncol = 4)\n\n# Assign row and column names\nrownames(mat) &lt;- c(\"Gene1\", \"Gene2\", \"Gene3\")\ncolnames(mat) &lt;- c(\"Sample1\", \"Sample2\", \"Sample3\", \"Sample4\")\n\nmat\n\n      Sample1 Sample2 Sample3 Sample4\nGene1       1       4       7      10\nGene2       2       5       8      11\nGene3       3       6       9      12\n\n\nMatrices are subset with this syntax:\n\nmat[rows,columns]\n\nLets say we only want to look at Genes 1 and 2 we can indicate that by:\n\nrownames\nrowindices\na boolean vector\n\n\n#rownames\nmat[c(\"Gene1\", \"Gene2\"), ]\n\n      Sample1 Sample2 Sample3 Sample4\nGene1       1       4       7      10\nGene2       2       5       8      11\n\n#rowindices\nmat[c(1,3),]\n\n      Sample1 Sample2 Sample3 Sample4\nGene1       1       4       7      10\nGene3       3       6       9      12\n\n#boolean vector\nkeep &lt;- c(T,F,T)\nmat[keep,]\n\n      Sample1 Sample2 Sample3 Sample4\nGene1       1       4       7      10\nGene3       3       6       9      12\n\n\nAnd similarly for columns in the second field:\n\n# Select Sample1 & Sample3 (all rows)\nmat[, c(\"Sample1\", \"Sample3\")]  \n\n      Sample1 Sample3\nGene1       1       7\nGene2       2       8\nGene3       3       9\n\n\nWe can use that syntax to subset the entire (Ranged)SummarizedExperiment object (not just the count matrix in the assay slot)!\nFor example we can make a new (Ranged)SummarizedExperiment object with only the samples treated with dexamethasone.\n\n#create a boolean vector of which samples to select\nairway$dex\n\n[1] untrt trt   untrt trt   untrt trt   untrt trt  \nLevels: trt untrt\n\nkeep &lt;- airway$dex == 'trt'\nkeep\n\n[1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n\n\n\n#use the vector to subset the object as if were a matrix!\ndex_treated_samples &lt;- airway[, airway$dex == \"trt\"]\nclass(dex_treated_samples)\n\n[1] \"RangedSummarizedExperiment\"\nattr(,\"package\")\n[1] \"SummarizedExperiment\"\n\ndex_treated_samples\n\nclass: RangedSummarizedExperiment \ndim: 63677 4 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(4): SRR1039509 SRR1039513 SRR1039517 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\nThat’s all for now!"
  },
  {
    "objectID": "presentations/s4_objects.html#s4-objects",
    "href": "presentations/s4_objects.html#s4-objects",
    "title": "R Objects: S4 Objects",
    "section": "",
    "text": "Bioconductor packages often use S4 objects to store their data. Some examples:\n\n(Ranged)SummarizedExperiment is an object that holds rectangular matrices of high-throughput omics data (RNA-Seq, ChIP-Seq, ATAC-Seq)\npyholoseq is an objects that stores phylogenetic data such as OTU tables, taxonomy ect for e.g. microbiome and general metagenomics\nDESeqDataSet is an object that stores counts, transformations and metadata for RNAseq analysis. It is subclass of (Ranged)SummarizedExperiment\n\nUnlike the S3 objects (named lists with a class) we looked at before, S4 objects have clearly defined elements called ‘slots’.\nA slot must have:\n\na name\na defined data type\nit may have validation rules\n\nThey are usually accessed by specialized functions. Let’s have a look at a SummarizedExperiment objects.\n\n\n\nlibrary(SummarizedExperiment)\nlibrary(airway)\nlibrary(tidyverse)\n\n#load the airway dataset\ndata(airway, package = 'airway')\n#You can read about the dataset by doing:\n#?airway\n\n\ntypeof(airway)\n\n[1] \"S4\"\n\nclass(airway)\n\n[1] \"RangedSummarizedExperiment\"\nattr(,\"package\")\n[1] \"SummarizedExperiment\"\n\n\nairway is a RangedSummarizedExperiment object. We can get a good overview of the content of Bioconductor S4 objects by just accessing them:\n\nairway\n\nclass: RangedSummarizedExperiment \ndim: 63677 8 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\nThere is only one assay and it is count data. We might have several assays in the same RangedSummarizedExperiment object, i.e. one for raw counts and one for normalized counts or rlog transformed counts.\nOur rows are genes with Ensembl Gene Identifiers (ENSG00000000003, ect) and our columns are samples. There have been 64102 genes measured (look at the dimensions).\n\n\n\nWe generally access slots by functions that have the same name as the slot. The (raw) counts for example are in the assay slot:\n\ncount_matrix &lt;- assay(airway, 'counts')\nhead(count_matrix)\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\nENSG00000000938          0          0          2          0          1\n                SRR1039517 SRR1039520 SRR1039521\nENSG00000000003       1047        770        572\nENSG00000000005          0          0          0\nENSG00000000419        799        417        508\nENSG00000000457        331        233        229\nENSG00000000460         63         76         60\nENSG00000000938          0          0          0\n\nclass(count_matrix)\n\n[1] \"matrix\" \"array\" \n\n\nThe count data is in an array/matrix where each row is a gene and each column is a sample and the gene names are in the row names. They don’t have their own column because unlike dataframes matrices can only have numeric values.\nWe haven’t explicitly worked with this data type before, but many functions in R can be applied to different types of objects so let’s try getting the row sums:\n\nrowSums(count_matrix) %&gt;% head()\n\nENSG00000000003 ENSG00000000005 ENSG00000000419 ENSG00000000457 ENSG00000000460 \n           5935               0            4279            1936             467 \nENSG00000000938 \n              3 \n\n\nWe can see that gene ENSG00000000005 does not have counts in any of our sample (the row sum is 0). How many of such genes are there?\n\nsum(rowSums(count_matrix) == 0)\n\n[1] 30208\n\n\nAbout half of the genes in the catalog have not been seen.\nRemember these are raw counts so we’re not going to do too much with them. If we are going to compare counts across samples or genes we will have to normalize them first. This is a topic for the RNAseq course! (insert shameless self promotion).\n\n\n\nThe metadata slot contains the experimental data underlying the counts:\n\nmetadata(airway)\n\n[[1]]\nExperiment data\n  Experimenter name: Himes BE \n  Laboratory: NA \n  Contact information:  \n  Title: RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells. \n  URL: http://www.ncbi.nlm.nih.gov/pubmed/24926665 \n  PMIDs: 24926665 \n\n  Abstract: A 226 word abstract is available. Use 'abstract' method.\n\n\nWhat we would traditionally describe as metadata in for example an RNAseq experiment is in the colData slot instead:\n\ncolData(airway)\n\nDataFrame with 8 rows and 9 columns\n           SampleName     cell      dex    albut        Run avgLength\n             &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt;   &lt;factor&gt; &lt;integer&gt;\nSRR1039508 GSM1275862  N61311     untrt    untrt SRR1039508       126\nSRR1039509 GSM1275863  N61311     trt      untrt SRR1039509       126\nSRR1039512 GSM1275866  N052611    untrt    untrt SRR1039512       126\nSRR1039513 GSM1275867  N052611    trt      untrt SRR1039513        87\nSRR1039516 GSM1275870  N080611    untrt    untrt SRR1039516       120\nSRR1039517 GSM1275871  N080611    trt      untrt SRR1039517       126\nSRR1039520 GSM1275874  N061011    untrt    untrt SRR1039520       101\nSRR1039521 GSM1275875  N061011    trt      untrt SRR1039521        98\n           Experiment    Sample    BioSample\n             &lt;factor&gt;  &lt;factor&gt;     &lt;factor&gt;\nSRR1039508  SRX384345 SRS508568 SAMN02422669\nSRR1039509  SRX384346 SRS508567 SAMN02422675\nSRR1039512  SRX384349 SRS508571 SAMN02422678\nSRR1039513  SRX384350 SRS508572 SAMN02422670\nSRR1039516  SRX384353 SRS508575 SAMN02422682\nSRR1039517  SRX384354 SRS508576 SAMN02422673\nSRR1039520  SRX384357 SRS508579 SAMN02422683\nSRR1039521  SRX384358 SRS508580 SAMN02422677\n\n\nThis dataframe tells us for each sample whether it was treated with dexamethasone (dex) or albuterol (albut), as well the Run ID, Experiment II and some other identifiers.\nA cool thing about (Ranged)SummarizedExperiment objects is that the $ syntax directly indexes the colData. For example we can get a vector of sample names or whether the samples were treated with dexamethasone like so:\n\n#the $ syntax directly indexes the metadata, i.e.\nairway$SampleName\n\n[1] GSM1275862 GSM1275863 GSM1275866 GSM1275867 GSM1275870 GSM1275871 GSM1275874\n[8] GSM1275875\n8 Levels: GSM1275862 GSM1275863 GSM1275866 GSM1275867 ... GSM1275875\n\nairway$dex\n\n[1] untrt trt   untrt trt   untrt trt   untrt trt  \nLevels: trt untrt\n\n\n\n\n\nAnother neat thing about (Ranged)SummarizedExperiment objects is that they can be subset like matrices.\nLet’s have brief example of a matrix:\n\nmat &lt;- matrix(1:12, nrow = 3, ncol = 4)\n\n# Assign row and column names\nrownames(mat) &lt;- c(\"Gene1\", \"Gene2\", \"Gene3\")\ncolnames(mat) &lt;- c(\"Sample1\", \"Sample2\", \"Sample3\", \"Sample4\")\n\nmat\n\n      Sample1 Sample2 Sample3 Sample4\nGene1       1       4       7      10\nGene2       2       5       8      11\nGene3       3       6       9      12\n\n\nMatrices are subset with this syntax:\n\nmat[rows,columns]\n\nLets say we only want to look at Genes 1 and 2 we can indicate that by:\n\nrownames\nrowindices\na boolean vector\n\n\n#rownames\nmat[c(\"Gene1\", \"Gene2\"), ]\n\n      Sample1 Sample2 Sample3 Sample4\nGene1       1       4       7      10\nGene2       2       5       8      11\n\n#rowindices\nmat[c(1,3),]\n\n      Sample1 Sample2 Sample3 Sample4\nGene1       1       4       7      10\nGene3       3       6       9      12\n\n#boolean vector\nkeep &lt;- c(T,F,T)\nmat[keep,]\n\n      Sample1 Sample2 Sample3 Sample4\nGene1       1       4       7      10\nGene3       3       6       9      12\n\n\nAnd similarly for columns in the second field:\n\n# Select Sample1 & Sample3 (all rows)\nmat[, c(\"Sample1\", \"Sample3\")]  \n\n      Sample1 Sample3\nGene1       1       7\nGene2       2       8\nGene3       3       9\n\n\nWe can use that syntax to subset the entire (Ranged)SummarizedExperiment object (not just the count matrix in the assay slot)!\nFor example we can make a new (Ranged)SummarizedExperiment object with only the samples treated with dexamethasone.\n\n#create a boolean vector of which samples to select\nairway$dex\n\n[1] untrt trt   untrt trt   untrt trt   untrt trt  \nLevels: trt untrt\n\nkeep &lt;- airway$dex == 'trt'\nkeep\n\n[1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n\n\n\n#use the vector to subset the object as if were a matrix!\ndex_treated_samples &lt;- airway[, airway$dex == \"trt\"]\nclass(dex_treated_samples)\n\n[1] \"RangedSummarizedExperiment\"\nattr(,\"package\")\n[1] \"SummarizedExperiment\"\n\ndex_treated_samples\n\nclass: RangedSummarizedExperiment \ndim: 63677 4 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(4): SRR1039509 SRR1039513 SRR1039517 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\nThat’s all for now!"
  },
  {
    "objectID": "presentations/presentation3.html",
    "href": "presentations/presentation3.html",
    "title": "Presentation 3 - EDA - Multivariate Analysis",
    "section": "",
    "text": "In this section, we’ll move past basic summary stats and simple bivariate plots to uncover more complex relationships in our data.\nWe’ll use PCA (Principal Component Analysis) — a popular dimensionality reduction method — to reveal hidden structure and highlight the most informative patterns. in the data.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "presentations/presentation3.html#load-packages-and-the-data",
    "href": "presentations/presentation3.html#load-packages-and-the-data",
    "title": "Presentation 3 - EDA - Multivariate Analysis",
    "section": "Load packages and the data",
    "text": "Load packages and the data\nLet’s get started by loading the necessary libraries and the cleaned dataset we explored in previous sections:\n\nlibrary(tidyverse)\n# FactoMineR for PCA computation and factoextra for pretty plots\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(ggpubr)\n# if (!require(\"BiocManager\", quietly = TRUE))\n#    install.packages(\"BiocManager\")\n# BiocManager::install(\"DESeq2\")\nlibrary(DESeq2)\n\n\n# Load the pre-saved RData file\nload(\"../data/Ovarian_comb_prep_Col.RData\")\n# Subset the data we will use and drop NA values\ndf_comb &lt;- df_comb %&gt;%\n  select(\n    where(is.integer) | \n    where(is.factor) | \n    where(is.numeric)\n  ) %&gt;% \n  drop_na(where(is.integer))",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "presentations/presentation3.html#normality-and-data-transformation",
    "href": "presentations/presentation3.html#normality-and-data-transformation",
    "title": "Presentation 3 - EDA - Multivariate Analysis",
    "section": "Normality and Data Transformation",
    "text": "Normality and Data Transformation\nBefore running PCA or other multivariate analyses, it’s important to check if our variables are roughly normally distributed, since many methods assume or work best with normal data.\nOne simple way to do this is with a QQ (Quantile-Quantile) plots, which visually compares the distribution of our data to a normal distribution.\n\nChecking for Normality: QQ Plot\nLet’s check the normality of all integer variables at once by reshaping the data and plotting QQ plots for each.\n\n# Pivot integer columns to long format for easy faceting\ndf_long &lt;- df_comb %&gt;% \n  pivot_longer(where(is.integer), \n               names_to = \"variable\", \n               values_to = \"value\")\n\n# QQ plots faceted by variable\nggplot(df_long, aes(sample = value)) + \n  geom_qq_line(color = \"red\") +\n  geom_qq(color = \"#482878FF\", alpha = 0.7) +\n  labs(title = \"QQ Plot for Gene Expression\",\n       x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal() +\n  facet_wrap(vars(variable), nrow = 2, scales = \"free\")\n\n\n\n\n\n\n\n\n\nIf the points fall along the diagonal line, the data is approximately normal.\nIf the points curve above the line, the data is right-skewed.\nIf the points fall below the line, the data is left-skewed.\n\nThis quick check shows whether a transformation is needed to improve symmetry and model accuracy.\nIn this case, we can see that Age is approximately normally distributed, while percent tumor cells and percent normal cells are not. This is very common — many variables, especially in biological or economic data, naturally tend to be skewed rather than perfectly normal.\n\n\nTransformations to Reduce Skewness\nDepending on your data and goals, it’s often necessary to scale or transform variables for meaningful analysis. If normality is an issue, transformations like a log or square root can help make distributions more symmetric and stabilize variance.\nLet’s apply a transformation to our data now to help reduce skewness and get our variables closer to normal.\n\ndf_raw &lt;- df_comb %&gt;%\n  select(where(is.integer))\n\ndf_scaled &lt;- df_raw %&gt;%\n  select(where(is.integer)) %&gt;%\n  mutate(\n    #days_to_death = log2(days_to_death + 1),\n    days_to_death = sqrt(days_to_death),\n    days_to_tumor_recurrence = log2(days_to_tumor_recurrence + 1),\n    percent_stromal_cells = log2(percent_stromal_cells + 1),\n    #percent_not_cancer_cells = log2(percent_not_cancer_cells + 1),\n    percent_normal_cells = log2(percent_normal_cells + 1),\n    percent_tumor_cells = percent_tumor_cells^3) \n\n\n\nHistogram Before and After Transformation\nLets take a look!\n\np1 &lt;- df_long %&gt;%\n  filter(variable %in% c(\"days_to_death\", \"percent_normal_cells\", \"percent_tumor_cells\")) %&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"#482878FF\", color = \"black\") +\n  theme_minimal() +\n  facet_wrap(vars(variable), nrow = 1, scales = \"free\") +\n  labs(title = \"Before\")\n\ndf_long &lt;- df_scaled %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"value\")\n\np2 &lt;- df_long %&gt;%\n  filter(variable %in% c(\"days_to_death\", \"percent_normal_cells\", \"percent_tumor_cells\")) %&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"#2A788EFF\", color = \"black\") +\n  theme_minimal() +\n  facet_wrap(vars(variable), nrow = 1, scales = \"free\") +\n  labs(title = \"After\")\n\nggarrange(p1, p2, nrow = 2, ncol = 1)\n\n\n\n\n\n\n\n\n\n\nSummary\n\nAlways check for normality before choosing a transformation.\nRight-skewed data can benefit from log or sqrt transformations.\nLeft-skewed data may benefit from squaring.\nQQ plots, histograms are excellent tools to assess transformation effectiveness.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "presentations/presentation3.html#applying-z-score-scaling",
    "href": "presentations/presentation3.html#applying-z-score-scaling",
    "title": "Presentation 3 - EDA - Multivariate Analysis",
    "section": "Applying Z-Score Scaling",
    "text": "Applying Z-Score Scaling\nZ-score standardization is essential when variables have different units or ranges\n\np1 &lt;- df_comb %&gt;%\n  select(where(is.integer)) %&gt;%\n  drop_na() %&gt;%\n  pivot_longer(everything(), names_to = \"Sample\", values_to = \"Value\") %&gt;%\n  ggplot(aes(x = Sample, y = Value, fill = Sample)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Boxplot: raw\") +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5)) +\n  scale_fill_viridis_d()\n\n  \np2 &lt;- as.tibble(scale(df_comb %&gt;%\n                        select(where(is.integer)) %&gt;% \n                        drop_na())) %&gt;%\n  pivot_longer(everything(), names_to = \"Sample\", values_to = \"Value\")  %&gt;%\n  ggplot(aes(x = Sample, y = Value, fill = Sample)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Scaled (Z-score)\") +\n  theme(legend.position=\"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5)) +\n  scale_fill_viridis_d()\n\nggarrange(p1, p2, nrow = 1)",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "presentations/presentation3.html#variance-stabilization",
    "href": "presentations/presentation3.html#variance-stabilization",
    "title": "Presentation 3 - EDA - Multivariate Analysis",
    "section": "Variance Stabilization",
    "text": "Variance Stabilization\nNow that we’ve explored data tranformations and PCAs, it’s time to address another important aspect: variance stabilization.\n\nLog or square-root transformations can help reduce variance when larger values have higher variability.\nIf that’s not enough, try more robust methods like:\n\nrlog() (regularized log transformation)\nvarianceStabilizingTransformation() (VST)\n\n\nThese advanced tools, available in the DESeq2 package, are specifically designed for stabilizing variance in count data.\n\nExample: Apply VST, rlog, and log2 Transformations\nLets examine this using our gene expression data.\nSince our expression values are already log2-transformed, we’ll first reverse them back to raw counts. Then, we can test and compare various transformations side-by-side to see how they affect variance and distribution.\n\n# 1. Extract COL genes and convert to counts\ndf_matrix &lt;- df_comb %&gt;%\n  select(starts_with('COL')) %&gt;%\n  mutate(across(everything(), ~ 2^.)) %&gt;%\n  mutate(across(everything(), as.integer)) %&gt;%\n  as.matrix()\n\n# 3. Create variance stabilized versions\nlog2_df &lt;- log2(df_matrix)\nrlog_df &lt;- rlog(df_matrix, blind = TRUE)\nvst_df  &lt;- varianceStabilizingTransformation(df_matrix, blind = TRUE)\n\n\n\nComparing Mean-Variance Relationships\nNow we visualize how well each transformation stabilizes variance:\n\n# 4. Function for mean-variance plot\nplot_mean_var &lt;- function(mat, title) {\n  df_mv &lt;- tibble(\n    mean = rowMeans(mat),\n    variance = apply(mat, 1, var),\n    rank = rank(rowMeans(mat))\n  )\n  \n  ggplot(df_mv, aes(x = rank, y = variance)) +\n    geom_point(alpha = 0.5, color = \"#482878FF\") +\n    theme_minimal() +\n    labs(title = title, x = \"Mean Rank\", y = \"Variance\")\n}\n\nmv1 &lt;- plot_mean_var(df_matrix, \"Mean-Variance: Raw Counts\")\nmv2 &lt;- plot_mean_var(log2_df, \"Mean-Variance: log2(counts + 1)\")\nmv3 &lt;- plot_mean_var(rlog_df, \"Mean-Variance: rlog\")\nmv4 &lt;- plot_mean_var(vst_df, \"Mean-Variance: VST\")\n\n# 5. Arrange and display plots\nggarrange(mv1, mv2, mv3, mv4, ncol = 2, nrow = 2)\n\n\n\n\n\n\n\n\nWe can see that log2 helps, but rlog and VST do a better job of flattening the variance. However, rlog may overly compress high-expression values. For that reason, we will proceed using the VST-transformed data.\n\n\nExpression Distribution Comparison\nLet’s inspect how the transformations affect overall data spread:\n\n# Function for boxplot\nplot_box &lt;- function(mat, title) {\n  df_long &lt;- as.data.frame(mat) %&gt;%\n    pivot_longer(everything(), names_to = \"Sample\", values_to = \"Expression\")\n  \n  ggplot(df_long, aes(x = Sample, y = Expression, fill = Sample)) +\n    geom_boxplot() +\n    theme_minimal() +\n    labs(title = title) +\n    theme(legend.position=\"none\",\n          axis.text.x = element_text(angle = 90, vjust = 0.5)) +\n    scale_fill_viridis_d()\n}\n\n# Generate plots\np1 &lt;- plot_box(log2_df, \"Boxplot: log2\")\np2 &lt;- plot_box(rlog_df, \"Boxplot: rlog\")\np3 &lt;- plot_box(vst_df, \"Boxplot: VST\")\n\n\n# Arrange and display plots\nggarrange(p1, p2, p3, nrow = 3)\n\n\n\n\n\n\n\n\nWe will go with the VST!\n\n\nCaution: Transformations Are Not a Cure-All\n\nNot every issue is solvable with a transformation\nConsider non-linear models or weighted least squares when appropriate\nInterpret results on the transformed scale, but report findings on the original scale when possible\n\n\nBest Practice: Try multiple transformations and assess both statistical fit and interpretability.",
    "crumbs": [
      "Course Material",
      "Presentations",
      "Presentation 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "presentations/presentation6.html",
    "href": "presentations/presentation6.html",
    "title": "Presentation 6: ???",
    "section": "",
    "text": "print('Hello World')\n\n[1] \"Hello World\""
  },
  {
    "objectID": "exercises/exercise4A.html",
    "href": "exercises/exercise4A.html",
    "title": "Exercise 4A: Scripting in R - Conditions and For-loops",
    "section": "",
    "text": "In this exercise you will practice your scripting in R.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 4A: Conditions and For-loops"
    ]
  },
  {
    "objectID": "exercises/exercise4A.html#getting-started",
    "href": "exercises/exercise4A.html#getting-started",
    "title": "Exercise 4A: Scripting in R - Conditions and For-loops",
    "section": "Getting started",
    "text": "Getting started\nLoad libraries and the joined diabetes data set.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 4A: Conditions and For-loops"
    ]
  },
  {
    "objectID": "exercises/exercise4A.html#if-else-statements",
    "href": "exercises/exercise4A.html#if-else-statements",
    "title": "Exercise 4A: Scripting in R - Conditions and For-loops",
    "section": "If-else statements",
    "text": "If-else statements\nIn these exercises we don’t use the dataframe yet, that comes later when we have loops. For this part, just declare variables to test your statements, e.g. bp &lt;- 120.\n\nWrite an if-else statement that prints whether a person has high (more than 100), low (lower than 50) or normal blood pressure (between 50 and 100).\nWrite an if-else statement that assigns people high, moderate or low health risk based on their smoking habits (variable Smoker) and BMI:\n\n\nSmoker and BMI greater than 35 -&gt; high risk\nSmoker or BMI greater than 35 -&gt; moderate risk\notherwise low risk\n\nAnd Smoker should be one of “Smoker”, “Former”, “Never”, “Unknown”.\nVerify that your statement works for different combinations of smoking habits and BMI.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 4A: Conditions and For-loops"
    ]
  },
  {
    "objectID": "exercises/exercise4A.html#loops",
    "href": "exercises/exercise4A.html#loops",
    "title": "Exercise 4A: Scripting in R - Conditions and For-loops",
    "section": "Loops",
    "text": "Loops\n\nCreate a vector with at least five elements of your choice. Use a for loop to print each element individually.\nPrint each column name in the diabetes_glucose data frame using a for loop.\nLoop over all rows of diabetes_glucose and determine whether the person’s blood pressure is high, low or normal with the same conditions as in 1. Print the blood pressure value as well as the statement so you can verify whether you have classified the blood pressure correctly as high, normal or low.\nLoop over all rows of diabetes_glucose and extract the smoking habits and BMI for each row and determine the health risk with the same conditions as in Exercise 4.2. Print the smoking habits and BMI as well as the health risk level to make it easier to see whether your code works correctly.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nExtract value for i’th row in specific column: df$col1[i]\nAn easy way to printing several variables is to pass a vector into print: print(c(this, and_that, and_this_too))\n\n\n\n\nDo the same as above but instead of printing the risk status, append it to a list. Start by initiating an empty list.\n\n\n# Initiate list\nrisk_status &lt;- list()\n\n\nCheck the length of the list. Is it as expected?\n\nSince we looped through all the rows in the diabetes_glucose dataframe, the list should be as long as there are row in the dataframe.\n\nAdd the list as a new column in the diabetes_glucose data frame. Note: Before assigning it, use the unlist() function to convert the list to a flat vector. This ensures that each value aligns correctly with the rows of the data frame.\nMake a list of all the column names in diabetes_glucose that contain categorical variables. Write a for loop that goes through the list and prints a barplot for each of the categorical variables.\nMake a list of all the column names in diabetes_glucose that contain numeric variables. Make a for loop that goes through the list and prints a boxplot for each of the categorical variables.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 4A: Conditions and For-loops"
    ]
  },
  {
    "objectID": "exercises/ExtraExercise5.html",
    "href": "exercises/ExtraExercise5.html",
    "title": "Extra Exercise 5",
    "section": "",
    "text": "e1. Find the best single predictor in the Diabetes dataset. This is done by comparing the null model (no predictors) to all possible models with one predictor, i.e. outcome ~ predictor, outcome ~ predictor2, ect. The null model can be formulated like so: outcome ~ 1 (only the intercept). Fit all possible one predictor models and compare their fit to the null model with a likelihood ratio test. Find the predictor with the lowest p-value in the likelihood ratio test. This can be done in a loop in order to avoid writing out all models.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo use a formula with a variable you will need to combine the literal part and the variable with paste, e.g. paste(\"Outcome ~\", my_pred).\n\n\n\n\n# Define the null model (intercept-only model)\nnull_model &lt;- glm(Diabetes ~ 1, data = train, family = binomial)\n\n# Get predictor names (excluding the outcome variable)\npredictors &lt;- setdiff(names(diabetes_nona), c(\"Diabetes\", \"ID\"))\n\n# Initialize an empty data frame to store results\nresults &lt;- data.frame(Predictor = character(), ChiSq = numeric(), P_Value = numeric(), stringsAsFactors = FALSE)\n\n# Loop through each predictor and fit a logistic regression model\nfor (pred in predictors) {\n  \n  # Fit model with single predictor\n  model_pred &lt;- glm(paste(\"Diabetes ~\", pred), data = train, family = binomial)\n  \n  # Perform Likelihood Ratio Test\n  test_result &lt;- anova(null_model, model_pred, test = \"Chisq\")\n  \n  # Extract Chi-square statistic and p-value\n  chi_sq &lt;- test_result$Deviance[2]  # The second row corresponds to the predictor model\n  p_value &lt;- test_result$`Pr(&gt;Chi)`[2]\n  \n  # Store results\n  results &lt;- rbind(results, data.frame(Predictor = pred, ChiSq = chi_sq, P_Value = p_value))\n}\n\n# Print the results sorted by p-value\nresults &lt;- results %&gt;% arrange(P_Value)\nprint(results)\n\ne2. Write a function that handles visualization of k-means clustering results. Think about which information you need to pass and what it should return."
  },
  {
    "objectID": "exercises/ExtraExercise5.html#extra-exercises",
    "href": "exercises/ExtraExercise5.html#extra-exercises",
    "title": "Extra Exercise 5",
    "section": "",
    "text": "e1. Find the best single predictor in the Diabetes dataset. This is done by comparing the null model (no predictors) to all possible models with one predictor, i.e. outcome ~ predictor, outcome ~ predictor2, ect. The null model can be formulated like so: outcome ~ 1 (only the intercept). Fit all possible one predictor models and compare their fit to the null model with a likelihood ratio test. Find the predictor with the lowest p-value in the likelihood ratio test. This can be done in a loop in order to avoid writing out all models.\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo use a formula with a variable you will need to combine the literal part and the variable with paste, e.g. paste(\"Outcome ~\", my_pred).\n\n\n\n\n# Define the null model (intercept-only model)\nnull_model &lt;- glm(Diabetes ~ 1, data = train, family = binomial)\n\n# Get predictor names (excluding the outcome variable)\npredictors &lt;- setdiff(names(diabetes_nona), c(\"Diabetes\", \"ID\"))\n\n# Initialize an empty data frame to store results\nresults &lt;- data.frame(Predictor = character(), ChiSq = numeric(), P_Value = numeric(), stringsAsFactors = FALSE)\n\n# Loop through each predictor and fit a logistic regression model\nfor (pred in predictors) {\n  \n  # Fit model with single predictor\n  model_pred &lt;- glm(paste(\"Diabetes ~\", pred), data = train, family = binomial)\n  \n  # Perform Likelihood Ratio Test\n  test_result &lt;- anova(null_model, model_pred, test = \"Chisq\")\n  \n  # Extract Chi-square statistic and p-value\n  chi_sq &lt;- test_result$Deviance[2]  # The second row corresponds to the predictor model\n  p_value &lt;- test_result$`Pr(&gt;Chi)`[2]\n  \n  # Store results\n  results &lt;- rbind(results, data.frame(Predictor = pred, ChiSq = chi_sq, P_Value = p_value))\n}\n\n# Print the results sorted by p-value\nresults &lt;- results %&gt;% arrange(P_Value)\nprint(results)\n\ne2. Write a function that handles visualization of k-means clustering results. Think about which information you need to pass and what it should return."
  },
  {
    "objectID": "exercises/ExtraExercise5.html#quarto",
    "href": "exercises/ExtraExercise5.html#quarto",
    "title": "Extra Exercise 5",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "exercises/ExtraExercise5.html#running-code",
    "href": "exercises/ExtraExercise5.html#running-code",
    "title": "Extra Exercise 5",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "exercises/exercise0.html",
    "href": "exercises/exercise0.html",
    "title": "Exercise 0: Getting started",
    "section": "",
    "text": "To get this material you went to the github repository of this course: https://github.com/Center-for-Health-Data-Science/R4DataScience, pressed Code → Download ZIP and downloaded the directory with the course material called R4DataScience or R4DataScience-main. This folder contains several sub directory with more and less relevant material. The files you need for this course is:\n\n\nslides/R4datascience_slides.pdf: contains the lecture slides in pdf format.\npresentations/*: all the scripts that the lectures will go through in plenum during the course.\nexercises/*: all the exercises that you will go through yourselves. You are currently looking at exercises/exercise0.qmd.\ndata/*: all the data you need for the exercises.\n\n\nUnder the course directory, make a sub directory for the outputs that you will generate doing the exercises.\n\nYour file tree should look something like this:",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 0: R script and Quarto"
    ]
  },
  {
    "objectID": "exercises/exercise0.html#file-management-data-download",
    "href": "exercises/exercise0.html#file-management-data-download",
    "title": "Exercise 0: Getting started",
    "section": "",
    "text": "To get this material you went to the github repository of this course: https://github.com/Center-for-Health-Data-Science/R4DataScience, pressed Code → Download ZIP and downloaded the directory with the course material called R4DataScience or R4DataScience-main. This folder contains several sub directory with more and less relevant material. The files you need for this course is:\n\n\nslides/R4datascience_slides.pdf: contains the lecture slides in pdf format.\npresentations/*: all the scripts that the lectures will go through in plenum during the course.\nexercises/*: all the exercises that you will go through yourselves. You are currently looking at exercises/exercise0.qmd.\ndata/*: all the data you need for the exercises.\n\n\nUnder the course directory, make a sub directory for the outputs that you will generate doing the exercises.\n\nYour file tree should look something like this:",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 0: R script and Quarto"
    ]
  },
  {
    "objectID": "exercises/exercise0.html#working-directories",
    "href": "exercises/exercise0.html#working-directories",
    "title": "Exercise 0: Getting started",
    "section": "Working directories",
    "text": "Working directories\n\nAs you have Rstudio open, access your current working directory by typing getwd() in the console.\n\nThe working directory in R (and other programming environments) is the folder on your computer where R looks for files to read or write by default. When you load or save data, R will use the working directory unless you specify another path. A path can either be absolute or relative:\n\nAbsolute path: The path from the root of your file system to the input file.\nRelative path: The path from the working directory to the input file.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 0: R script and Quarto"
    ]
  },
  {
    "objectID": "exercises/exercise0.html#r-script",
    "href": "exercises/exercise0.html#r-script",
    "title": "Exercise 0: Getting started",
    "section": "R Script",
    "text": "R Script\n\nCreate an R script and save it in your exercise folder.\n\nAn R script is a plain text file containing a series of R commands and code used for data analysis. R scripts have a .R extension and can be executed line-by-line in an interactive R session or as a whole script. They are ideal for automating workflows and keeping your analyses reproducible and organized. R scripts can be submitted to a job on a supercomputer unlike Quarto documents.\n\nType getwd() in your R script and run the line. Compare the working directory with the one from the console.\nChange the working directory using setwd().\nRun getwd() again.\nRead in the file from data/diabetes.csv using the read_csv() function and check the structure of the data with the str() function. Re-save the file.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 0: R script and Quarto"
    ]
  },
  {
    "objectID": "exercises/exercise0.html#quarto",
    "href": "exercises/exercise0.html#quarto",
    "title": "Exercise 0: Getting started",
    "section": "Quarto",
    "text": "Quarto\n\nCreate an Quarto document and save it in your exercise folder.\n\nQuarto is an open-source publishing system designed to help you create dynamic, reproducible documents, presentations, and websites. It extends the ideas of tools like R Markdown, combining simplicity with powerful customization options for modern scientific and technical communication.\n\nType getwd() in a code chunk in your Quarto document and run the line. Compare the working directory with the one from the console.\nChange the working directory in one chunk using setwd().\nRun getwd() in the same chunk as setwd() AND in another chunk. What do you observe?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nsetwd('PATH')\ngetwd()\n\n\ngetwd()\n\n\n\n\n\nCreate a code chunk and write the same code as you did in 8. Write a description of what you did above the code chunk. Re-save the file.\nRender the Quarto document and have a look at the html file.\n\n\n\n\n\n\n\nResources for Quarto\n\n\n\n\n\n\nQuarto website\n\n“Get started with Quarto” tutorial for RStudio\n\n“Get started with Quarto” video for RStudio\nComprehensive guides to Quarto basics",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 0: R script and Quarto"
    ]
  },
  {
    "objectID": "exercises/exercise0.html#r-project",
    "href": "exercises/exercise0.html#r-project",
    "title": "Exercise 0: Getting started",
    "section": "R project",
    "text": "R project\nAn R project in RStudio creates a self-contained working environment tied to a specific folder, which becomes the default working directory for all scripts, data, and outputs. This structure helps organize files, ensures reproducibility, and simplifies path management. By default, a Quarto document’s working directory is its file location. While this can be changed chunk-wise, the working directory for R scripts can be set globally for all scripts in a folder by creating an R project. The R project is a small configuration file, usually placed in the root of the project folder, and requires no manual interaction—it quietly ensures your workflows remain well-organized.\n\n\n\n\n\n\nBefore doing this exercise, we need to brief you about what will happen when you do the instructions below:\n\n\nRstudio will ask you if you want to save your current workspace and to that you will answer Don’t Save.\nRstudio will close down your current session and open a new one which means that this Quarto will not be open in Rstudio anymore. After you have done the instructions below, simply navigate to this document and reopen it.\n\nGreat - now to the exercise:\nCreate an R project by clicking the Project (None) in the top right → New Project → Existing Directory and choose an appropriate location. Look at the top-right corner to check that you are in your R project.\n\nReopen the the R script and Quarto document you created in Exercise 4 and 9 respectively. Check each of their working directories. Are they as you expect? Explain.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nThe working directory of the R script is the same as the location of the .Rproj file.\nThe working directory of the Quarto document is always the same as the location of the document.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 0: R script and Quarto"
    ]
  },
  {
    "objectID": "exercises/exercise3.html",
    "href": "exercises/exercise3.html",
    "title": "Exercise 3: Exploratory Data Analysis",
    "section": "",
    "text": "Load in the packages the you think you need for this exercise. You will be doing PCA, so have a look at which packages we used in Presentation 3. No worries if you forget any, you always load then later on.\nRead in the joined diabetes data set you created in Exercise 2. If you did not make it all the way through Exercise 2 you can find the dataset you need in ../data/exercise2_diabetes_glucose.xlsx.\nHave a look at the data type (numeric, categorical, factor) of each column to ensure these make sense. If need, convert variables to the correct (most logical) type.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "exercises/exercise3.html#getting-started",
    "href": "exercises/exercise3.html#getting-started",
    "title": "Exercise 3: Exploratory Data Analysis",
    "section": "",
    "text": "Load in the packages the you think you need for this exercise. You will be doing PCA, so have a look at which packages we used in Presentation 3. No worries if you forget any, you always load then later on.\nRead in the joined diabetes data set you created in Exercise 2. If you did not make it all the way through Exercise 2 you can find the dataset you need in ../data/exercise2_diabetes_glucose.xlsx.\nHave a look at the data type (numeric, categorical, factor) of each column to ensure these make sense. If need, convert variables to the correct (most logical) type.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "exercises/exercise3.html#check-data-distributions",
    "href": "exercises/exercise3.html#check-data-distributions",
    "title": "Exercise 3: Exploratory Data Analysis",
    "section": "Check Data Distributions",
    "text": "Check Data Distributions\nLet’s have a look at the distributions of the numerical variables.\n\nMake histograms of the three Glucose (mmol/L) measurements in your dataset. What do you observe? Are the three groups of values normally distributed?\nJust as in question 3 above, make histograms of the three Glucose (mmol/L) measurement variables, BUT this time stratify your dataset by the variable Diabetes. How do your distributions look now?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTry: facet_wrap(Var1 ~ Var2, scales = \"free\").\n\n\n\n\nMake a qqnorm plot for the other numeric variables; Age, Bloodpressure, BMI, PhysicalActivity and Serum_ca2. What do the plots tell you?\nFrom the qq-norm plot above you will see that especially one of the variables seems to be far from normally distributed. What type of transformation could you potentially apply to this variable to make it normal? Transform and make a histogram or qqnorm plot. Did the transformation help?\n\nLuckily for us it is not a requirement for dimensionality reduction methods like PCA that neither variables nor their residuals were normally distributed. Requirements for normality becomes important when performing statistical tests and modelling, including t-tests, ANOVA and regression models (more on this in part 5).",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "exercises/exercise3.html#data-cleaning",
    "href": "exercises/exercise3.html#data-cleaning",
    "title": "Exercise 3: Exploratory Data Analysis",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nWhile PCA is very forgiving in terms of variable distributions, there are some things it does not handle well, including missing values and varying ranges of numeric variables. So, before you go on you need to do a little data management.\n\nThe Glucose (mmol/L) variable in the dataset which denotes the result of the Oral Glucose Tolerance Test with measurements at times the 0, 60, 120 min should be separated out into three columns, one for each time point.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\npivot_wider(names_from = ..., values_from = ..., names_prefix = 'Measurement\\_')\n\n\n\n\nRemove any missing values from your dataset.\nExtract all numeric variables AND scale these so they have comparable ranges. This numeric dataset you will use for PCA.\nExtract all categorical variables from your dataset and save them to a new object. You will use these as labels for the PCA.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "exercises/exercise3.html#principal-component-analysis",
    "href": "exercises/exercise3.html#principal-component-analysis",
    "title": "Exercise 3: Exploratory Data Analysis",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\n\nCalculate the PCs on your numeric dataset using the function PCA() from the FactoMineR package. Note that you should set scale.unit = FALSE as you have already scaled your dataset.\nMake a plot that shows how much variance is captured by each component (in each dimension). There are two ways of making this plot: either you can use the function fviz_screeplot() from the factoextra package as we did in the exercise OR, you can use ggplot2, by extracting the res.pca$eig and plotting the percentage of variance column.\nNow, make a biplot (the round one with the arrows we showed in the presentation). This type of plot is a little complicated with ggplot alone, so either use the fviz_pca_var() function from the factoextra package we showed in the presentation, or - if you want to challenge yourself - try autoplot() from the ggfortify package.\nHave a look at the $var$contrib from your PCA object and compare it to the biplot, what do they tell you, i.e. which variables contribute the most to PC1 (dim1) and PC2 (dim2)? Also, look at the correlation matrix between variables in each component ($var$cor), what can you conclude from this?\nPlot your samples in the new dimensions, i.e. PC1 (dim1) vs PC2 (dim2), with the function fviz_pca_ind(). Add color and/or labels to the points using categorical variables you extracted above. What pattern of clustering (in any) do you observe?\nTry to call $var$cos2 from your PCA object. What does it tell you about the first 5 components? Are there any of these, in addition to Dim 1 and Dim 2, which seem to capture some variance? If so, try to plot these components and overlay the plot with the categorical variables from above to figure out which it might be capturing.\n\nThe Oral Glucose Tolerance Test is the one used to produce the OGTT measurements (0, 60, 120). As these measurement are directly used to diagnose diabetes we are not surprised that they separate the dataset well - we are being completely bias.\n\nOmit the Glucose measurement columns, calculate a PCA and create the plot above. Do the remaining variables still capture the Diabetes pattern?\nFrom the exploratory analysis you have performed, which variables (numeric and/or categorical) would you as a minimum include in a model for prediction of Diabetes?\n\nN.B again you should not include the actual glucose measurement variables as they where used to the define the outcome you are looking at.",
    "crumbs": [
      "Course Material",
      "Exercises",
      "Exercise 3: Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "exercises/exercise6.html",
    "href": "exercises/exercise6.html",
    "title": "Exercise 6: Understanding and improving an R script",
    "section": "",
    "text": "In this exercise you go through the script analysis.R together with your neighbors, find out what the data is that is being worked with and which analysis is done and how. There are also some things that could be an issue, so have an eye out and try to think about how you could improve this script."
  },
  {
    "objectID": "tdhh/tdhh4A.html",
    "href": "tdhh/tdhh4A.html",
    "title": "Exercise 4A - Solutions: Scripting in R - Conditions and For-loops",
    "section": "",
    "text": "In this exercise you will practice your scripting in R.",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 4A: Conditions and For-loops"
    ]
  },
  {
    "objectID": "tdhh/tdhh4A.html#getting-started",
    "href": "tdhh/tdhh4A.html#getting-started",
    "title": "Exercise 4A - Solutions: Scripting in R - Conditions and For-loops",
    "section": "Getting started",
    "text": "Getting started\nLoad libraries and the joined diabetes data set.\n\nlibrary(tidyverse)\nlibrary(glue)\n\n\ndiabetes_glucose &lt;- readxl::read_excel('../data/exercise2_diabetes_glucose.xlsx')\ndiabetes_glucose\n\n# A tibble: 1,422 × 13\n   ID    Sex      Age BloodPressure   BMI PhysicalActivity Smoker  Diabetes\n   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 34120 Female    28            75  25.4               92 Never          0\n 2 34120 Female    28            75  25.4               92 Never          0\n 3 34120 Female    28            75  25.4               92 Never          0\n 4 27458 Female    55            72  24.6               86 Never          0\n 5 27458 Female    55            72  24.6               86 Never          0\n 6 27458 Female    55            72  24.6               86 Never          0\n 7 70630 Male      22            80  24.9              139 Unknown        0\n 8 70630 Male      22            80  24.9              139 Unknown        0\n 9 70630 Male      22            80  24.9              139 Unknown        0\n10 13861 Female    56            72  37.1               64 Unknown        1\n# ℹ 1,412 more rows\n# ℹ 5 more variables: Serum_ca2 &lt;dbl&gt;, Married &lt;chr&gt;, Work &lt;chr&gt;,\n#   Measurement &lt;chr&gt;, `Glucose (mmol/L)` &lt;dbl&gt;",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 4A: Conditions and For-loops"
    ]
  },
  {
    "objectID": "tdhh/tdhh4A.html#if-else-statements",
    "href": "tdhh/tdhh4A.html#if-else-statements",
    "title": "Exercise 4A - Solutions: Scripting in R - Conditions and For-loops",
    "section": "If-else statements",
    "text": "If-else statements\nIn these exercises we don’t use the dataframe yet, that comes later when we have loops. For this part, just declare variables to test your statements, e.g. bp &lt;- 120.\n\nWrite an if-else statement that prints whether a person has high (more than 100), low (lower than 50) or normal blood pressure (between 50 and 100).\n\n\nbp &lt;- 80\n\nif (bp &gt; 100){\n  print('High blood pressure')\n} else if (bp &lt; 50) {\n  print('Low blood pressure')\n} else {\n  print('Normal blood pressure')\n} \n\n[1] \"Normal blood pressure\"\n\n\n\nWrite an if-else statement that assigns people high, moderate or low health risk based on their smoking habits (variable Smoker) and BMI:\n\n\nSmoker and BMI greater than 35 -&gt; high risk\nSmoker or BMI greater than 35 -&gt; moderate risk\notherwise low risk\n\nAnd Smoker should be one of “Smoker”, “Former”, “Never”, “Unknown”.\nVerify that your statement works for different combinations of smoking habits and BMI.\n\nSmoker &lt;- 'Smoker'\nBMI &lt;- 40\n\nif (Smoker == 'Smoker' & BMI &gt; 35){\n  print('High risk')\n} else if (Smoker == 'Smoker' | BMI &gt; 35) {\n  print('Moderate risk')\n} else {\n  print('Low risk')\n}\n\n[1] \"High risk\"",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 4A: Conditions and For-loops"
    ]
  },
  {
    "objectID": "tdhh/tdhh4A.html#loops",
    "href": "tdhh/tdhh4A.html#loops",
    "title": "Exercise 4A - Solutions: Scripting in R - Conditions and For-loops",
    "section": "Loops",
    "text": "Loops\n\nCreate a vector with at least five elements of your choice. Use a for loop to print each element individually.\n\n\nmy_v &lt;- c(1, 78, 5, 'hello', 7)\n\nfor (el in my_v) {\n  print(el)\n}\n\n[1] \"1\"\n[1] \"78\"\n[1] \"5\"\n[1] \"hello\"\n[1] \"7\"\n\n\n\nPrint each column name in the diabetes_glucose data frame using a for loop.\n\n\nfor (col in colnames(diabetes_glucose)) {\n  print(col)\n}\n\n[1] \"ID\"\n[1] \"Sex\"\n[1] \"Age\"\n[1] \"BloodPressure\"\n[1] \"BMI\"\n[1] \"PhysicalActivity\"\n[1] \"Smoker\"\n[1] \"Diabetes\"\n[1] \"Serum_ca2\"\n[1] \"Married\"\n[1] \"Work\"\n[1] \"Measurement\"\n[1] \"Glucose (mmol/L)\"\n\n\n\nLoop over all rows of diabetes_glucose and determine whether the person’s blood pressure is high, low or normal with the same conditions as in 1. Print the blood pressure value as well as the statement so you can verify whether you have classified the blood pressure correctly as high, normal or low.\n\n\n#We'll only show the first 10 rows here for brevity\n#for (i in 1:nrow(diabetes_glucose)) {\n\nfor (i in 1:10) {\n  bp &lt;- diabetes_glucose$BloodPressure[i]\n\n  if (bp &gt; 100){\n    print(paste(bp,'is high blood pressure'))\n  } else if (bp &lt; 50) {\n    print(paste(bp,'is low blood pressure'))\n  } else {\n    print(paste(bp,'is normal blood pressure'))\n  } \n  \n}\n\n[1] \"75 is normal blood pressure\"\n[1] \"75 is normal blood pressure\"\n[1] \"75 is normal blood pressure\"\n[1] \"72 is normal blood pressure\"\n[1] \"72 is normal blood pressure\"\n[1] \"72 is normal blood pressure\"\n[1] \"80 is normal blood pressure\"\n[1] \"80 is normal blood pressure\"\n[1] \"80 is normal blood pressure\"\n[1] \"72 is normal blood pressure\"\n\n\n\nLoop over all rows of diabetes_glucose and extract the smoking habits and BMI for each row and determine the health risk with the same conditions as in Exercise 4.2. Print the smoking habits and BMI as well as the health risk level to make it easier to see whether your code works correctly.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nExtract value for i’th row in specific column: df$col1[i]\nAn easy way to printing several variables is to pass a vector into print: print(c(this, and_that, and_this_too))\n\n\n\n\n#We'll only show the first 10 rows here for brevity\n#for (i in 1:nrow(diabetes_glucose)) {\n\nfor (i in 1:10) {\n  Smoker &lt;- diabetes_glucose$Smoker[i]\n  BMI &lt;- diabetes_glucose$BMI[i]\n\n  if (Smoker == 'Smoker' & BMI &gt; 35){\n    print(c(Smoker, BMI, 'High risk'))\n  } else if (Smoker == 'Smoker' | BMI &gt; 35) {\n    print(c(Smoker, BMI,'Moderate risk'))\n  } else {\n    print(c(Smoker, BMI,'Low risk'))\n  }\n}\n\n[1] \"Never\"    \"25.4\"     \"Low risk\"\n[1] \"Never\"    \"25.4\"     \"Low risk\"\n[1] \"Never\"    \"25.4\"     \"Low risk\"\n[1] \"Never\"    \"24.6\"     \"Low risk\"\n[1] \"Never\"    \"24.6\"     \"Low risk\"\n[1] \"Never\"    \"24.6\"     \"Low risk\"\n[1] \"Unknown\"  \"24.9\"     \"Low risk\"\n[1] \"Unknown\"  \"24.9\"     \"Low risk\"\n[1] \"Unknown\"  \"24.9\"     \"Low risk\"\n[1] \"Unknown\"       \"37.1\"          \"Moderate risk\"\n\n\n\nDo the same as above but instead of printing the risk status, append it to a list. Start by initiating an empty list.\n\n\n# Initiate list\nrisk_status &lt;- list()\n\n\nfor (i in 1:nrow(diabetes_glucose)) {\n  Smoker &lt;- diabetes_glucose$Smoker[i]\n  BMI &lt;- diabetes_glucose$BMI[i]\n    \n  if (Smoker == 'Smoker' & BMI &gt; 35){\n    risk_status &lt;- append(risk_status, 'High risk')\n  } else if (Smoker == 'Smoker' | BMI &gt; 35) {\n    risk_status &lt;- append(risk_status, 'Moderate risk')\n  } else {\n    risk_status &lt;- append(risk_status, 'Low risk')\n  }\n}\n\n\nrisk_status %&gt;% head()\n\n[[1]]\n[1] \"Low risk\"\n\n[[2]]\n[1] \"Low risk\"\n\n[[3]]\n[1] \"Low risk\"\n\n[[4]]\n[1] \"Low risk\"\n\n[[5]]\n[1] \"Low risk\"\n\n[[6]]\n[1] \"Low risk\"\n\n\n\nCheck the length of the list. Is it as expected?\n\nSince we looped through all the rows in the diabetes_glucose dataframe, the list should be as long as there are row in the dataframe.\n\nlength(risk_status)\n\n[1] 1422\n\n\n\nnrow(diabetes_glucose)\n\n[1] 1422\n\n\n\nAdd the list as a new column in the diabetes_glucose data frame. Note: Before assigning it, use the unlist() function to convert the list to a flat vector. This ensures that each value aligns correctly with the rows of the data frame.\n\n\ndiabetes_glucose$risk_status &lt;- unlist(risk_status)\n\n\ndiabetes_glucose %&gt;% select(BMI, Smoker, risk_status)\n\n# A tibble: 1,422 × 3\n     BMI Smoker  risk_status  \n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        \n 1  25.4 Never   Low risk     \n 2  25.4 Never   Low risk     \n 3  25.4 Never   Low risk     \n 4  24.6 Never   Low risk     \n 5  24.6 Never   Low risk     \n 6  24.6 Never   Low risk     \n 7  24.9 Unknown Low risk     \n 8  24.9 Unknown Low risk     \n 9  24.9 Unknown Low risk     \n10  37.1 Unknown Moderate risk\n# ℹ 1,412 more rows\n\n\n\nMake a list of all the column names in diabetes_glucose that contain categorical variables. Write a for loop that goes through the list and prints a barplot for each of the categorical variables.\n\n\ncategorical &lt;- list('Sex', 'Smoker', 'Diabetes', 'Married', 'Work')\n\n\nfor (var in categorical){\n  \n  p &lt;- ggplot(diabetes_glucose, \n         aes(x = !!sym(var))) + \n    geom_bar() +\n    labs(title = paste('Barplot of', var))\n  \n  print(p)\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMake a list of all the column names in diabetes_glucose that contain numeric variables. Make a for loop that goes through the list and prints a boxplot for each of the categorical variables.\n\n\nhead(diabetes_glucose)\n\n# A tibble: 6 × 14\n  ID    Sex      Age BloodPressure   BMI PhysicalActivity Smoker Diabetes\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 34120 Female    28            75  25.4               92 Never         0\n2 34120 Female    28            75  25.4               92 Never         0\n3 34120 Female    28            75  25.4               92 Never         0\n4 27458 Female    55            72  24.6               86 Never         0\n5 27458 Female    55            72  24.6               86 Never         0\n6 27458 Female    55            72  24.6               86 Never         0\n# ℹ 6 more variables: Serum_ca2 &lt;dbl&gt;, Married &lt;chr&gt;, Work &lt;chr&gt;,\n#   Measurement &lt;chr&gt;, `Glucose (mmol/L)` &lt;dbl&gt;, risk_status &lt;chr&gt;\n\n\n\nnumeric &lt;- list('Age', 'BloodPressure', 'BMI', 'PhysicalActivity', 'Serum_ca2')\n\n\nfor (var in numeric){\n  \n  p &lt;- ggplot(diabetes_glucose, \n         aes(y = !!sym(var))) + \n    geom_boxplot() + \n    labs(title = paste('Boxplot of', var))\n  \n  print(p)\n  \n}\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_boxplot()`).",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 4A: Conditions and For-loops"
    ]
  },
  {
    "objectID": "tdhh/tdhh1.html",
    "href": "tdhh/tdhh1.html",
    "title": "Solution 1: Data Cleaning",
    "section": "",
    "text": "Load packages.\n\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n\nLoad in the diabetes_clinical_toy_messy.xlsx data set.\n\n\ndiabetes_clinical &lt;- read_excel('../data/diabetes_clinical_toy_messy.xlsx')\nhead(diabetes_clinical)\n\n# A tibble: 6 × 9\n     ID Sex      Age BloodPressure   BMI PhysicalActivity Smoker  Diabetes\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1  9046 Male      34            84  24.7               93 Unknown        0\n2 51676 Male      25            74  22.5              102 Unknown        0\n3 31112 Male      30             0  32.3               75 Former         1\n4 60182 Male      50            80  34.5               98 Unknown        1\n5  1665 Female    27            60  26.3               82 Never          0\n6 56669 Male      35            84  35                 58 Smoker         1\n# ℹ 1 more variable: Serum_ca2 &lt;dbl&gt;",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 1: Data Cleaning"
    ]
  },
  {
    "objectID": "tdhh/tdhh1.html#getting-started",
    "href": "tdhh/tdhh1.html#getting-started",
    "title": "Solution 1: Data Cleaning",
    "section": "",
    "text": "Load packages.\n\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n\nLoad in the diabetes_clinical_toy_messy.xlsx data set.\n\n\ndiabetes_clinical &lt;- read_excel('../data/diabetes_clinical_toy_messy.xlsx')\nhead(diabetes_clinical)\n\n# A tibble: 6 × 9\n     ID Sex      Age BloodPressure   BMI PhysicalActivity Smoker  Diabetes\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1  9046 Male      34            84  24.7               93 Unknown        0\n2 51676 Male      25            74  22.5              102 Unknown        0\n3 31112 Male      30             0  32.3               75 Former         1\n4 60182 Male      50            80  34.5               98 Unknown        1\n5  1665 Female    27            60  26.3               82 Never          0\n6 56669 Male      35            84  35                 58 Smoker         1\n# ℹ 1 more variable: Serum_ca2 &lt;dbl&gt;",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 1: Data Cleaning"
    ]
  },
  {
    "objectID": "tdhh/tdhh1.html#explore-the-data",
    "href": "tdhh/tdhh1.html#explore-the-data",
    "title": "Solution 1: Data Cleaning",
    "section": "Explore the data",
    "text": "Explore the data\n\nHow many missing values (NA’s) are there in each column.\n\n\ncolSums(is.na(diabetes_clinical))\n\n              ID              Sex              Age    BloodPressure \n               0                0                3                0 \n             BMI PhysicalActivity           Smoker         Diabetes \n               3                0                0                0 \n       Serum_ca2 \n               0 \n\n\n\nCheck the ranges and distribution of each of the numeric variables in the dataset. Do any values seem weird or unexpected? Extract summary statistics on these, e.g. means and standard deviation.\n\nFor the numerical variables we’ll plot and check the range:\n\n# Range\nrange(diabetes_clinical$Age, na.rm = TRUE)\n\n[1] 21 81\n\n# Histogram\ndiabetes_clinical$Age %&gt;% hist()\n\n\n\n\n\n\n\n# ggplot2 boxplot\ndiabetes_clinical %&gt;% \n  ggplot(aes(y = Age, x = 1)) + \n  geom_boxplot(fill=\"steelblue\") + \n  theme_minimal()\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nOdd: Some BloodPressure values are 0.\n\nrange(diabetes_clinical$BloodPressure, na.rm = TRUE)\n\n[1]   0 114\n\ndiabetes_clinical %&gt;% \n  ggplot(aes(x = BloodPressure)) + \n  geom_histogram(color='black', fill='grey') + \n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nOdd: Some BMI values are 0.\n\nrange(diabetes_clinical$BMI, na.rm = TRUE)\n\n[1]  0.0 57.1\n\ndiabetes_clinical %&gt;% \n  ggplot(aes(y = BMI, x = 1)) + \n  geom_violin(fill=\"#ADC698\") +\n  geom_point() +\n  theme_minimal()\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nrange(diabetes_clinical$PhysicalActivity, na.rm = TRUE)\n\n[1]  19 177\n\ndiabetes_clinical %&gt;% \n  ggplot(aes(y = PhysicalActivity)) + \n  geom_boxplot(color='black', fill='grey') + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nrange(diabetes_clinical$Serum_ca2, na.rm = TRUE)\n\n[1]  8.7 10.2\n\ndiabetes_clinical %&gt;% \n  ggplot(aes(x = Serum_ca2)) + \n  geom_histogram(color='black', fill='#ADC698') + \n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nSome variables in the dataset are categorical or factor variables. Figure out what levels these have and how many observations there are for each level.\n\nFor the categorical variables we can use table() (or if you prefer, count()):\nThe Sex values are not consistent.\n\ntable(diabetes_clinical$Sex)\n\n\nFEMALE Female   Male   male \n     2    291    237      2 \n\ntable(diabetes_clinical$Smoker)\n\n\n Former   Never  Smoker Unknown \n    132     159     162      79 \n\ntable(diabetes_clinical$Diabetes)\n\n\n  0   1 \n267 265",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 1: Data Cleaning"
    ]
  },
  {
    "objectID": "tdhh/tdhh1.html#clean-up-the-data",
    "href": "tdhh/tdhh1.html#clean-up-the-data",
    "title": "Solution 1: Data Cleaning",
    "section": "Clean up the data",
    "text": "Clean up the data\nNow that we have had a look at the data, it is time to correct fixable mistakes and remove observations that cannot be corrected.\nConsider the following:\n\nWhat should we do with the rows that contain NA’s? Do we remove them or keep them?\nWhich odd things in the data can we correct with confidence and which cannot?\nAre there zeros in the data? Are they true zeros or errors?\nDo you want to change any of the classes of the variables?\n\n\nMake a clean version of the dataset according to your considerations.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nHave a look at BloodPressure, BMI, and Sex.\n\n\n\nMy considerations:\n\nWhen modelling, rows with NA’s in the variables we want to model should be removed as we cannot model on NAs. Since there are only NA’s in Age and BMI, the rows can be left until we need to do a model with these columns.\nThe different spellings in Sex should be regularized so that there is only one spelling for each category. Since most rows have the first letter as capital letter and the remaining letter as lowercase we will use that.\nThere are zeros in BMI and BloodPressure. These are considered false zeros as is does not make sense that these variables have a value of 0.\n\nCheck number of rows before cleaning.\n\nnrow(diabetes_clinical)\n\n[1] 532\n\n\nCleaning data according to considerations.\n\ndiabetes_clinical_clean &lt;- diabetes_clinical %&gt;% \n  mutate(Sex = str_to_title(Sex)) %&gt;% \n  filter(BMI != 0, BloodPressure != 0) \n\nCheck the variables that were altered\n\n# Unique sexes now\ndiabetes_clinical_clean$Sex %&gt;% unique()\n\n[1] \"Male\"   \"Female\"\n\n# Range of BMI and blood pressure\nrange(diabetes_clinical_clean$BMI, na.rm = TRUE)\n\n[1] 15.3 57.1\n\nrange(diabetes_clinical_clean$BloodPressure, na.rm = TRUE)\n\n[1]  30 114\n\n\nCheck number of rows after cleaning.\n\nnrow(diabetes_clinical_clean)\n\n[1] 490",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 1: Data Cleaning"
    ]
  },
  {
    "objectID": "tdhh/tdhh1.html#metadata",
    "href": "tdhh/tdhh1.html#metadata",
    "title": "Solution 1: Data Cleaning",
    "section": "Metadata",
    "text": "Metadata\n\nThere is some metadata to accompany the dataset you have just cleaned in diabetes_meta_toy_messy.csv. This is a csv file, not an excel sheet, so you need to use the read_delim() function to load it. Load in the dataset and inspect it.\n\n\ndiabetes_meta &lt;- read_delim('../data/diabetes_meta_toy_messy.csv')\nhead(diabetes_meta)\n\n# A tibble: 6 × 3\n     ID Married Work         \n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        \n1 48368 Yes     Private      \n2 36706 No      Public       \n3 32729 Yes     Private      \n4 48272 Yes     Private      \n5  9404 Yes     Private      \n6 16934 Yes     Self-employed\n\n\n7.1. How many missing values (NA’s) are there in each column.\n\ncolSums(is.na(diabetes_meta))\n\n     ID Married    Work \n      0       0       0 \n\n\n7.2. Check the distribution of each of the variables. Consider that the variables are of different classes. Do any of the distributions seam odd to you?\nFor the categorical variables:\n\ntable(diabetes_meta$Married)\n\n\n  No  No   Yes Yes  \n 178    1  332    4 \n\ntable(diabetes_meta$Work)\n\n\n      Private        Public       Retired Self-employed \n          273           150             6            86 \n\n\nBy investigating the unique values of the Married variable we see that some of the values have whitespace.\n\nunique(diabetes_meta$Married)\n\n[1] \"Yes\"  \"No\"   \"Yes \" \"No \" \n\n\n7.3. Make a clean version of the dataset according to your considerations.\nMy considerations:\n\nThe Married variable has whitespace in the some of the values. The values “Yes” and “Yes” will be interpreted as different values. We can confidently remove all the whitespaces in this variable.\n\nCheck number of rows before cleaning.\n\nnrow(diabetes_meta)\n\n[1] 515\n\n\n\ndiabetes_meta_clean &lt;- diabetes_meta %&gt;% \n  mutate(Married = str_trim(Married))\n\nCheck the unique marital status now.\n\nunique(diabetes_meta_clean$Married)\n\n[1] \"Yes\" \"No\" \n\n\nCheck number of rows after cleaning.\n\nnrow(diabetes_meta_clean)\n\n[1] 515",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 1: Data Cleaning"
    ]
  },
  {
    "objectID": "tdhh/tdhh1.html#join-the-datasets",
    "href": "tdhh/tdhh1.html#join-the-datasets",
    "title": "Solution 1: Data Cleaning",
    "section": "Join the datasets",
    "text": "Join the datasets\n\nConsider which variable the datasets should be joined on.\nConsider how you want to join the datasets. Do you want to use full_join, inner_join, left_join and rigth_join?\n\nThe joining variable must be the same type in both datasets.\n\nJoin the cleaned versions of the clinical and metadataset by the variable and with the function you considered above.\n\n\n# We use inner_join to have as few NAs as possiable\ndiabetes_join &lt;- diabetes_clinical_clean %&gt;% \n  inner_join(diabetes_meta_clean, by = 'ID')\n\nnrow(diabetes_join)\n\n[1] 474\n\n\n\nHow many rows does the joined dataset have? Explain how the join-function you used resulted in the given number of rows.\n\n\n# Because we used `full_join`, all the unique IDs across both data sets are kept.\nc(diabetes_clinical_clean$ID, diabetes_meta_clean$ID) %&gt;% \n  unique() %&gt;% \n  length()\n\n[1] 531\n\nnrow(diabetes_join)\n\n[1] 474",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 1: Data Cleaning"
    ]
  },
  {
    "objectID": "tdhh/tdhh1.html#manipulate-some-strings",
    "href": "tdhh/tdhh1.html#manipulate-some-strings",
    "title": "Solution 1: Data Cleaning",
    "section": "Manipulate some strings",
    "text": "Manipulate some strings\nWhen we look at the column Work, we can see that we have three levels Public, Private and Self-employed. Some special characters, including: - / \\[\\] (), etc. can sometimes prove problematic in variable names, as they are also used for operations in R. For example, are dashes (-) used to indicate a contrast in some functions.\n\nTo avoid potential issues in downstream analysis, change the Work variable so that Self-employed becomes SelfEmployed.\n\n\ndiabetes_join &lt;- diabetes_join %&gt;% \n  mutate(Work = str_replace_all(Work, '-e', 'E'))\n\ntable(diabetes_join$Work)\n\n\n     Private       Public      Retired SelfEmployed \n         252          141            4           77 \n\n\nAdditionally, we are not so happy with the fact that the ID is simply denoted by a number. The reason for this is that if we write out our dataset and read it in again (which we will do later), R will think it is a numeric or integer variable.\n\nAdd the string ID_ in front of the number and convert it to a factor variable.\n\n\ndiabetes_join &lt;-  diabetes_join %&gt;% \n  mutate(ID = paste0(\"ID_\", as.factor(ID)))\n\nhead(diabetes_join$ID)\n\n[1] \"ID_34120\" \"ID_27458\" \"ID_70630\" \"ID_13861\" \"ID_68794\" \"ID_64778\"\n\n\n\nExport the joined dataset. Think about which directory you want to save the file in.\n\n\nwritexl::write_xlsx(diabetes_join, '../data/exercise1_diabetes_join.xlsx')",
    "crumbs": [
      "Course Material",
      "Solutions",
      "Solution 1: Data Cleaning"
    ]
  },
  {
    "objectID": "tdhh/tdhh5B.html",
    "href": "tdhh/tdhh5B.html",
    "title": "Exercise 5B - Solutions",
    "section": "",
    "text": "Load the R packages needed for analysis:\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(randomForest)"
  },
  {
    "objectID": "tdhh/tdhh5B.html#summary-statistics",
    "href": "tdhh/tdhh5B.html#summary-statistics",
    "title": "Exercise 5B - Solutions",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\nLoad in the dataset Obt_Perio_ML.Rdata and inspect it.\n\n\nload(file = \"../data/Obt_Perio_ML.Rdata\")\n\n\nDo some basic summary statistics and distributional plots to get a feel for the data. Which types of variables do we have?\n\n\n# Reshape data to long format for ggplot2\nlong_data &lt;- optML %&gt;% \n  dplyr::select(where(is.numeric)) %&gt;%\n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"value\")\n\n# Plot histograms for each numeric variable in one grid\nggplot(long_data, aes(x = value)) +\n  geom_histogram(binwidth = 0.5, fill = \"#9395D3\", color ='grey30') +\n  facet_wrap(~ variable, scales = \"free\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSome of the numeric variables are actually categorical. We have identified them in the facCols vector. Here, we change their type from numeric to character (since the other categorical variables are of this type). This code is sightly different from changing the type to factor, why we have written the code for you. Try to understand what is going on.\n\n\nfacCols &lt;- c(\"Race\", \n             \"ETXU_CAT5\", \n             \"BL.Anti.inf\", \n             \"BL.Antibio\", \n             \"V3.Anti.inf\", \n             \"V3.Antibio\", \n             \"V3.Bac.vag\", \n             \"V5.Anti.inf\",\n             \"V5.Antibio\",\n             \"V5.Bac.vag\",\n             \"X..Vis.Att\")\n\n\noptML &lt;- optML %&gt;%\n  mutate(across(all_of(facCols), as.character))\n\nhead(optML)\n\n# A tibble: 6 × 89\n  PID   Apgar1 Apgar5 Birthweight GA.at.outcome Any.SAE. Clinic Group   Age\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1 P10        9      9        3107           278 No       NY     C        30\n2 P170       9      9        3040           286 No       MN     C        20\n3 P280       8      9        3370           282 No       MN     T        29\n4 P348       9      9        3180           275 No       KY     C        18\n5 P402       8      9        2615           267 No       KY     C        18\n6 P209       8      9        3330           284 No       MN     C        18\n# ℹ 80 more variables: Race &lt;chr&gt;, Education &lt;chr&gt;, Public.Asstce &lt;chr&gt;,\n#   BMI &lt;dbl&gt;, Use.Tob &lt;chr&gt;, N.prev.preg &lt;dbl&gt;, Live.PTB &lt;chr&gt;,\n#   Any.stillbirth &lt;chr&gt;, Any.live.ptb.sb.sp.ab.in.ab &lt;chr&gt;,\n#   EDC.necessary. &lt;chr&gt;, N.qualifying.teeth &lt;dbl&gt;, BL.GE &lt;dbl&gt;, BL..BOP &lt;dbl&gt;,\n#   BL.PD.avg &lt;dbl&gt;, BL..PD.4 &lt;dbl&gt;, BL..PD.5 &lt;dbl&gt;, BL.CAL.avg &lt;dbl&gt;,\n#   BL..CAL.2 &lt;dbl&gt;, BL..CAL.3 &lt;dbl&gt;, BL.Calc.I &lt;dbl&gt;, BL.Pl.I &lt;dbl&gt;,\n#   V3.GE &lt;dbl&gt;, V3..BOP &lt;dbl&gt;, V3.PD.avg &lt;dbl&gt;, V3..PD.4 &lt;dbl&gt;, …\n\n\n\nMake count tables of your categorical/factor variables, are they balanced?\n\n\n# Count observations per level/group for each categorical variable\n\nfactor_counts &lt;- optML[,-1] %&gt;%\n  dplyr::select(where(is.character)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n  count(Variable, Level, name = \"Count\")\n\nfactor_counts\n\n# A tibble: 66 × 3\n   Variable                    Level Count\n   &lt;chr&gt;                       &lt;chr&gt; &lt;int&gt;\n 1 Any.SAE.                    No      865\n 2 Any.SAE.                    Yes     135\n 3 Any.live.ptb.sb.sp.ab.in.ab No      432\n 4 Any.live.ptb.sb.sp.ab.in.ab Yes     568\n 5 Any.stillbirth              No      894\n 6 Any.stillbirth              Yes     106\n 7 BL.Anti.inf                 0       853\n 8 BL.Anti.inf                 1       147\n 9 BL.Antibio                  0       924\n10 BL.Antibio                  1        76\n# ℹ 56 more rows"
  },
  {
    "objectID": "tdhh/tdhh5B.html#part-1-elastic-net-regression",
    "href": "tdhh/tdhh5B.html#part-1-elastic-net-regression",
    "title": "Exercise 5B - Solutions",
    "section": "Part 1: Elastic Net Regression",
    "text": "Part 1: Elastic Net Regression\n\nAs you will use the response Preg.ended...37.wk, you should remove the other five possible outcome variables measures from your dataset.\n\n\noptML &lt;- optML %&gt;% \n  dplyr::select(!c(Apgar1, Apgar5, GA.at.outcome, Birthweight, Any.SAE.))\n\n\nElastic net regression can be sensitive to large differences in the range of numeric/integer variables, as such these variables should be scaled. Scale all numeric/integer variables in your dataset.\n\n\noptML &lt;- optML %&gt;% \n  mutate(across(where(is.numeric), scale))\n\n\nSplit your dataset into train and test set, you should have 70% of the data in the training set and 30% in the test set. How you chose to split is up to you, BUT afterwards you should ensure that for the categorical/factor variables all levels are represented in both sets.\n\n\n# Set seed\nset.seed(123)\n\n# Training set\ntrain &lt;- optML %&gt;% \n  sample_frac(0.70) \n\n# Check group levels\ntrain_counts &lt;- train[,-1] %&gt;%\n  dplyr::select(where(is.character)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n  count(Variable, Level, name = \"Count\")\n\ntrain_counts\n\n# A tibble: 64 × 3\n   Variable                    Level Count\n   &lt;chr&gt;                       &lt;chr&gt; &lt;int&gt;\n 1 Any.live.ptb.sb.sp.ab.in.ab No      302\n 2 Any.live.ptb.sb.sp.ab.in.ab Yes     398\n 3 Any.stillbirth              No      629\n 4 Any.stillbirth              Yes      71\n 5 BL.Anti.inf                 0       606\n 6 BL.Anti.inf                 1        94\n 7 BL.Antibio                  0       649\n 8 BL.Antibio                  1        51\n 9 Bact.vag                    No      620\n10 Bact.vag                    Yes      80\n# ℹ 54 more rows\n\ntest  &lt;- anti_join(optML, train, by = 'PID') \n\n\n\n# Check group levels\n#test_counts &lt;- optML[,-1] %&gt;%\n#  dplyr::select(where(is.character)) %&gt;%\n#  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n#  count(Variable, Level, name = \"Count\")\n\n#test_counts\n\n\nAfter dividing into train and test set pull out the outcome variable, Preg.ended...37.wk, into its own vector for both datasets. Name these y_train and y_test.\n\n\ny_train &lt;- train %&gt;%\n  pull(Preg.ended...37.wk)\n\n\ny_test &lt;- test %&gt;% \n  pull(Preg.ended...37.wk)\n\n\nRemove the outcome variable, Preg.ended...37.wk, from the train and test set, as well as PID (if you have not already done so), as we should obviously not use this for training or testing.\n\n\ntrain &lt;- train %&gt;% \n  dplyr::select(-c(PID, Preg.ended...37.wk))\n\n\ntest &lt;- test %&gt;% \n  dplyr::select(-c(PID, Preg.ended...37.wk))\n\nYou will employ the package glmnet to perform Elastic Net Regression. The main function from this package is glmnet() which we will use to fit the model. Additionally, you will also perform cross validation with cv.glmnet() to obtain the best value of the model hyper-parameter, lambda (\\(λ\\)).\nAs we are working with a mix of categorical and numerical predictors, it is advisable to dummy-code the variables. You can easily do this by creating a model matrix for both the test and train set.\n\nCreate the model matrix needed for input to glmnet() and cv.glmnet().\n\n\nmodTrain &lt;- model.matrix(~ ., data = train)\nmodTest &lt;- model.matrix(~ ., data = test)\n\n\nCreate your Elastic Net Regression model with glmnet().\n\n\nEN_model &lt;- glmnet(modTrain, y_train, alpha = 0.5, family = \"binomial\")\n\n\nUse cv.glmnet() to attain the best value of the hyperparameter lambda (\\(λ\\)). Remember to set a seed for reproducible results.\n\n\nset.seed(123)\ncv_model &lt;- cv.glmnet(modTrain, y_train, alpha = 0.5, family = \"binomial\")\n\n\nPlot all the values of lambda tested during cross validation by calling plot() on the output of your cv.glmnet(). Extract the best lambda value from the cv.glmnet() model and save it as an object.\n\n\nplot(cv_model)\n\n\n\n\n\n\n\nbestLambda &lt;- cv_model$lambda.min\n\nNow, let’s see how well your model performed.\n\nPredict if an individual is likely to give birth before the 37th week using your model and your test set. See pseudo-code below\n\n\ny_pred &lt;- predict(EN_model, s = bestLambda, newx = modTest, type = 'class')\n\n\nJust like for the logistic regression model you can calculate the accuracy of the prediction by comparing it to y_test with confusionMatrix(). Do you have a good accuracy? N.B look at the 2x2 contingency table, what does it tell you?\n\n\ny_pred &lt;- as.factor(y_pred)\n\ncaret::confusionMatrix(y_pred, y_test)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 170  27\n         1  35  68\n                                         \n               Accuracy : 0.7933         \n                 95% CI : (0.743, 0.8377)\n    No Information Rate : 0.6833         \n    P-Value [Acc &gt; NIR] : 1.448e-05      \n                                         \n                  Kappa : 0.533          \n                                         \n Mcnemar's Test P-Value : 0.374          \n                                         \n            Sensitivity : 0.8293         \n            Specificity : 0.7158         \n         Pos Pred Value : 0.8629         \n         Neg Pred Value : 0.6602         \n             Prevalence : 0.6833         \n         Detection Rate : 0.5667         \n   Detection Prevalence : 0.6567         \n      Balanced Accuracy : 0.7725         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\n\nLastly, let’s extract the variables which were retained in the model (e.g. not penalized out). We do this by calling the coefficient with coef() on our model. See pseudo-code below.\n\n\ncoeffs &lt;- coef(EN_model, s = bestLambda)\n\n# Convert coefficients to a data frame for easier viewing\ncoeffsDat &lt;- as.data.frame(as.matrix(coeffs)) %&gt;% \n  rownames_to_column(var = 'VarName')\n\n\nMake a plot that shows the absolute importance of the variables retained in your model. This could be a barplot with variable names on the y-axis and the length of the bars denoting absolute size of coefficient.\n\n\n# Make dataframe ready for plotting, remove intercept and coeffcients that are zero\ncoeffsDat &lt;- coeffsDat %&gt;% \n  mutate(AbsImp = abs(s1)) %&gt;%\n  arrange(AbsImp) %&gt;%\n  mutate(VarName = factor(VarName, levels=VarName)) %&gt;%\n  filter(AbsImp &gt; 0 & VarName != \"(Intercept)\")\n\n\n# Plot\nggplot(coeffsDat, aes(x = VarName, y = AbsImp)) +\n  geom_bar(stat = \"identity\", fill = \"#9395D3\") +\n  coord_flip() +\n  labs(title = \"Feature Importance for Elastic Net\", \n       x = \"Features\", \n       y = \"Absolute Coefficients\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nNow repeat what you just did above, but this time instead of using Preg.ended...37.wk as outcome, try using a continuous variable, such as GA.at.outcome. N.B remember this means that you should evaluate the model using the RMSE and a scatter plot instead of the accuracy!"
  },
  {
    "objectID": "tdhh/tdhh5B.html#part-2-random-forest",
    "href": "tdhh/tdhh5B.html#part-2-random-forest",
    "title": "Exercise 5B - Solutions",
    "section": "Part 2: Random Forest",
    "text": "Part 2: Random Forest\nNow, lets make a Random Forest. We will continue using the Obt_Perio_ML.Rdata with Preg.ended...37.wk as outcome.\n\nJust like in the section on Elastic Net above, remove the outcome variables you will not be using and split the dataset into test and train set - this time keep the outcome variable Preg.ended...37.wk in the dataset. Remember to remove the PID column before training!\n\n\nload(file = \"../data/Obt_Perio_ML.Rdata\")\n\noptML &lt;- optML %&gt;%\n  mutate(Preg.ended...37.wk = factor(Preg.ended...37.wk, levels = c(0, 1), labels = c(\"No\", \"Yes\")))\n\n\noptML &lt;- optML %&gt;% \n  dplyr::select(-c(Apgar1, Apgar5, GA.at.outcome, Birthweight, Any.SAE.))\n\n\nset.seed(123)\n\n# Training set\ntrain &lt;- optML %&gt;% \n  sample_frac(0.70) \n\n\ntest  &lt;- anti_join(optML, train, by = 'PID') \n\n\ntrain &lt;- train %&gt;% \n  dplyr::select(-PID)\n\n\ntest &lt;- test %&gt;% \n  dplyr::select(-PID)\n\n\nSet up a Random Forest model with cross-validation. See pseudo-code below. Remember to set a seed.\n\n\nset.seed(123)\n\n# Set up cross-validation: 5-fold CV\nRFcv &lt;- trainControl(\n  method = \"cv\",\n  number = 5,\n  classProbs = TRUE,\n  summaryFunction = twoClassSummary,\n  savePredictions = \"final\"\n)\n\n# Train Random Forest\nset.seed(123)\nrf_model &lt;- train(\n  Preg.ended...37.wk ~ .,  # formula interface\n  data = train,\n  method = \"rf\",           # random forest\n  trControl = RFcv,\n  metric = \"ROC\",          # optimize AUC\n  #preProcess = c(\"center\", \"scale\"),  # optional\n  tuneLength = 5           # try 5 different mtry values\n)\n\n\n# Model summary\nprint(rf_model)\n\nRandom Forest \n\n700 samples\n 82 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 561, 560, 559, 560, 560 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.9489303  1.0000000  0.8617908\n  23    0.9477493  0.9870033  0.8744681\n  44    0.9473824  0.9783544  0.8743794\n  65    0.9504678  0.9762038  0.8786348\n  86    0.9516068  0.9675316  0.8786348\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 86.\n\n\n\nPlot your model fit. How does your model improve when you add 10, 20, 30, etc. predictors?\n\n\n# Best parameters\nrf_model$bestTune\n\n  mtry\n5   86\n\n# Plot performance\nplot(rf_model)\n\n\n\n\n\n\n\n\n\nUse your test set to evaluate your model performance. How does the random forest compare to the elastic net regression?\n\n\n# Predict class probabilities\ny_pred &lt;- predict(rf_model, newdata = test, type = \"prob\")\n\ny_pred &lt;- as.factor(ifelse(y_pred$Yes &gt; 0.5, \"Yes\", \"No\"))\n\ncaret::confusionMatrix(y_pred, test$Preg.ended...37.wk)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  200   5\n       Yes   5  90\n                                          \n               Accuracy : 0.9667          \n                 95% CI : (0.9396, 0.9839)\n    No Information Rate : 0.6833          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.923           \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9756          \n            Specificity : 0.9474          \n         Pos Pred Value : 0.9756          \n         Neg Pred Value : 0.9474          \n             Prevalence : 0.6833          \n         Detection Rate : 0.6667          \n   Detection Prevalence : 0.6833          \n      Balanced Accuracy : 0.9615          \n                                          \n       'Positive' Class : No              \n                                          \n\n\n\nExtract the predictive variables with the greatest importance from your fit.\n\n\nvarImpOut &lt;- varImp(rf_model)\n\nvarImpOut$importance\n\n                                    Overall\nClinicMN                         0.28829380\nClinicMS                         0.08531874\nClinicNY                         0.00000000\nGroupT                           1.07928446\nAge                             10.07240535\nRace                             8.80554719\nEducationLT 8 yrs                1.00267684\nEducationMT 12 yrs              11.89503411\nPublic.AsstceYes                 0.32547600\nBMI                             17.14493555\nUse.TobYes                       0.93360319\nN.prev.preg                      7.18283216\nLive.PTBYes                      0.66173901\nAny.stillbirthYes                0.45096245\nAny.live.ptb.sb.sp.ab.in.abYes   2.29188038\nEDC.necessary.Yes                0.99830824\nN.qualifying.teeth               6.22248572\nBL.GE                           17.08238031\nBL..BOP                         21.25428834\nBL.PD.avg                        8.55879761\nBL..PD.4                        16.60436630\nBL..PD.5                         7.86271337\nBL.CAL.avg                       5.38848754\nBL..CAL.2                       10.93763207\nBL..CAL.3                        7.66267752\nBL.Calc.I                       16.55322925\nBL.Pl.I                         18.97386933\nV3.GE                            8.49168429\nV3..BOP                         13.16516679\nV3.PD.avg                       10.98797558\nV3..PD.4                        11.45223757\nV3..PD.5                         5.15730509\nV3.CAL.avg                       7.18719557\nV3..CAL.2                        8.20104082\nV3..CAL.3                        6.53037740\nV3.Calc.I                       11.77132909\nV3.Pl.I                         10.67073735\nV5.GE                           19.81274399\nV5..BOP                         15.88170278\nV5.PD.avg                       13.79948005\nV5..PD.4                        14.54736293\nV5..PD.5                        10.37373918\nV5.CAL.avg                      16.32330976\nV5..CAL.2                        9.75285428\nV5..CAL.3                       14.94613503\nV5.Calc.I                       10.58727260\nV5.Pl.I                         30.99189383\nN.PAL.sites1                     0.56069981\nN.PAL.sites3-33                  0.16453921\nBact.vagYes                      0.37261426\nGest.diabYes                    12.56167674\nUTIYes                           2.39344325\nPre.eclampYes                  100.00000000\nBL.Anti.inf                      1.05967811\nBL.Antibio                       0.70625225\nV3.Anti.inf                     14.78301202\nV3.Antibio                       3.32100410\nV3.Bac.vag                       1.20343079\nV5.Anti.inf                      0.42436300\nV5.Antibio                       2.17934136\nV5.Bac.vag                       0.98530726\nX..Vis.Att                      84.09386777\nX1st.Miss.Vis                    6.10011462\nOAA1                            15.05230148\nOCR1                            17.64066750\nOFN1                            14.44055733\nOPG1                            35.49263153\nOPI1                            49.02466848\nOTD1                            30.90159839\nOTF1                            17.66169583\nOCRP1                           25.68160225\nOPGE21                          48.56993592\nOMMP91                          32.19459414\nOFIBRIN1                        25.33219646\nOAA5                            33.61847538\nOCR5                            56.15240845\nOFN5                            40.00491564\nOPG5                            51.88932333\nOPI5                            42.36834783\nOTD5                            15.66229106\nOTF5                            44.52703717\nOCRP5                           21.79824614\nOPGE25                          31.84668616\nOMMP95                          26.57561280\nETXU_CAT5                        4.08094946\nOFIBRIN5                        32.77152764\n\nvarImportance &lt;- as.data.frame(as.matrix(varImpOut$importance)) %&gt;% \n  rownames_to_column(var = 'VarName') %&gt;%\n  arrange(desc(Overall))\n\nvarImportance\n\n                          VarName      Overall\n1                   Pre.eclampYes 100.00000000\n2                      X..Vis.Att  84.09386777\n3                            OCR5  56.15240845\n4                            OPG5  51.88932333\n5                            OPI1  49.02466848\n6                          OPGE21  48.56993592\n7                            OTF5  44.52703717\n8                            OPI5  42.36834783\n9                            OFN5  40.00491564\n10                           OPG1  35.49263153\n11                           OAA5  33.61847538\n12                       OFIBRIN5  32.77152764\n13                         OMMP91  32.19459414\n14                         OPGE25  31.84668616\n15                        V5.Pl.I  30.99189383\n16                           OTD1  30.90159839\n17                         OMMP95  26.57561280\n18                          OCRP1  25.68160225\n19                       OFIBRIN1  25.33219646\n20                          OCRP5  21.79824614\n21                        BL..BOP  21.25428834\n22                          V5.GE  19.81274399\n23                        BL.Pl.I  18.97386933\n24                           OTF1  17.66169583\n25                           OCR1  17.64066750\n26                            BMI  17.14493555\n27                          BL.GE  17.08238031\n28                       BL..PD.4  16.60436630\n29                      BL.Calc.I  16.55322925\n30                     V5.CAL.avg  16.32330976\n31                        V5..BOP  15.88170278\n32                           OTD5  15.66229106\n33                           OAA1  15.05230148\n34                      V5..CAL.3  14.94613503\n35                    V3.Anti.inf  14.78301202\n36                       V5..PD.4  14.54736293\n37                           OFN1  14.44055733\n38                      V5.PD.avg  13.79948005\n39                        V3..BOP  13.16516679\n40                   Gest.diabYes  12.56167674\n41             EducationMT 12 yrs  11.89503411\n42                      V3.Calc.I  11.77132909\n43                       V3..PD.4  11.45223757\n44                      V3.PD.avg  10.98797558\n45                      BL..CAL.2  10.93763207\n46                        V3.Pl.I  10.67073735\n47                      V5.Calc.I  10.58727260\n48                       V5..PD.5  10.37373918\n49                            Age  10.07240535\n50                      V5..CAL.2   9.75285428\n51                           Race   8.80554719\n52                      BL.PD.avg   8.55879761\n53                          V3.GE   8.49168429\n54                      V3..CAL.2   8.20104082\n55                       BL..PD.5   7.86271337\n56                      BL..CAL.3   7.66267752\n57                     V3.CAL.avg   7.18719557\n58                    N.prev.preg   7.18283216\n59                      V3..CAL.3   6.53037740\n60             N.qualifying.teeth   6.22248572\n61                  X1st.Miss.Vis   6.10011462\n62                     BL.CAL.avg   5.38848754\n63                       V3..PD.5   5.15730509\n64                      ETXU_CAT5   4.08094946\n65                     V3.Antibio   3.32100410\n66                         UTIYes   2.39344325\n67 Any.live.ptb.sb.sp.ab.in.abYes   2.29188038\n68                     V5.Antibio   2.17934136\n69                     V3.Bac.vag   1.20343079\n70                         GroupT   1.07928446\n71                    BL.Anti.inf   1.05967811\n72              EducationLT 8 yrs   1.00267684\n73              EDC.necessary.Yes   0.99830824\n74                     V5.Bac.vag   0.98530726\n75                     Use.TobYes   0.93360319\n76                     BL.Antibio   0.70625225\n77                    Live.PTBYes   0.66173901\n78                   N.PAL.sites1   0.56069981\n79              Any.stillbirthYes   0.45096245\n80                    V5.Anti.inf   0.42436300\n81                    Bact.vagYes   0.37261426\n82               Public.AsstceYes   0.32547600\n83                       ClinicMN   0.28829380\n84                N.PAL.sites3-33   0.16453921\n85                       ClinicMS   0.08531874\n86                       ClinicNY   0.00000000\n\n\n\nMake a logistic regression using the same dataset (you already have your train data, test data). How do the results of Elastic Net regression and Random Forest compare to the output of your glm.\n\n\n# Model\nmodel1 &lt;- glm(Preg.ended...37.wk ~ ., data = train, family = 'binomial')\n\n\n# Filter for significant p-values and convert to tibble\nmodel1out &lt;- coef(summary(model1)) %&gt;% \n  as.data.frame() %&gt;%\n  rownames_to_column(var = 'VarName') %&gt;% \n  filter(`Pr(&gt;|z|)` &lt;= 0.05 & VarName != \"(Intercept)\")\n\nmodel1out\n\n                          VarName     Estimate   Std. Error   z value\n1              EducationMT 12 yrs  0.961956470 0.3382249163  2.844132\n2  Any.live.ptb.sb.sp.ab.in.abYes  0.720906039 0.3153296987  2.286198\n3                       BL.Calc.I  1.026477452 0.4900989813  2.094429\n4                      V5.CAL.avg  3.110659656 1.2473092305  2.493896\n5                       V5..CAL.2 -0.098574105 0.0273704946 -3.601473\n6                         V5.Pl.I -1.338674452 0.5689180486 -2.353018\n7                 N.PAL.sites3-33 -2.331440689 0.7140054954 -3.265298\n8                    Gest.diabYes  1.830763333 0.6214980869  2.945726\n9                          UTIYes  0.821778414 0.3467958330  2.369632\n10                  Pre.eclampYes  2.566754720 0.4750007799  5.403685\n11                     X..Vis.Att -0.927874732 0.2340978787 -3.963619\n12                         OPGE21  0.001396325 0.0004603115  3.033435\n13                           OCR5 -0.251528837 0.0787526271 -3.193910\n14                         OPGE25 -0.002095017 0.0006322809 -3.313427\n       Pr(&gt;|z|)\n1  4.453253e-03\n2  2.224269e-02\n3  3.622179e-02\n4  1.263495e-02\n5  3.164189e-04\n6  1.862173e-02\n7  1.093490e-03\n8  3.221972e-03\n9  1.780581e-02\n10 6.528545e-08\n11 7.382213e-05\n12 2.417867e-03\n13 1.403597e-03\n14 9.216003e-04\n\n# Compare output from Elastic Net with output from glm model\nintersect(as.character(coeffsDat$VarName), model1out$VarName) %&gt;%\n  sort()\n\n [1] \"Any.live.ptb.sb.sp.ab.in.abYes\" \"BL.Calc.I\"                     \n [3] \"Gest.diabYes\"                   \"OCR5\"                          \n [5] \"OPGE21\"                         \"OPGE25\"                        \n [7] \"Pre.eclampYes\"                  \"UTIYes\"                        \n [9] \"V5..CAL.2\"                      \"V5.CAL.avg\"                    \n[11] \"V5.Pl.I\""
  },
  {
    "objectID": "tdhh/tdhh6.html",
    "href": "tdhh/tdhh6.html",
    "title": "Exercise 6: Understanding and improving an R script",
    "section": "",
    "text": "In this exercise you got through the script analysis.R together with your neighbors, find out what the data is that is being worked and which analysis is done and how. There are also some things that could be an issue, so have an eye out and try to think how you could improve the script.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n\ndf &lt;- read_rds('../data/patient_data.rds')\nhead(df)\n\n# A tibble: 6 × 7\n  patient_id   age sex     hdl   ldl    tg bp_readings     \n       &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;          \n1       2463    51 M        60   166   186 &lt;tibble [5 × 2]&gt;\n2       2511    51 F        55    NA   118 &lt;tibble [5 × 2]&gt;\n3       8718    68 F        61    81    93 &lt;tibble [5 × 2]&gt;\n4       2986    60 F        58   154   257 &lt;tibble [5 × 2]&gt;\n5       1842    77 M        77    60   101 &lt;tibble [5 × 2]&gt;\n6       9334    46 F        56   146   159 &lt;tibble [5 × 2]&gt;\n\nstr(df)\n\ntibble [50 × 7] (S3: tbl_df/tbl/data.frame)\n $ patient_id : int [1:50] 2463 2511 8718 2986 1842 9334 3371 4761 6746 9819 ...\n $ age        : int [1:50] 51 51 68 60 77 46 79 78 63 33 ...\n $ sex        : chr [1:50] \"M\" \"F\" \"F\" \"F\" ...\n $ hdl        : num [1:50] 60 55 61 58 77 56 70 49 55 48 ...\n $ ldl        : num [1:50] 166 NA 81 154 60 146 123 62 124 122 ...\n $ tg         : num [1:50] 186 118 93 257 101 159 144 88 171 193 ...\n $ bp_readings:List of 50\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 107 109 109 108 105\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 130 130 129 131 129\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 118 120 117 120 116\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 135 132 141 134 136\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 144 145 143 145 145\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 119 119 119 119 123\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 112 111 110 112 113\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 140 141 139 138 143\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 107 106 105 107 107\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 108 110 108 110 107\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 147 147 146 147 145\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 112 109 116 113 109\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 129 128 127 133 132\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 111 110 112 110 110\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 107 105 106 110 107\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 152 152 152 154 151\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 112 110 115 111 111\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 100 98 97 99 101\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 114 116 115 113 114\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 144 143 143 146 142\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 128 132 128 128 127\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 121 120 118 121 122\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 134 135 132 132 133\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 148 151 146 148 152\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 127 127 124 126 128\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 134 133 133 133 134\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 114 117 114 116 115\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 111 111 108 110 110\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 104 104 107 109 107\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 116 116 112 115 116\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 159 161 161 160 156\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 153 155 152 153 153\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 129 130 129 126 130\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 133 134 132 133 133\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 114 114 117 114 114\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 113 115 115 115 112\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 134 138 134 138 131\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 110 110 112 109 108\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 108 106 106 107 109\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 108 104 108 110 112\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 107 110 109 104 106\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 109 108 110 109 106\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 104 107 106 104 106\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 121 118 122 120 122\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 116 116 117 119 116\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 104 106 102 103 107\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 108 109 104 105 108\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 108 110 108 109 110\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 114 117 114 114 110\n  ..$ : tibble [5 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ time_point    : chr [1:5] \"bp1\" \"bp2\" \"bp3\" \"bp4\" ...\n  .. ..$ blood_pressure: num [1:5] 126 126 125 124 126\n\n\nLibraries and data are being loaded. The data is an R dataset (rds). The user then has a look at what the data contains with head and str. We can see that there are 7 columns. The last one, bp_readings is nested. Perhaps str is not such a good idea for this type of data since the output becomes very long due to the nesting.\n\ncolSums(is.na(df))\n\n patient_id         age         sex         hdl         ldl          tg \n          0           0           0           0           3           2 \nbp_readings \n          0 \n\ndf &lt;- na.omit(df)\ncolSums(is.na(df))\n\n patient_id         age         sex         hdl         ldl          tg \n          0           0           0           0           0           0 \nbp_readings \n          0 \n\n\nThe author of the script is checking for NA values in the data, then omitting them and checking again.\n\ndf$total_chol &lt;- df$hdl + df$ldl + (df$tg/5) \n\nThe author adds a new column which is the total cholesterol after the Friedewald equation.\n\\[ Total Cholesterol = HDL + LDL + \\frac{Triglycerides}{5} \\]\n\nhigh_chol_patients &lt;- df %&gt;%\n  filter(total_chol &gt; 240) %&gt;%\n  select(patient_id, age, total_chol, sex)\n\ntable(high_chol_patients$sex)\n\n\nF f M \n4 2 6 \n\n\nA subset of patients with high total cholesterol is made. The author investigates the distribution of sexes in the subset and discovers that in some rows female is coded as ‘f’ instead of ‘F’.\n\nhigh_chol_patients &lt;- high_chol_patients %&gt;%\n  mutate(sex_fixed = ifelse(sex == 'f', 'F', sex))\n\nThe author fixes the lower case ‘f’ but only in the subset, not the whole dataset. They also do not investigate whether the same problem is present for ‘m’/‘M’ in the whole dataset.\n\nbp1 &lt;- df %&gt;%\n  unnest(bp_readings) %&gt;%\n  select(patient_id, time_point, blood_pressure)\n\nbp2 &lt;- bp1 %&gt;% pivot_wider(names_from = time_point, values_from = blood_pressure)\nmean_bp &lt;- rowMeans(bp2[,2:6])\nbp2$mean &lt;- mean_bp\n\nThe author unnests the original dataset into a new dataframe (which is fine). They then create another new dataframe which has the same information in wide format. This could have been done in one step to avoid having too many very similar dataframes. Also, it is not very clear from the naming what is in the mean column, and generally names that are also function names should be avoided since especially in tidyverse it is not always clear whether code refers to the column mean or the function mean.\nA mean value is calculated across the 5 blood pressure measurements and added to the newest dataframe.\n\nfor (i in 1:nrow(bp2)) {\n  if (bp2$mean[i] &gt; 140) {\n    bp2$bp_category[i] &lt;- \"Hypertension\"\n  } else if (bp2$mean[i] &lt; 120) {\n    bp2$bp_category[i] &lt;- \"elevated BP\"\n  } else {\n    bp2$bp_category[i] &lt;- \"Normal BP\"\n  }\n}\n\nWarning: Unknown or uninitialised column: `bp_category`.\n\n\nThe author new iterates over the new mean column and creates a new column that contains an evaluation of the blood pressure. This could have been done more elegantly with mutate and case_when. The more severe problem is that the second condition is the wrong way around: bp2$mean[i] &lt; 120. Blood pressure levels are considered elevated at above 120, not below. This is also has a consequence that the elevated and normal labels are switched. Lastly, the elevated label is spelled with a lower case letter whereas the other categories begin with an upper case letter (inconsistent naming).\n\nmerge_df &lt;- merge(df, bp2, by = 'patient_id')\n\nThe author now chooses to merge the blood pressure dataframe back into the original dataset. This is messy because the blood pressure measurements now exist twice. They should at least have dropped the nested column.\n\nmen &lt;- merge_df %&gt;%\n  filter(sex == 'M')\nwomen &lt;- merge_df %&gt;%\n  filter(sex == 'F')\n\nThe author now divides the merged data including the blood pressure category into two more dataframes for men and women. They loose some of the data because they have not investigated and fixed misspellings of ‘M’ and ‘F’ in the original dataframe.\n\nggplot(men, aes(x = age, fill = bp_category)) +\n  geom_histogram(position = 'dodge', binwidth = 10)\n\n\n\n\n\n\n\nggplot(women, aes(x = age, fill = bp_category)) +\n  geom_histogram(position = 'dodge', binwidth = 10)\n\n\n\n\n\n\n\n\nThe plots are fine, though they are missing data points as discussed above. The same could have been achived without two extra dataframes by using filter on the merged dataframe and then piping into ggplot."
  },
  {
    "objectID": "data/data.html",
    "href": "data/data.html",
    "title": "Data",
    "section": "",
    "text": "DOWNLOAD DATA  \n\nAfter download, unzip the data folder and place it somewhere you can find it again."
  },
  {
    "objectID": "data/data.html#download-data",
    "href": "data/data.html#download-data",
    "title": "Data",
    "section": "",
    "text": "DOWNLOAD DATA  \n\nAfter download, unzip the data folder and place it somewhere you can find it again."
  },
  {
    "objectID": "data/data.html#download-presentations",
    "href": "data/data.html#download-presentations",
    "title": "Data",
    "section": "Download presentations",
    "text": "Download presentations\nIt can be nice to follow along the presentation scripts as we go through them.\n\n  DOWNLOAD PRESENTATIONS"
  }
]