---
title: "Exercise 5 - Modelling"
format: html
project:
 type: website
 output-dir: ../docs
---

## Introduction

In this exercise you will fit and interpret simple models.

1. Load packages

## Part 1: Linear regression

We will use the dataset described below to fit a linear regression model. 

2. Load the data `boston.csv` and inspect it.

This dataset describes conditions surrounding the Boston housing market in the 1970s. Each row describes a zone in the Boston area (so there is more than one house in each row).

The columns are:

    crim - per capita crime rate
    indus - proportion of non-retail businesses
    nox - Nitrogen oxides concentration (air pollution)
    rm - average number of rooms
    neighborhood - the type of neighborhood the zone is in
    medv - median value per house in 1000s


### Explore the data

3. Does the datatype of each column fit to it what it describes? Do you need to change any data types?


### Making a model

4. Split the dataset into test and training data.

5. Fit a model of how well the number of rooms (`rm`), crime rate (`crim`) and neighborhood type (`neighborhood`) predict the value of the houses (`medv`).

6. Describe what information you get from the model summary. 

7. Scale the numeric predictor columns and redo the modelling. What has changed?

::: {.callout-tip collapse="true"}
## Hint
There is a scale function, see `?scale()`.
:::

## Part 2: Logistic regression 

For this part we will use the joined diabetes data since it has a categorical outcome (Diabetes yes or no). We will not use the oral Glucose measurements as predictors since this is literally how you define diabetes, so we're loading the joined dataset we created in exercise 2, e.g. 'diabetes_join.xlsx' or what you have named it. 


We choose to make a regression model of `Diabetes` as predicted by serum calcium levels (`Serum_ca2`), `BMI` and smoking habits (`Smoker`).

8. We cannot have NA values in our predictors so remove all rows with NAs and save the result into a new dataframe `diabetes_nona`.


9. Make the outcome variable into a factor if it is not already.


10. Scale all numeric predictors. Check your result.


11. Split your data into training and test data. Take care that the two classes of the outcome variable are in the same ratio in both training and test data. 


12. Fit a regression model with `Serum_ca2`, `BMI` and `Smoker` as predictors. Check the model summary.


13. Create a second model with only `BMI` and `Smoker` as predictors. Compare the fit of your second model to the first one (including `Serum_ca2`). Is there a significant gain, i.e. better fit when including the serum calcium levels as predictor? Which model do you think is better?

--------------------

## Extra exercises



e1. Find the best single predictor in the Diabetes dataset. This is done by comparing the null model (no predictors) to all possible models with one predictor, i.e. `outcome ~ predictor`, `outcome ~ predictor2`, ect. The null model can be formulated like so: `outcome ~ 1` (only the intercept). Fit all possible one predictor models and compare their fit to the null model with a likelihood ratio test. Find the predictor with the lowest p-value in the likelihood ratio test. This can be done in a loop in order to avoid writing out all models. 

::: {.callout-tip collapse="true"}
## Hint
To use a formula with a variable you will need to combine the literal part and the variable with paste, e.g. `paste("Outcome ~", my_pred)`.
:::


