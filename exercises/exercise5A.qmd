---
title: "Exercise 5A: Intro to Regression in R"
format: html
project:
 type: website
 output-dir: ../docs
---

## Introduction

In this exercise you will fit and interpret simple models.

1.  Load packages

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(readxl)
library(ggfortify)
library(factoextra)
```

## Part 1: Linear regression

We will use the dataset described below to fit a linear regression model.

2.  Load the data `boston.csv` and inspect it.

This dataset describes conditions surrounding the Boston housing market in the 1970s. Each row describes a zone in the Boston area (so there is more than one house in each row).

The columns are:

```         
crim - per capita crime rate
indus - proportion of non-retail businesses
nox - Nitrogen oxides concentration (air pollution)
rm - average number of rooms
neighborhood - the type of neighborhood the zone is in
medv - median value per house in 1000s
```

### Explore the data

3.  Does the datatype of each column fit to it what it describes? Do you need to change any data types?

### Making a model

4.  Split the dataset into test and training data. N.B: For any categorical variables in the boston dataset ensure that all levels are represented in both training and test set.

::: {.callout-tip collapse="true"}
## Hint

There are many ways split a dataset - an easy way is to use the function `sample_frac()`.
:::

5.  Using your training data fit a linear model with number of rooms (`rm`), crime rate (`crim`) and neighborhood type (`neighborhood`) as the predictors and the value of a house (`medv`) as the response variable (y).

6.  Describe what information you get from the model summary.

7.  If you wanted to know if there is a difference in the value of houses between the `Suburban` and `Urban` neighborhood what could you do to the variable `neighborhood` before modelling?

8.  For linear regression there is an assumption that the model residuals (errors) are normally distributed. An easy way visualize this is by simply calling `plot()` on your model (see below). What do you think based on the plots?

```{r, eval=FALSE}
#RMSE
par(mfrow=c(2,2))
plot(model)
```

9.  Now, use our test set to predict the response `medv` (`median value per house in 1000s`).

10. Evaluate how well our model performs. There are different ways of doing this but lets use the classic measure of RMSE (Root Mean Square Error). The psedo-code below shows how to calculate the RMSE. A small RMSE (close to zero), indicates a good model.

```{r, eval=FALSE}
#RMSE
rmse <- sqrt(mean((y_test - y_pred)^2))
```

11. Make a scatter plot to visualize how the predicted values fit with the observed values.

::: {.callout-tip collapse="true"}
## Hint

Plot `y_test` against `y_pred`.
:::

## Part 2: Logistic regression

For this part we will use the joined diabetes data since it has a categorical outcome (Diabetes yes or no). We will not use the oral Glucose measurements as predictors since this is literally how you define diabetes, so we're loading the joined dataset we created in exercise 1, e.g. 'diabetes_join.xlsx' or what you have named it. N.B if you did not manage to finish making this dataset or forgot to save it, you can find a copy here: `../out/diabetes_join.Rdata`. Navigate to the file in the file window of Rstudio and click on it. Click "Yes" to confirm that the file can be loaded in your environment and check that it has happened.

As the outcome we are studying, `Diabetes`, is categorical variable we will perform logistic regression. We select serum calcium levels (`Serum_ca2`), `BMI` and smoking habits (`Smoker`) as predictive variables.

12. Read in the Diabetes dataset.

13. Logistic regression does not allow for any missing values so first ensure you do not have NAs in your dataframe. Ensure that your outcome variable `Diabetes` is a factor.

14. Split your data into training and test data. Take care that the two classes of the outcome variable are represented in both training and test data, and at similar ratios.

15. Fit a logistic regression model with `Serum_ca2`, `BMI` and `Smoker` as predictors and `Diabetes` as outcome, using your training data.

::: {.callout-tip collapse="true"}
## Hint

glm(..., family = 'binomial')
:::

16. Check the model summary and try to determine whether you could potentially drop one or more of your variables? If so, make this alternative model (model2) and compare it to the original model. Is there a significant loss/gain, i.e. better fit when including the serum calcium levels as predictor?

17. Now, use your model to predict Diabetes class based on your test set. What does the output of the prediction mean?

::: {.callout-tip collapse="true"}
## Hint

`predict(... , type ='response')`
:::

18. Lets evaluate the performance of our model. As we are performing classification, measures such as mse/rmse will not work, instead we will calculate the accuracy. In order to get the accuracy you must first convert our predictions into Diabetes class labels (e.g. 0 or 1).

```{r, eval=FALSE}
caret::confusionMatrix(y_pred, y_test)
```

## Part 3: Clustering

In this part we will run clustering on the joined diabetes dataset (`diabetes_join.xlsx`) from exercise 1. Load it here if you don't have it already from Part 2 above.

19. Before running K-means clustering. Remove any missing values across all variables in your dataset. Consider which columns you can use and if you have to do anything to them before clustering?

20. Run the k-means clustering algorithm with 4 centers on the data.

21. Visualize the results of your clustering.

22. Investigate the best number of clusters. Try to use the `silhouette` metric. How many clusters are there?

23. Re-do the clustering (plus visualization) with that number. What do you think the clusters represent in this case?
